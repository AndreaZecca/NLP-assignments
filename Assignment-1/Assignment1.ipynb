{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
        "\n",
        "**Keywords**: POS tagging, Sequence labelling, RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEMlC0FYO0l2"
      },
      "source": [
        "\n",
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "* Eleonora Mancini -> e.mancini@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDP-HaMpO0l3"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "You are tasked to address the task of POS tagging.\n",
        "\n",
        "<center>\n",
        "    <img src=\"./images/pos_tagging.png\" alt=\"POS tagging\" />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Kd1J3ALIqdcH"
      },
      "outputs": [],
      "source": [
        "# best models is a dictionary to store the best models for each seed\n",
        "# best losses keep track of the best loss for each seed after a training loop, so that we can re-train the model without\n",
        "#   loosing the best model after the first iteration\n",
        "# total epochs is a dictionary to keep track of the total epochs for each seed, just for information purposes\n",
        "# run this cell only once\n",
        "\n",
        "best_models = {}\n",
        "best_losses = {}\n",
        "total_epochs = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "c7ApYRXjqdcI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "from torchtext.vocab import GloVe\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.utils.rnn as rnn\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from operator import itemgetter\n",
        "import spacy\n",
        "\n",
        "SEED = 0\n",
        "DATA_FOLDER = './data'\n",
        "WEIGHTS_FOLDER = './weights'\n",
        "\n",
        "LOWER = True\n",
        "\n",
        "NUMBER = False\n",
        "NUM_RE = r\"(\\d*\\,)?\\d+.\\d*\"\n",
        "NUM_TOKEN = '<num>'\n",
        "\n",
        "NER = False\n",
        "NER_TOKEN = {\n",
        "    'CARDINAL' : '<num>',\n",
        "    'ORG' : '<org>',\n",
        "    'PERSON' : '<per>',\n",
        "}\n",
        "\n",
        "\n",
        "PAD_INDEX = 0\n",
        "PAD_TOKEN = '<pad>'\n",
        "\n",
        "EMBEDDING_DIMENSION = 300\n",
        "OOV_EMBEDDING_TYPE = 'mean' # either \"mean\" or \"random\"\n",
        "MEAN_EMBEDDING_WINDOW = 1\n",
        "FIXED_OOV = False\n",
        "FIXED_OOV_VECTOR = np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr8QdeOXO0l4"
      },
      "source": [
        "# [Task 1 - 0.5 points] Corpus\n",
        "\n",
        "You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n",
        "\n",
        "**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n",
        "\n",
        "### Example\n",
        "\n",
        "```Pierre\tNNP\t2\n",
        "Vinken\tNNP\t8\n",
        ",\t,\t2\n",
        "61\tCD\t5\n",
        "years\tNNS\t6\n",
        "old\tJJ\t2\n",
        ",\t,\t2\n",
        "will\tMD\t0\n",
        "join\tVB\t8\n",
        "the\tDT\t11\n",
        "board\tNN\t9\n",
        "as\tIN\t9\n",
        "a\tDT\t15\n",
        "nonexecutive\tJJ\t15\n",
        "director\tNN\t12\n",
        "Nov.\tNNP\t9\n",
        "29\tCD\t16\n",
        ".\t.\t8\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd_jkm9hO0l5"
      },
      "source": [
        "### Splits\n",
        "\n",
        "The corpus contains 200 documents.\n",
        "\n",
        "   * **Train**: Documents 1-100\n",
        "   * **Validation**: Documents 101-150\n",
        "   * **Test**: Documents 151-199"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hx0FbERO0l6"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Download** the corpus.\n",
        "* **Encode** the corpus into a pandas.DataFrame object.\n",
        "* **Split** it in training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YpKS7sJpqdcJ"
      },
      "outputs": [],
      "source": [
        "def mask_number(text):\n",
        "    try:\n",
        "        if re.match(NUM_RE, text):\n",
        "            return NUM_TOKEN\n",
        "        else:\n",
        "            return text\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "def mask_ner(text_tokens):\n",
        "    text_tokens_str = [str(token) for token in text_tokens]\n",
        "    text = ' '.join(text_tokens_str)\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "      if ent.label_ in NER_TOKEN:\n",
        "          for sub_token in ent.text.split():\n",
        "            text_tokens[text_tokens == sub_token] = NER_TOKEN[ent.label_]\n",
        "    return text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CCvjLqXxqdcJ"
      },
      "outputs": [],
      "source": [
        "def preprcess_dataset(df) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "    Preprocess the dataset, lowercase the words and/or mask the numbers\n",
        "  \"\"\"\n",
        "  if NER:\n",
        "    df['word'] = mask_ner(df['word'])\n",
        "  if LOWER:\n",
        "    df['word'] = df[\"word\"].str.lower()\n",
        "  if NUMBER:\n",
        "    df['word'] = df['word'].apply(mask_number)\n",
        "  return df\n",
        "\n",
        "def encode_dataset(dataset_name: str) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "    Encode the dataset as a pandas dataframe, each row is a sentence, each sentence is a list of words and a list of corresponding tags\n",
        "  \"\"\"\n",
        "  dataset_folder = os.path.join(DATA_FOLDER+ \"/dataset\")\n",
        "\n",
        "  dataframe_rows = []\n",
        "  unique_tags = set()\n",
        "  unique_words = set()\n",
        "\n",
        "  for doc in sorted(os.listdir(dataset_folder)):\n",
        "    if doc.endswith(\".csv\") or doc.endswith(\".pkl\"): continue\n",
        "    doc_num = int(doc[5:8])\n",
        "    doc_path = os.path.join(dataset_folder,doc)\n",
        "\n",
        "    with open(doc_path, mode='r', encoding='utf-8') as file:\n",
        "      df = pd.read_csv(file,sep='\\t', header=None, skip_blank_lines=False)\n",
        "      df.rename(columns={0:'word', 1:\"TAG\", 2:\"remove\"}, inplace=True)\n",
        "      df.drop(\"remove\", axis=1, inplace=True)\n",
        "\n",
        "      df = preprcess_dataset(df)\n",
        "\n",
        "      df[\"group_num\"] = df.isnull().all(axis=1).cumsum()\n",
        "      df.dropna(inplace=True)\n",
        "      df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "      unique_tags.update(df['TAG'].unique())\n",
        "      unique_words.update(df['word'].unique())\n",
        "\n",
        "      df_list = [df.iloc[rows] for _, rows in df.groupby('group_num').groups.items()]\n",
        "      for n,d in enumerate(df_list):\n",
        "          dataframe_row = {\n",
        "              \"split\" : 'train' if doc_num<=100 else ('val' if doc_num<=150  else 'test'),\n",
        "              \"doc_id\" : doc_num,\n",
        "              \"sentence_num\" : n,\n",
        "              \"words\": d['word'].tolist(),\n",
        "              \"tags\":  d['TAG'].tolist(),\n",
        "              \"num_tokens\": len(d['word'])\n",
        "          }\n",
        "          dataframe_rows.append(dataframe_row)\n",
        "\n",
        "  dataframe_path = os.path.join(DATA_FOLDER, dataset_name)\n",
        "  df_final = pd.DataFrame(dataframe_rows)\n",
        "  df_final.to_csv(dataframe_path + \".csv\")\n",
        "\n",
        "  unique_tags_words_path = os.path.join(DATA_FOLDER, \"unique_tags_words.pkl\")\n",
        "  unique_tags_words = [unique_tags, unique_words]\n",
        "  with open(unique_tags_words_path, 'wb') as f:\n",
        "    pickle.dump(unique_tags_words, f)\n",
        "\n",
        "  return  df_final, unique_tags, unique_words\n",
        "\n",
        "df, unique_tags, unique_words = encode_dataset(\"encoded_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "135YHdmjPj60",
        "outputId": "2c11b8fc-b896-45c3-929c-5a4f50cd08de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>doc_id</th>\n",
              "      <th>sentence_num</th>\n",
              "      <th>words</th>\n",
              "      <th>tags</th>\n",
              "      <th>num_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1975</th>\n",
              "      <td>val</td>\n",
              "      <td>101</td>\n",
              "      <td>12</td>\n",
              "      <td>[about, a, quarter, of, this, share, has, alre...</td>\n",
              "      <td>[IN, DT, NN, IN, DT, NN, VBZ, RB, VBN, VBN, ,,...</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3262</th>\n",
              "      <td>test</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>[intelogic, trace, inc., ,, san, antonio, ,, t...</td>\n",
              "      <td>[NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      split  doc_id  sentence_num  \\\n",
              "0     train       1             0   \n",
              "1975    val     101            12   \n",
              "3262   test     151             0   \n",
              "\n",
              "                                                  words  \\\n",
              "0     [pierre, vinken, ,, 61, years, old, ,, will, j...   \n",
              "1975  [about, a, quarter, of, this, share, has, alre...   \n",
              "3262  [intelogic, trace, inc., ,, san, antonio, ,, t...   \n",
              "\n",
              "                                                   tags  num_tokens  \n",
              "0     [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...          18  \n",
              "1975  [IN, DT, NN, IN, DT, NN, VBZ, RB, VBN, VBN, ,,...          51  \n",
              "3262  [NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...          40  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sort_values(\"doc_id\").groupby('split').head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7Ynce_z1qdcK"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, unique_tags, unique_words):\n",
        "        self.unique_tags = unique_tags\n",
        "        self.unique_words = unique_words\n",
        "        self.tag2int = {}\n",
        "        self.int2tag = {}\n",
        "        self.tag2int[PAD_TOKEN] = PAD_INDEX\n",
        "        self.int2tag[PAD_INDEX] = PAD_TOKEN\n",
        "        self.word2int = {}\n",
        "        self.int2word = {}\n",
        "        self.word2int[PAD_TOKEN] = PAD_INDEX\n",
        "        self.int2word[PAD_INDEX] = PAD_TOKEN\n",
        "        self.build_vocab()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        for i, word in enumerate(self.unique_words):\n",
        "            self.word2int[word] = i+1\n",
        "            self.int2word[i+1] = word\n",
        "        for i, tag in enumerate(self.unique_tags):\n",
        "            self.tag2int[tag] = i+1\n",
        "            self.int2tag[i+1] = tag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.unique_words) + 1\n",
        "\n",
        "    def w2i(self, word):\n",
        "        return self.word2int[word]\n",
        "\n",
        "    def i2w(self, index):\n",
        "        return self.int2word[index]\n",
        "\n",
        "    def t2i(self, tag):\n",
        "        return self.tag2int[tag]\n",
        "\n",
        "    def i2t(self, index):\n",
        "        return self.int2tag[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HnPvsbn8qdcK"
      },
      "outputs": [],
      "source": [
        "vocab = Vocabulary(unique_tags, unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "r_nawsS0qdcK"
      },
      "outputs": [],
      "source": [
        "def build_indexed_dataframe(df):\n",
        "    \"\"\"\n",
        "        Builds a dataframe with the same structure as the original one, but with the words and tags replaced by their index\n",
        "    \"\"\"\n",
        "    indexed_rows = []\n",
        "    for words,tags in zip(df['words'], df['tags']):\n",
        "        indexed_row = {'indexed_words': [vocab.w2i(word) for word in words], 'indexed_tags': [vocab.t2i(tag) for tag in tags]}\n",
        "        indexed_rows.append(indexed_row)\n",
        "\n",
        "    indexed_df = pd.DataFrame(indexed_rows)\n",
        "\n",
        "    indexed_df.insert(0,'split',df['split'])\n",
        "    indexed_df.insert(1,'num_tokens',df['num_tokens'])\n",
        "\n",
        "    indexed_df_path = os.path.join(DATA_FOLDER, \"indexed_dataset.pkl\")\n",
        "    indexed_df.to_pickle(indexed_df_path)\n",
        "\n",
        "    return indexed_df\n",
        "\n",
        "indexed_dataset = build_indexed_dataframe(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQvHxKRcO0l8"
      },
      "source": [
        "# [Task 2 - 0.5 points] Text encoding\n",
        "\n",
        "To train a neural POS tagger, you first need to encode text into numerical format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnUuOWTO0l9"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Embed words using **GloVe embeddings**.\n",
        "* You are **free** to pick any embedding dimension.\n",
        "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eVsfe-QKO0l9"
      },
      "outputs": [],
      "source": [
        "glove_embeddings = GloVe(name='42B', dim=EMBEDDING_DIMENSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "bbvoppGqqdcL"
      },
      "outputs": [],
      "source": [
        "def get_oov_by_splits(splits, emb_model):\n",
        "    \"\"\"\n",
        "        Returns a dictionary with the oov words for each split\n",
        "    \"\"\"\n",
        "    oov_by_split = {}\n",
        "    for split in splits:\n",
        "        oov_by_split[split] = set()\n",
        "        for words in df[df['split']==split]['words']:\n",
        "            for word in words:\n",
        "                if word not in emb_model.stoi:\n",
        "                    oov_by_split[split].add(word)\n",
        "    return oov_by_split\n",
        "\n",
        "def get_oov_neighbors(oovs, sentences):\n",
        "    \"\"\"\n",
        "        Returns a dictionary with the oov words and their neighbors\n",
        "    \"\"\"\n",
        "    oov_neighbors = {}\n",
        "    for oov in oovs:\n",
        "        oov_neighbors[oov] = set()\n",
        "        for sentence in sentences:\n",
        "            # Save all the words that are in a range of MEAN_EMBEDDING_WINDOW words before and after the oov word\n",
        "            if oov in sentence:\n",
        "                oov_idx = sentence.index(oov)\n",
        "                for i in range(max(0, oov_idx-MEAN_EMBEDDING_WINDOW), min(len(sentence), oov_idx+MEAN_EMBEDDING_WINDOW+1)):\n",
        "                    oov_neighbors[oov].add(sentence[i])\n",
        "    return oov_neighbors\n",
        "\n",
        "def build_split_matrix(oovs, oov_neighbors, emb_model):\n",
        "    embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIMENSION), dtype=np.float32)\n",
        "    for word, idx in vocab.word2int.items():\n",
        "        if word in oovs:\n",
        "            neighboring_wvs = []\n",
        "            for neighbor in oov_neighbors[word]:\n",
        "                if neighbor not in oovs:\n",
        "                    neighboring_wvs.append(emb_model[neighbor])\n",
        "            # If there is at least one neighbor, compute the mean of their word vectors\n",
        "            if len(neighboring_wvs) > 1:\n",
        "                embedding_vector = torch.mean(torch.stack(neighboring_wvs), dim=0)\n",
        "            else:\n",
        "                embedding_vector = FIXED_OOV_VECTOR if FIXED_OOV else np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n",
        "        else:\n",
        "            embedding_vector = emb_model[word]\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Hdd7Yzo7qdcM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOVs in train set:  181\n",
            "example:  ['1.5805', 'index-options', 'energy-services', '30,841', '271,124', 'school-research', 'achievement-test', 'shangkun', 'monchecourt', '143.93']\n",
            "OOVs in val set:  108\n",
            "OOVs in test set:  90\n"
          ]
        }
      ],
      "source": [
        "def build_embedding_matrix(emb_model):\n",
        "    \"\"\"\n",
        "        Creates the embedding matrix from the embedding model starting from the vocabulary and from pre-trained Glove embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIMENSION), dtype=np.float32)    # len(vocab) already includes the padding token\n",
        "\n",
        "    if OOV_EMBEDDING_TYPE == 'random':\n",
        "\n",
        "        for word, idx in vocab.word2int.items():\n",
        "            if word in emb_model.stoi:\n",
        "                embedding_vector = emb_model[word]\n",
        "            else:\n",
        "                embedding_vector = FIXED_OOV_VECTOR if FIXED_OOV else np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n",
        "            embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "        path = os.path.join(DATA_FOLDER, \"emb_matrix\")\n",
        "        np.save(path, embedding_matrix, allow_pickle=True)\n",
        "\n",
        "        return embedding_matrix, [], [], []\n",
        "\n",
        "    if OOV_EMBEDDING_TYPE == 'mean':\n",
        "        oov_by_split = get_oov_by_splits(['train', 'val', 'test'], emb_model)\n",
        "\n",
        "        train_oov = oov_by_split['train']\n",
        "        train_oov.add(PAD_TOKEN)\n",
        "        print(\"OOVs in train set: \", len(train_oov))\n",
        "        print(\"example: \", list(train_oov)[:10])\n",
        "\n",
        "        val_oov = oov_by_split['val']\n",
        "        # val_oov = val_oov - train_oov\n",
        "        print(\"OOVs in val set: \", len(val_oov))\n",
        "\n",
        "        test_oov = oov_by_split['test']\n",
        "        # test_oov = test_oov - train_oov - val_oov\n",
        "        print(\"OOVs in test set: \", len(test_oov))\n",
        "\n",
        "        train_sentences = df[df['split']=='train']['words'].tolist()\n",
        "        #val_sentences = df[df['split']=='val']['words'].tolist()\n",
        "        #test_sentences = df[df['split']=='test']['words'].tolist()\n",
        "\n",
        "        train_oov_neighbors = get_oov_neighbors(train_oov, train_sentences)\n",
        "        #val_oov_neighbors = get_oov_neighbors(val_oov, val_sentences)\n",
        "        #test_oov_neighbors = get_oov_neighbors(test_oov, test_sentences)\n",
        "\n",
        "        # Now we construct three different embedding matrix, one for each split\n",
        "        train_embedding_matrix = build_split_matrix(train_oov, train_oov_neighbors, emb_model)\n",
        "\n",
        "        val_embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIMENSION), dtype=np.float32)\n",
        "        for word, idx in vocab.word2int.items():\n",
        "            if word in train_oov:\n",
        "                embedding_vector = train_embedding_matrix[idx]\n",
        "            elif word in val_oov:\n",
        "                embedding_vector = FIXED_OOV_VECTOR if FIXED_OOV else np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n",
        "            else:\n",
        "                embedding_vector = emb_model[word]\n",
        "            val_embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "        test_embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIMENSION), dtype=np.float32)\n",
        "        for word, idx in vocab.word2int.items():\n",
        "            if word in train_oov:\n",
        "                embedding_vector = train_embedding_matrix[idx]\n",
        "            elif word in val_oov:\n",
        "                embedding_vector = val_embedding_matrix[idx]\n",
        "            elif word in test_oov:\n",
        "                embedding_vector = FIXED_OOV_VECTOR if FIXED_OOV else np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n",
        "            else:\n",
        "                embedding_vector = emb_model[word]\n",
        "            test_embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "        # Finally, we build the embedding matrix for the whole dataset\n",
        "        return test_embedding_matrix, train_embedding_matrix, val_embedding_matrix, test_embedding_matrix\n",
        "\n",
        "\n",
        "embedding_matrix, train_embedding_matrix, val_embedding_matrix, test_embedding_matrix = build_embedding_matrix(glove_embeddings);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-L3ceydfqdcM"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "        Loads the dataframe, the embedding matrix, the unique tags and unique words and the indexed dataframe\n",
        "    \"\"\"\n",
        "    df_path = os.path.join(DATA_FOLDER,'encoded_dataset.csv')\n",
        "    emb_matrix_path = os.path.join(DATA_FOLDER,'emb_matrix.npy')\n",
        "    unique_tags_words_path = os.path.join(DATA_FOLDER, \"unique_tags_words.pkl\")\n",
        "    indexed_dataset_path = os.path.join(DATA_FOLDER,'indexed_dataset.pkl')\n",
        "\n",
        "    if os.path.exists(emb_matrix_path) and os.path.exists(indexed_dataset_path) and os.path.exists(unique_tags_words_path) and os.path.exists(df_path):\n",
        "        emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n",
        "        indexed_dataset = pd.read_pickle(indexed_dataset_path)\n",
        "        unique_tags_words = pickle.load(open(unique_tags_words_path, 'rb'))\n",
        "        unique_tags, unique_words = unique_tags_words[0], unique_tags_words[1]\n",
        "        df = pd.read_csv(df_path, index_col=0)\n",
        "\n",
        "    else:\n",
        "        print('What you are looking for is not present in the folder')\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    return df, emb_matrix, unique_tags, unique_words, indexed_dataset\n",
        "\n",
        "df, embedding_matrix, unique_tags, unique_words, indexed_dataset = load_data()\n",
        "vocab = Vocabulary(unique_tags, unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4AChgkfO0l_"
      },
      "source": [
        "# [Task 3 - 1.0 points] Model definition\n",
        "\n",
        "You are now tasked to define your neural POS tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkERwLU7O0l_"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
        "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
        "\n",
        "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
        "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
        "\n",
        "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
        "\n",
        "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "babZmAQlWGIw"
      },
      "source": [
        "### Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Issj3-NcqdcM"
      },
      "outputs": [],
      "source": [
        "def create_emb_layer(weights_matrix, pad_idx = PAD_INDEX):\n",
        "    \"\"\"\n",
        "        Creates the embedding layer from the embedding matrix\n",
        "    \"\"\"\n",
        "    matrix = torch.Tensor(weights_matrix)\n",
        "    emb_layer = nn.Embedding.from_pretrained(matrix, freeze=True, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable\n",
        "    return emb_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AfDR1l1qdcN"
      },
      "source": [
        "### Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RSGVLm3iWF2a"
      },
      "outputs": [],
      "source": [
        "class Baseline(nn.Module):\n",
        "    def __init__(self, lstm_dimension, dense_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = create_emb_layer(embedding_matrix)\n",
        "        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=EMBEDDING_DIMENSION, hidden_size=lstm_dimension, batch_first=True)\n",
        "        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n",
        "\n",
        "    def forward(self, sentences, sentences_length):\n",
        "        embedded_sentences = self.embedding_layer(sentences)\n",
        "        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, _ = self.bidirectional_layer(packed_sentences)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        output = self.dense_layer(output)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd-pJQyu6h-V"
      },
      "source": [
        "### Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4pOy7gXR62UC"
      },
      "outputs": [],
      "source": [
        "class Model1(nn.Module):\n",
        "    def __init__(self, lstm_dimension, dense_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = create_emb_layer(embedding_matrix)\n",
        "        self.bidirectional_layer_1 = nn.LSTM(bidirectional=True, input_size=EMBEDDING_DIMENSION, hidden_size=lstm_dimension, batch_first=True)\n",
        "        self.bidirectional_layer_2 = nn.LSTM(bidirectional=True, input_size=lstm_dimension*2, hidden_size=lstm_dimension, batch_first=True)\n",
        "        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n",
        "\n",
        "    def forward(self, sentences, sentences_length):\n",
        "        embedded_sentences = self.embedding_layer(sentences)\n",
        "        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, _ = self.bidirectional_layer_1(packed_sentences)\n",
        "        packed_output, _ = self.bidirectional_layer_2(packed_output)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        output = self.dense_layer(output)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY9Vy9997y5Q"
      },
      "source": [
        "### Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_RM739C970T5"
      },
      "outputs": [],
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self, lstm_dimension, dense_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = create_emb_layer(embedding_matrix)\n",
        "        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=EMBEDDING_DIMENSION, hidden_size=lstm_dimension, batch_first=True)\n",
        "        self.dense_layer_1 = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n",
        "        self.dense_layer_2 = nn.Linear(in_features=dense_dimension, out_features=dense_dimension)\n",
        "\n",
        "    def forward(self, sentences, sentences_length):\n",
        "        embedded_sentences = self.embedding_layer(sentences)\n",
        "        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, _ = self.bidirectional_layer(packed_sentences)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        output = self.dense_layer_1(output)\n",
        "        output = self.dense_layer_2(output)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9vgsKiaO0mA"
      },
      "source": [
        "# [Task 4 - 1.0 points] Metrics\n",
        "\n",
        "Before training the models, you are tasked to define the evaluation metrics for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_T6Bm1fO0mA"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
        "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n",
        "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpoB8xTRO0mA"
      },
      "source": [
        "**Note**: What about OOV tokens?\n",
        "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
        "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
        "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Xx7AKsk69Zcw"
      },
      "outputs": [],
      "source": [
        "def accuracy_and_f1(y_pred, y_true):\n",
        "    \"\"\"\n",
        "        Computes the accuracy and the f1 score\n",
        "    \"\"\"\n",
        "    correct = y_pred.eq(y_true)\n",
        "    acc = correct.sum()/y_true.shape[0]\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    return acc,f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHsdrfEaqdcO"
      },
      "source": [
        "### Let's check our OOV tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFrYzM4dqdcO",
        "outputId": "d159dba8-ef07-4712-9edf-c1bfd16915c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of unique words in dataset: 10947\n",
            "Total OOV terms: 356 (3.25%)\n"
          ]
        }
      ],
      "source": [
        "def check_OOV_terms(embedding_model, words):\n",
        "    \"\"\"\n",
        "        Checks the OOV terms in the dataset and returns the list of OOV terms and their indexes\n",
        "    \"\"\"\n",
        "    oov_words = []\n",
        "    int_oov_words = []\n",
        "\n",
        "    for word in words:\n",
        "        if word not in embedding_model.itos:\n",
        "           oov_words.append(word)\n",
        "           int_oov_words.append(vocab.w2i(word))\n",
        "\n",
        "    print(\"Total number of unique words in dataset:\",len(words))\n",
        "    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(words))*100))\n",
        "    return oov_words, int_oov_words\n",
        "\n",
        "oov_words, int_oov_words = check_OOV_terms(glove_embeddings, unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu8LiuybqdcP"
      },
      "source": [
        "Depending on the text pre-processing, you may have OOV tokens in the dataframe.\n",
        "If we apply lowercasing we have 6.18% of OOV tokens in total. While, when we don't apply lowercasing we have 31.29% of OOV tokens in total. <br>\n",
        "So we might proceed by using lowercased words, since the number of OOV decrease significantly, but we have to be careful because we could lose some information. <br>\n",
        "For example, the word \"Pierre\" is a name, but \"pierre\" is a noun. So, we have to find a way to deal with this problem.<br>\n",
        "We decided to test the model with the two different approaches and see which one is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy2OFkD_O0mA"
      },
      "source": [
        "# [Task 5 - 1.0 points] Training and Evaluation\n",
        "\n",
        "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G20Sgu1O0mB"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Train **all** models on the train set.\n",
        "* Evaluate **all** models on the validation set.\n",
        "* Compute metrics on the validation set.\n",
        "* Pick **at least** three seeds for robust estimation.\n",
        "* Pick the **best** performing model according to the observed validation set performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Q_nD09EnqdcT"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model):\n",
        "    \"\"\"\n",
        "        Initializes the weights of the model\n",
        "    \"\"\"\n",
        "    for _, param in model.named_parameters():\n",
        "        if isinstance(model, nn.LSTM) or isinstance(model, nn.Linear):\n",
        "            nn.init.normal_(param.data, mean = 0, std = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "G3hhIQxZqdcU"
      },
      "outputs": [],
      "source": [
        "def number_parameters(model):\n",
        "    \"\"\"\n",
        "        Computes the number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tPYvnHO8qdcU"
      },
      "outputs": [],
      "source": [
        "def get_to_be_masked_tags():\n",
        "    \"\"\"\n",
        "        Returns the tags that have to be masked\n",
        "    \"\"\"\n",
        "    punctuation_tags = ['$', '``', '.', ',', '#', 'SYM', ':', \"''\",'-RRB-','-LRB-']\n",
        "    token_punctuation = [vocab.t2i(tag) for tag in punctuation_tags]\n",
        "    return torch.LongTensor(token_punctuation+[PAD_INDEX])\n",
        "\n",
        "to_mask = get_to_be_masked_tags()\n",
        "\n",
        "def reshape_and_mask(predictions,targets):\n",
        "    \"\"\"\n",
        "        Reshapes the predictions and the targets and masks the elements that have to be masked\n",
        "    \"\"\"\n",
        "    non_masked_elements = torch.isin(targets, to_mask, invert=True)\n",
        "    return predictions[non_masked_elements],targets[non_masked_elements]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BdvD5aq-EiHV"
      },
      "outputs": [],
      "source": [
        "class PosDataset(Dataset):\n",
        "    \"\"\"\n",
        "        Dataset class for the POS tagging task\n",
        "    \"\"\"\n",
        "    def __init__(self, text, labels):\n",
        "        self.labels = labels\n",
        "        self.text = text\n",
        "        self.sentence_lengths = [len(sentence) for sentence in self.text]\n",
        "    def __len__(self):\n",
        "            return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "            label = self.labels[idx]\n",
        "            text = self.text[idx]\n",
        "            sample = (text, label, self.sentence_lengths[idx])\n",
        "            return sample\n",
        "\n",
        "\n",
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "        Collate function for the dataloader\n",
        "    \"\"\"\n",
        "    return ([x[0] for x in data], [x[1] for x in data], [x[2] for x in data])\n",
        "\n",
        "\n",
        "def create_dataloaders(b_s):\n",
        "    \"\"\"\n",
        "        Creates the dataloaders for the train, validation and test sets\n",
        "    \"\"\"\n",
        "    train_df = indexed_dataset[indexed_dataset['split'] == 'train'].reset_index(drop=True)\n",
        "    val_df = indexed_dataset[indexed_dataset['split'] == 'val'].reset_index(drop=True)\n",
        "    test_df = indexed_dataset[indexed_dataset['split'] == 'test'].reset_index(drop=True)\n",
        "\n",
        "    #create DataframeDataset objects for each split\n",
        "    train_dataset = PosDataset(train_df.iloc[:,2],train_df.iloc[:,3])\n",
        "    val_dataset = PosDataset(val_df.iloc[:,2],val_df.iloc[:,3])\n",
        "    test_dataset = PosDataset(test_df.iloc[:,2],test_df.iloc[:,3])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n",
        "\n",
        "    return train_dataloader,val_dataloader,test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "wuKaQtDuGAia"
      },
      "outputs": [],
      "source": [
        "tr_dl, val_dl, test_dl = create_dataloaders(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xzlrW52UIRet"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, loss_function, dataloader, val_dataloader, optimizer, scheduler, name, padding_value = PAD_INDEX):\n",
        "    \"\"\"\n",
        "        Training loop for the model with the given parameters\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    best_epoch_loss = np.inf\n",
        "    epochs_previously_trained = 0\n",
        "\n",
        "    if SEED in best_losses.keys():\n",
        "        if name in best_losses[SEED].keys():\n",
        "            best_epoch_loss = best_losses[SEED][name]\n",
        "\n",
        "    if SEED in total_epochs.keys():\n",
        "        if name in total_epochs[SEED].keys():\n",
        "            epochs_previously_trained = total_epochs[SEED][name]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        for sentences, pos, s_len in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            tensor_sentences = [torch.LongTensor(s) for s in sentences]\n",
        "            tensor_pos = [torch.LongTensor(p) for p in pos]\n",
        "\n",
        "            padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = padding_value)\n",
        "            padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n",
        "\n",
        "            predicted = model(padded_sentences, s_len)\n",
        "\n",
        "            predicted = predicted.view(-1,predicted.shape[-1])\n",
        "            targets = padded_pos.view(-1)\n",
        "\n",
        "            predicted, targets = reshape_and_mask(predicted, targets)\n",
        "\n",
        "            loss = loss_function(predicted, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_epoch_val_loss = 0\n",
        "\n",
        "        # TODO: va qui?\n",
        "        model.eval()\n",
        "\n",
        "        for sentences, pos, s_len in val_dataloader:\n",
        "            tensor_sentences = [torch.LongTensor(s) for s in sentences]\n",
        "            tensor_pos = [torch.LongTensor(p) for p in pos]\n",
        "\n",
        "            padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = padding_value)\n",
        "            padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n",
        "\n",
        "            predicted = model(padded_sentences, s_len)\n",
        "\n",
        "            predicted = predicted.view(-1,predicted.shape[-1])\n",
        "            targets = padded_pos.view(-1)\n",
        "\n",
        "            predicted, targets = reshape_and_mask(predicted, targets)\n",
        "\n",
        "            loss = loss_function(predicted, targets)\n",
        "            total_epoch_val_loss += loss.item()\n",
        "\n",
        "        if total_epoch_val_loss < best_epoch_loss:\n",
        "            best_epoch_loss = total_epoch_val_loss\n",
        "            if SEED not in best_models.keys():\n",
        "                best_models[SEED] = {}\n",
        "            best_models[SEED][name] = model.state_dict()\n",
        "\n",
        "            if SEED not in best_losses.keys():\n",
        "                best_losses[SEED] = {}\n",
        "            best_losses[SEED][name] = best_epoch_loss\n",
        "\n",
        "        scheduler.step(total_epoch_val_loss)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        print(f'Train epoch [{epoch+1 + epochs_previously_trained}/{epochs + epochs_previously_trained}] val loss: {total_epoch_val_loss:.2f} time: {elapsed:.2f}s')\n",
        "    if SEED not in total_epochs.keys():\n",
        "        total_epochs[SEED] = {}\n",
        "    total_epochs[SEED][name] = epochs + epochs_previously_trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "hhON5TP-qdcV"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loss_function, dataloader, padding_value=PAD_INDEX,  verbose=True):\n",
        "    \"\"\"\n",
        "        Evaluation function for the model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for sentences, pos, s_len in dataloader:\n",
        "        tensor_sentences = [torch.LongTensor(s) for s in sentences]\n",
        "        tensor_pos = [torch.LongTensor(p) for p in pos]\n",
        "\n",
        "        padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = padding_value)\n",
        "        padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n",
        "\n",
        "        predicted = model(padded_sentences, s_len)\n",
        "        predicted = predicted.view(-1,predicted.shape[-1])\n",
        "        targets = padded_pos.view(-1)\n",
        "\n",
        "        predicted, targets = reshape_and_mask(predicted, targets)\n",
        "\n",
        "        loss = loss_function(predicted, targets)\n",
        "\n",
        "        predicted = predicted.argmax(dim=1)\n",
        "\n",
        "        tot_pred = torch.cat((tot_pred,predicted))\n",
        "        tot_targ = torch.cat((tot_targ,targets))\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    full_accuracy, full_f1 = accuracy_and_f1(tot_pred,tot_targ)\n",
        "\n",
        "    if verbose: print(f'Eval: loss: {epoch_loss:.2f} accuracy: {full_accuracy:.2f} f1: {full_f1:.2f}')\n",
        "\n",
        "    return full_accuracy, full_f1, tot_pred, tot_targ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "4AxSUD_LqdcV"
      },
      "outputs": [],
      "source": [
        "def load_best_model(model, name):\n",
        "    \"\"\"\n",
        "        Loads the best model for the given seed and the given model name\n",
        "    \"\"\"\n",
        "    model.load_state_dict(best_models[SEED][name])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "gx4R4GqLqdcV"
      },
      "outputs": [],
      "source": [
        "def save_model(model, base_name):\n",
        "    \"\"\"\n",
        "        Saves the model in a file\n",
        "    \"\"\"\n",
        "    name = base_name + \"_\" + str(SEED)\n",
        "    name += \"_lower\" * LOWER\n",
        "    name += \"_number\" * NUMBER\n",
        "    name += \"_ner\" * NER\n",
        "    name += f'{OOV_EMBEDDING_TYPE}_{MEAN_EMBEDDING_WINDOW}' * (OOV_EMBEDDING_TYPE == 'mean')\n",
        "    name += \"_fixed_oov\" * FIXED_OOV\n",
        "    name += \".pt\"\n",
        "    path = os.path.join(WEIGHTS_FOLDER, name)\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_model(model, name):\n",
        "    \"\"\"\n",
        "        Loads the model from a file\n",
        "    \"\"\"\n",
        "    path = os.path.join(WEIGHTS_FOLDER, name)\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "hbF51IMSqdcV"
      },
      "outputs": [],
      "source": [
        "LSTM_DIMENSION = 16\n",
        "DENSE_DIMENSION = len(unique_tags) + 1\n",
        "INITIAL_LEARNING_RATE = 0.01\n",
        "LR_DECAY_FACTOR = 0.1\n",
        "LR_DECAY_PATIENCE = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5VTbyfJqdcV"
      },
      "source": [
        "Baseline model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "oqz5g46RqdcV"
      },
      "outputs": [],
      "source": [
        "loss_function_baseline = CrossEntropyLoss()\n",
        "baseline_model = Baseline(LSTM_DIMENSION, DENSE_DIMENSION)\n",
        "optimizer_baseline = Adam(baseline_model.parameters(), lr=INITIAL_LEARNING_RATE)\n",
        "scheduler_baseline = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_baseline, mode='min', factor=LR_DECAY_FACTOR, patience=LR_DECAY_PATIENCE, verbose=True)\n",
        "baseline_model.apply(initialize_weights);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmThhKzyqdcV"
      },
      "source": [
        "Double LSTM model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BSXzv3OnqdcW"
      },
      "outputs": [],
      "source": [
        "loss_double_lstm = CrossEntropyLoss()\n",
        "double_lstm_model = Model1(LSTM_DIMENSION, DENSE_DIMENSION)\n",
        "optimizer_double_lstm = Adam(double_lstm_model.parameters(), lr=INITIAL_LEARNING_RATE)\n",
        "scheduler_double_lstm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_double_lstm, mode='min', factor=LR_DECAY_FACTOR, patience=LR_DECAY_PATIENCE, verbose=True)\n",
        "double_lstm_model.apply(initialize_weights);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYiljd4pqdcW"
      },
      "source": [
        "Double Dense model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "k737UyulqdcW"
      },
      "outputs": [],
      "source": [
        "loss_double_dense = CrossEntropyLoss()\n",
        "double_dense_model = Model2(LSTM_DIMENSION, DENSE_DIMENSION)\n",
        "optimizer_double_dense = Adam(double_dense_model.parameters(), lr=INITIAL_LEARNING_RATE)\n",
        "scheduler_double_dense = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_double_dense, mode='min', factor=LR_DECAY_FACTOR, patience=LR_DECAY_PATIENCE, verbose=True)\n",
        "double_dense_model.apply(initialize_weights);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWKGS73yqdcW",
        "outputId": "ce4e82c3-6a05-4401-f8ae-44a3632dfdd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in baseline model: 42222\n",
            "Number of parameters in double lstm model: 48622\n",
            "Number of parameters in double dense model: 44384\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of parameters in baseline model: {number_parameters(baseline_model)}')\n",
        "print(f'Number of parameters in double lstm model: {number_parameters(double_lstm_model)}')\n",
        "print(f'Number of parameters in double dense model: {number_parameters(double_dense_model)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e6QALPPqdcX",
        "outputId": "86364ac2-0147-4427-a6c3-453a0902fa7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train epoch [1/20] val loss: 32.03 time: 8.73s\n",
            "Train epoch [2/20] val loss: 23.66 time: 7.69s\n",
            "Train epoch [3/20] val loss: 21.13 time: 8.07s\n",
            "Train epoch [4/20] val loss: 19.96 time: 9.51s\n",
            "Train epoch [5/20] val loss: 19.82 time: 7.40s\n",
            "Train epoch [6/20] val loss: 20.26 time: 7.38s\n",
            "Train epoch [7/20] val loss: 20.43 time: 8.10s\n",
            "Epoch 00008: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Train epoch [8/20] val loss: 20.97 time: 9.10s\n",
            "Train epoch [9/20] val loss: 20.64 time: 6.88s\n",
            "Train epoch [10/20] val loss: 20.78 time: 6.43s\n",
            "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Train epoch [11/20] val loss: 20.62 time: 6.59s\n",
            "Train epoch [12/20] val loss: 20.77 time: 6.37s\n",
            "Train epoch [13/20] val loss: 20.56 time: 6.48s\n",
            "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Train epoch [14/20] val loss: 20.60 time: 6.58s\n",
            "Train epoch [15/20] val loss: 20.91 time: 6.65s\n",
            "Train epoch [16/20] val loss: 20.86 time: 6.39s\n",
            "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Train epoch [17/20] val loss: 20.66 time: 6.73s\n",
            "Train epoch [18/20] val loss: 20.73 time: 6.48s\n",
            "Train epoch [19/20] val loss: 20.83 time: 6.30s\n",
            "Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Train epoch [20/20] val loss: 20.57 time: 6.69s\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_BASELINE = 20\n",
        "train(baseline_model, EPOCHS_BASELINE, loss_function_baseline, tr_dl, val_dl, optimizer_baseline, scheduler_baseline, 'baseline')\n",
        "best_baseline_model = load_best_model(baseline_model, 'baseline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ73tOYoqdcX",
        "outputId": "b0baf675-5840-41cd-873d-18f3345b7e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train epoch [1/20] val loss: 51.89 time: 12.63s\n",
            "Train epoch [2/20] val loss: 28.81 time: 12.44s\n",
            "Train epoch [3/20] val loss: 23.97 time: 12.28s\n",
            "Train epoch [4/20] val loss: 22.40 time: 12.22s\n",
            "Train epoch [5/20] val loss: 21.34 time: 13.07s\n",
            "Train epoch [6/20] val loss: 21.27 time: 13.49s\n",
            "Train epoch [7/20] val loss: 21.91 time: 12.10s\n",
            "Train epoch [8/20] val loss: 21.06 time: 13.07s\n",
            "Train epoch [9/20] val loss: 22.21 time: 13.05s\n",
            "Train epoch [10/20] val loss: 23.09 time: 14.24s\n",
            "Epoch 00011: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Train epoch [11/20] val loss: 24.02 time: 12.00s\n",
            "Train epoch [12/20] val loss: 22.88 time: 11.68s\n",
            "Train epoch [13/20] val loss: 23.00 time: 11.55s\n",
            "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Train epoch [14/20] val loss: 23.00 time: 11.74s\n",
            "Train epoch [15/20] val loss: 23.28 time: 11.79s\n",
            "Train epoch [16/20] val loss: 22.96 time: 12.73s\n",
            "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Train epoch [17/20] val loss: 22.96 time: 12.11s\n",
            "Train epoch [18/20] val loss: 22.87 time: 14.81s\n",
            "Train epoch [19/20] val loss: 22.91 time: 13.51s\n",
            "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Train epoch [20/20] val loss: 22.97 time: 11.98s\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_DOUBLE_LSTM = 20\n",
        "train(double_lstm_model, EPOCHS_DOUBLE_LSTM, loss_double_lstm, tr_dl, val_dl, optimizer_double_lstm, scheduler_double_lstm, 'double_lstm')\n",
        "best_double_lstm_model = load_best_model(double_lstm_model, 'double_lstm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM06lQoQqdcX",
        "outputId": "b4817854-6700-4fc1-aabc-3b8696a3eb7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train epoch [1/20] val loss: 26.98 time: 6.99s\n",
            "Train epoch [2/20] val loss: 21.77 time: 8.33s\n",
            "Train epoch [3/20] val loss: 21.28 time: 7.77s\n",
            "Train epoch [4/20] val loss: 22.01 time: 7.14s\n",
            "Train epoch [5/20] val loss: 24.16 time: 7.28s\n",
            "Epoch 00006: reducing learning rate of group 0 to 1.0000e-03.\n",
            "Train epoch [6/20] val loss: 26.31 time: 6.81s\n",
            "Train epoch [7/20] val loss: 24.77 time: 7.13s\n",
            "Train epoch [8/20] val loss: 24.76 time: 7.55s\n",
            "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Train epoch [9/20] val loss: 24.81 time: 6.32s\n",
            "Train epoch [10/20] val loss: 25.07 time: 6.71s\n",
            "Train epoch [11/20] val loss: 24.80 time: 6.80s\n",
            "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Train epoch [12/20] val loss: 25.19 time: 6.45s\n",
            "Train epoch [13/20] val loss: 24.95 time: 7.80s\n",
            "Train epoch [14/20] val loss: 24.94 time: 6.70s\n",
            "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n",
            "Train epoch [15/20] val loss: 24.98 time: 6.36s\n",
            "Train epoch [16/20] val loss: 25.14 time: 6.65s\n",
            "Train epoch [17/20] val loss: 25.17 time: 6.39s\n",
            "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Train epoch [18/20] val loss: 25.28 time: 7.01s\n",
            "Train epoch [19/20] val loss: 25.42 time: 8.03s\n",
            "Train epoch [20/20] val loss: 25.18 time: 9.94s\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_DOUBLE_DENSE = 20\n",
        "train(double_dense_model, EPOCHS_DOUBLE_DENSE, loss_double_dense, tr_dl, val_dl, optimizer_double_dense, scheduler_double_dense, 'double_dense')\n",
        "best_double_dense_model = load_best_model(double_dense_model, 'double_dense')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Q53i6uJmqdcX",
        "outputId": "bc152e32-19c1-4c1e-b475-7655e62a48c7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>baseline</td>\n",
              "      <td>0.931195</td>\n",
              "      <td>0.755471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>double_lstm</td>\n",
              "      <td>0.932111</td>\n",
              "      <td>0.758547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>double_dense</td>\n",
              "      <td>0.929715</td>\n",
              "      <td>0.768535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          model  accuracy        f1\n",
              "0      baseline  0.931195  0.755471\n",
              "1   double_lstm  0.932111  0.758547\n",
              "2  double_dense  0.929715  0.768535"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_accuracy_val, baseline_f1_val, baseline_pred_val, baseline_targ_val = evaluate(best_baseline_model, loss_function_baseline, val_dl, verbose=False)\n",
        "double_lstm_accuracy_val, double_lstm_f1_val, double_lstm_pred_val, double_lstm_targ_val = evaluate(best_double_lstm_model, loss_double_lstm, val_dl, verbose=False)\n",
        "double_dense_accuracy_val, double_dense_f1_val, double_dense_pred_val, double_dense_targ_val = evaluate(best_double_dense_model, loss_double_dense, val_dl, verbose=False)\n",
        "\n",
        "\n",
        "results = pd.DataFrame(columns=['model','accuracy','f1'])\n",
        "results.loc[0] = ['baseline', baseline_accuracy_val.item(), baseline_f1_val]\n",
        "results.loc[1] = ['double_lstm', double_lstm_accuracy_val.item(), double_lstm_f1_val]\n",
        "results.loc[2] = ['double_dense', double_dense_accuracy_val.item(), double_dense_f1_val]\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "eBiqtx9KqdcX",
        "outputId": "969b18a9-5398-4945-d83c-9ac500642aee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>baseline</td>\n",
              "      <td>0.936622</td>\n",
              "      <td>0.851025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>double_lstm</td>\n",
              "      <td>0.934327</td>\n",
              "      <td>0.854943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>double_dense</td>\n",
              "      <td>0.930169</td>\n",
              "      <td>0.813856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          model  accuracy        f1\n",
              "0      baseline  0.936622  0.851025\n",
              "1   double_lstm  0.934327  0.854943\n",
              "2  double_dense  0.930169  0.813856"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_accuracy_test, baseline_f1_test, baseline_pred_test, baseline_targ_test = evaluate(best_baseline_model, loss_function_baseline, test_dl, verbose=False)\n",
        "double_lstm_accuracy_test, double_lstm_f1_test, double_lstm_pred_test, double_lstm_targ_test = evaluate(best_double_lstm_model, loss_double_lstm, test_dl, verbose=False)\n",
        "double_dense_accuracy_test, double_dense_f1_test, double_dense_pred_test, double_dense_targ_test = evaluate(best_double_dense_model, loss_double_dense, test_dl, verbose=False)\n",
        "\n",
        "\n",
        "results = pd.DataFrame(columns=['model','accuracy','f1'])\n",
        "results.loc[0] = ['baseline', baseline_accuracy_test.item(), baseline_f1_test]\n",
        "results.loc[1] = ['double_lstm', double_lstm_accuracy_test.item(), double_lstm_f1_test]\n",
        "results.loc[2] = ['double_dense', double_dense_accuracy_test.item(), double_dense_f1_test]\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Y4mcAHhHqdcY"
      },
      "outputs": [],
      "source": [
        "save_model(best_baseline_model, 'baseline')\n",
        "save_model(best_double_lstm_model, 'doublelstm')\n",
        "save_model(best_double_dense_model, 'doubledense')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sN60F5MO0mB"
      },
      "source": [
        "# [Task 6 - 1.0 points] Error Analysis\n",
        "\n",
        "You are tasked to evaluate your best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IvcbIwiO0mB"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Compare the errors made on the validation and test sets.\n",
        "* Aggregate model errors into categories (if possible)\n",
        "* Comment the about errors and propose possible solutions on how to address them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "VV9HZZLJqdcY"
      },
      "outputs": [],
      "source": [
        "def build_classification_report(targ,pred,unique_tags):\n",
        "    \"\"\"\n",
        "        Build classification report and prints it\n",
        "    \"\"\"\n",
        "    report = classification_report(targ,pred,zero_division=0,output_dict=False,target_names=unique_tags)\n",
        "    print(report)\n",
        "\n",
        "def build_confusion_matrix(targ,pred,unique_tags):\n",
        "    \"\"\"\n",
        "        Build confusion matrix, plot and returns it\n",
        "    \"\"\"\n",
        "    cf_matrix = confusion_matrix(targ, pred)\n",
        "    df_cm = pd.DataFrame(cf_matrix, index = unique_tags, columns = unique_tags)\n",
        "    plt.figure(figsize = (40,32))\n",
        "    sn.heatmap(df_cm, annot=True, cmap=\"Blues\", linewidths= 0.05, linecolor='white')\n",
        "    return df_cm\n",
        "\n",
        "def build_errors_dictionary(df_cm):\n",
        "    \"\"\"\n",
        "        Build errors dictionary and prints it\n",
        "    \"\"\"\n",
        "    errors = {}\n",
        "    for true_tag,row in df_cm.iterrows():    #loop on the rows of the dataframe\n",
        "\n",
        "        tag_errors = []\n",
        "        for pred_tag, occurrences in row.items() :     #loop on each column of that specific row\n",
        "            if not pred_tag==true_tag and occurrences!=0 :\n",
        "                tag_errors.append((pred_tag,occurrences))\n",
        "\n",
        "        tag_errors.sort(key = itemgetter(1),reverse=True)   #sort it so that the tag that is more mistaken for the correct one is the first one to appear on the left\n",
        "\n",
        "        if tag_errors:\n",
        "            errors[true_tag] = tag_errors     #put it in the dict only if there are actually errors\n",
        "\n",
        "    errors = dict(sorted(errors.items(), key = lambda item : item[1][0][1],reverse=True))    #sort the dictionary in order to have the more wrongly classified tags on top\n",
        "\n",
        "    #pretty print\n",
        "    print('true_TAG --> (pred_TAG, n_times)\\n')\n",
        "    for k,v in errors.items():\n",
        "        print(k,'-->',*v)\n",
        "\n",
        "def get_tag_distribution(indexed_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "        Count number of occurrences of each TAG in the train set\n",
        "    \"\"\"\n",
        "    tag_frequency = {}\n",
        "    df_temp = indexed_df[indexed_df['split'] == 'train']\n",
        "    for _ ,row in df_temp.iterrows():\n",
        "        for key in row['indexed_tags']:\n",
        "            tag_frequency[int2tag[key]] = tag_frequency.get(int2tag[key],0) + 1\n",
        "\n",
        "    return dict(sorted(tag_frequency.items(), key=lambda item: item[1], reverse = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "_jpwUoKoqdcY",
        "outputId": "fac4c457-5f13-4e78-b6e1-a8ff271821f7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'baseline_pred' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb Cell 66\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#Y256sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tags \u001b[39m=\u001b[39m [vocab\u001b[39m.\u001b[39mi2t(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(baseline_pred\u001b[39m.\u001b[39mtolist() \u001b[39m+\u001b[39m baseline_targ\u001b[39m.\u001b[39mtolist())]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#Y256sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_cm \u001b[39m=\u001b[39m build_confusion_matrix(baseline_targ,baseline_pred,tags)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#Y256sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m build_errors_dictionary(df_cm)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'baseline_pred' is not defined"
          ]
        }
      ],
      "source": [
        "tags = [vocab.i2t(i) for i in set(baseline_pred.tolist() + baseline_targ.tolist())]\n",
        "\n",
        "df_cm = build_confusion_matrix(baseline_targ,baseline_pred,tags)\n",
        "\n",
        "build_errors_dictionary(df_cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32F8eX_hO0mB"
      },
      "source": [
        "# [Task 7 - 1.0 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv4OVz1vO0mC"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkAPKCbIO0mC"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "449lJeI-O0mC"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
        "* You can upload **model weights** in a cloud repository and report the link in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adKOVVP-O0mC"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9faC6QnO0mC"
      },
      "source": [
        "### Trainable Embeddings\n",
        "\n",
        "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KML9gLHgO0mC"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "You **should not** change the architecture of a model (i.e., its layers).\n",
        "\n",
        "However, you are **free** to play with their hyper-parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cgg47FVO0mD"
      },
      "source": [
        "### Neural Libraries\n",
        "\n",
        "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qmgkNGsO0mD"
      },
      "source": [
        "### Keras TimeDistributed Dense layer\n",
        "\n",
        "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKxd8OlIO0mD"
      },
      "source": [
        "### Error Analysis\n",
        "\n",
        "Some topics for discussion include:\n",
        "   * Model performance on most/less frequent classes.\n",
        "   * Precision/Recall curves.\n",
        "   * Confusion matrices.\n",
        "   * Specific misclassified samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m5SkbO0O0mD"
      },
      "source": [
        "### Punctuation\n",
        "\n",
        "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
        "\n",
        "You should **ignore** it during metrics computation.\n",
        "\n",
        "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urd139anO0mD"
      },
      "source": [
        "# The End"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
