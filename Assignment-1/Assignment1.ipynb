{"cells":[{"cell_type":"markdown","metadata":{"id":"-WeCeITXoxLf"},"source":["# Assignment 1\n","\n","**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n","\n","**Keywords**: POS tagging, Sequence labelling, RNNs"]},{"cell_type":"markdown","metadata":{"id":"mEMlC0FYO0l2"},"source":["\n","# Contact\n","\n","For any doubt, question, issue or help, you can always contact us at the following email addresses:\n","\n","Teaching Assistants:\n","\n","* Federico Ruggeri -> federico.ruggeri6@unibo.it\n","* Eleonora Mancini -> e.mancini@unibo.it\n","\n","Professor:\n","\n","* Paolo Torroni -> p.torroni@unibo.it"]},{"cell_type":"markdown","metadata":{"id":"tDP-HaMpO0l3"},"source":["# Introduction\n","\n","You are tasked to address the task of POS tagging.\n","\n","<center>\n","    <img src=\"./images/pos_tagging.png\" alt=\"POS tagging\" />\n","</center>"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18918,"status":"ok","timestamp":1699279441314,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"DM-Jq2mLO6Nj","outputId":"052e880c-0294-4a1f-d0f0-fed00fdee923"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":7380,"status":"ok","timestamp":1699279473704,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"ykSh18iMPDEU"},"outputs":[],"source":["#!cp -rf /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/data ./\n","#!cp -rf /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/images ./\n","#!cp /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/data.csv ./"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seed = 0"]},{"cell_type":"markdown","metadata":{"id":"sr8QdeOXO0l4"},"source":["# [Task 1 - 0.5 points] Corpus\n","\n","You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n","\n","**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n","\n","### Example\n","\n","```Pierre\tNNP\t2\n","Vinken\tNNP\t8\n",",\t,\t2\n","61\tCD\t5\n","years\tNNS\t6\n","old\tJJ\t2\n",",\t,\t2\n","will\tMD\t0\n","join\tVB\t8\n","the\tDT\t11\n","board\tNN\t9\n","as\tIN\t9\n","a\tDT\t15\n","nonexecutive\tJJ\t15\n","director\tNN\t12\n","Nov.\tNNP\t9\n","29\tCD\t16\n",".\t.\t8\n","```"]},{"cell_type":"markdown","metadata":{"id":"Dd_jkm9hO0l5"},"source":["### Splits\n","\n","The corpus contains 200 documents.\n","\n","   * **Train**: Documents 1-100\n","   * **Validation**: Documents 101-150\n","   * **Test**: Documents 151-199"]},{"cell_type":"markdown","metadata":{"id":"4Hx0FbERO0l6"},"source":["### Instructions\n","\n","* **Download** the corpus.\n","* **Encode** the corpus into a pandas.DataFrame object.\n","* **Split** it in training, validation, and test sets."]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoding dataset as pandas dataframe...\n"]},{"name":"stdout","output_type":"stream","text":["Encoding completed!\n","Some words from the dataset: ['reds', 'berlin', '21-month', 'witches', '8.47', 'begins', 'nickel', 'wear', 'filled', 'prying', 'serves', 'certificates', 'judicial', 'pressured', 'patricia']\n","Some tags from the dataset: ['PDT', 'JJS', 'CD', 'FW', 'PRP', 'SYM', 'VBG', 'WP', 'VB', 'SYM', 'VBG', 'WP$', 'WDT', 'NNP', 'CC']\n","\n","encoded dataframe:\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import random\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","\n","data_folder = \"./data\"\n","def encode_dataset(dataset_name: str, to_lower: bool) -> pd.DataFrame:\n","  \"\"\"\n","    Takes the dataset and encodes it in a pandas dataframe having six columns ['split', 'doc_id', 'sentence_num', 'words', 'tags', 'num_tokens']. Computes also unique tags set and unique words set and returns them with the dataframe.\n","  \n","  \"\"\"\n","  print(\"Encoding dataset as pandas dataframe...\")\n","\n","  dataset_folder = os.path.join(data_folder+ \"/dataset\")\n","  \n","  dataframe_rows = []             #dataframe that will contain all the sentences in all the documents, each sentence as a list of word and a list of corresponding tags\n","  unique_tags = set()             \n","  unique_words = set()\n","\n","  for doc in os.listdir(dataset_folder):\n","    if doc.endswith(\".csv\") or doc.endswith(\".pkl\"): continue\n","    doc_num = int(doc[5:8])\n","    doc_path = os.path.join(dataset_folder,doc)\n","\n","    with open(doc_path, mode='r', encoding='utf-8') as file:\n","      df = pd.read_csv(file,sep='\\t',header=None,skip_blank_lines=False)\n","      df.rename(columns={0:'word',1:\"TAG\",2:\"remove\"},inplace=True)\n","      df.drop(\"remove\",axis=1,inplace=True)\n","\n","      if to_lower: df['word'] = df[\"word\"].str.lower() #set all words to lower case\n","      \n","      #create another column that indicate the group id by sentence \n","      df[\"group_num\"] = df.isnull().all(axis=1).cumsum()\n","      df.dropna(inplace=True)\n","      df.reset_index(drop=True, inplace=True)\n","      \n","      unique_tags.update(df['TAG'].unique())     #save all the unique tags in a set \n","      unique_words.update(df['word'].unique())   #save all the unique words in a set \n","\n","      #generate sentence list in a document \n","      df_list = [df.iloc[rows] for _, rows in df.groupby('group_num').groups.items()]\n","      for n,d in enumerate(df_list) :           #for each sentence create a row in the final dataframe\n","          dataframe_row = {\n","              \"split\" : 'train' if doc_num<=100 else ('val' if doc_num<=150  else 'test'),\n","              \"doc_id\" : doc_num,\n","              \"sentence_num\" : n,\n","              \"words\": d['word'].tolist(),\n","              \"tags\":  d['TAG'].tolist(),\n","              \"num_tokens\": len(d['word'])\n","          }\n","          dataframe_rows.append(dataframe_row)\n","\n","  dataframe_path = os.path.join(data_folder, dataset_name)\n","  df_final = pd.DataFrame(dataframe_rows)\n","  df_final.to_csv(dataframe_path + \".csv\")                      #save as csv to inspect\n","\n","  print(\"Encoding completed!\")\n","    \n","  return  df_final, unique_tags, unique_words\n","\n","df, unique_tags, unique_words = encode_dataset(\"encoded_dataset\", to_lower = True)\n","\n","print('Some words from the dataset:', random.choices(list(unique_words),k=15))\n","print('Some tags from the dataset:', random.choices(list(unique_tags),k=15))\n","\n","print('\\nencoded dataframe:')"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1699279510294,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"135YHdmjPj60","outputId":"c778bad9-720a-43f9-97ab-520cf4e345e6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>split</th>\n","      <th>doc_id</th>\n","      <th>sentence_num</th>\n","      <th>words</th>\n","      <th>tags</th>\n","      <th>num_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3249</th>\n","      <td>train</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n","      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>3250</th>\n","      <td>train</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>[mr., vinken, is, chairman, of, elsevier, n.v....</td>\n","      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>3376</th>\n","      <td>train</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>[rudolph, agnew, ,, 55, years, old, and, forme...</td>\n","      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>2243</th>\n","      <td>train</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>[although, preliminary, findings, were, report...</td>\n","      <td>[IN, JJ, NNS, VBD, VBN, RBR, IN, DT, NN, IN, ,...</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>2269</th>\n","      <td>train</td>\n","      <td>3</td>\n","      <td>29</td>\n","      <td>[it, has, no, bearing, on, our, work, force, t...</td>\n","      <td>[PRP, VBZ, DT, NN, IN, PRP$, NN, NN, NN, .]</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3494</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>9</td>\n","      <td>[in, a, second, area, of, common, concern, ,, ...</td>\n","      <td>[IN, DT, JJ, NN, IN, JJ, NN, ,, DT, NN, NN, ,,...</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>3485</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>0</td>\n","      <td>[a, house-senate, conference, approved, major,...</td>\n","      <td>[DT, NNP, NN, VBD, JJ, NNS, IN, DT, NN, IN, JJ...</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>3489</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>4</td>\n","      <td>[these, fiscal, pressures, are, also, a, facto...</td>\n","      <td>[DT, JJ, NNS, VBP, RB, DT, NN, IN, VBG, DT, NN...</td>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>3490</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>5</td>\n","      <td>[to, accommodate, the, additional, cash, assis...</td>\n","      <td>[TO, VB, DT, JJ, NN, NN, ,, DT, NNP, NNPS, NNP...</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>3486</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>1</td>\n","      <td>[for, the, agency, for, international, develop...</td>\n","      <td>[IN, DT, NNP, IN, NNP, NNP, ,, NNS, VBD, $, CD...</td>\n","      <td>51</td>\n","    </tr>\n","    <tr>\n","      <th>764</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>2</td>\n","      <td>[mr., ackerman, already, is, seeking, to, oust...</td>\n","      <td>[NNP, NNP, RB, VBZ, VBG, TO, VB, NNP, NNP, IN,...</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>763</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>1</td>\n","      <td>[the, move, boosts, intelogic, chairman, asher...</td>\n","      <td>[DT, NN, VBZ, NNP, NNP, NNP, NNP, POS, NN, TO,...</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>762</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>0</td>\n","      <td>[intelogic, trace, inc., ,, san, antonio, ,, t...</td>\n","      <td>[NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>765</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>3</td>\n","      <td>[the, action, followed, by, one, day, an, inte...</td>\n","      <td>[DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>766</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>4</td>\n","      <td>[in, new, york, stock, exchange, composite, tr...</td>\n","      <td>[IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...</td>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      split  doc_id  sentence_num  \\\n","3249  train       1             0   \n","3250  train       1             1   \n","3376  train       2             0   \n","2243  train       3             3   \n","2269  train       3            29   \n","3494    val     101             9   \n","3485    val     101             0   \n","3489    val     101             4   \n","3490    val     101             5   \n","3486    val     101             1   \n","764    test     151             2   \n","763    test     151             1   \n","762    test     151             0   \n","765    test     151             3   \n","766    test     151             4   \n","\n","                                                  words  \\\n","3249  [pierre, vinken, ,, 61, years, old, ,, will, j...   \n","3250  [mr., vinken, is, chairman, of, elsevier, n.v....   \n","3376  [rudolph, agnew, ,, 55, years, old, and, forme...   \n","2243  [although, preliminary, findings, were, report...   \n","2269  [it, has, no, bearing, on, our, work, force, t...   \n","3494  [in, a, second, area, of, common, concern, ,, ...   \n","3485  [a, house-senate, conference, approved, major,...   \n","3489  [these, fiscal, pressures, are, also, a, facto...   \n","3490  [to, accommodate, the, additional, cash, assis...   \n","3486  [for, the, agency, for, international, develop...   \n","764   [mr., ackerman, already, is, seeking, to, oust...   \n","763   [the, move, boosts, intelogic, chairman, asher...   \n","762   [intelogic, trace, inc., ,, san, antonio, ,, t...   \n","765   [the, action, followed, by, one, day, an, inte...   \n","766   [in, new, york, stock, exchange, composite, tr...   \n","\n","                                                   tags  num_tokens  \n","3249  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...          18  \n","3250  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...          13  \n","3376  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...          26  \n","2243  [IN, JJ, NNS, VBD, VBN, RBR, IN, DT, NN, IN, ,...          35  \n","2269        [PRP, VBZ, DT, NN, IN, PRP$, NN, NN, NN, .]          10  \n","3494  [IN, DT, JJ, NN, IN, JJ, NN, ,, DT, NN, NN, ,,...          43  \n","3485  [DT, NNP, NN, VBD, JJ, NNS, IN, DT, NN, IN, JJ...          44  \n","3489  [DT, JJ, NNS, VBP, RB, DT, NN, IN, VBG, DT, NN...          39  \n","3490  [TO, VB, DT, JJ, NN, NN, ,, DT, NNP, NNPS, NNP...          26  \n","3486  [IN, DT, NNP, IN, NNP, NNP, ,, NNS, VBD, $, CD...          51  \n","764   [NNP, NNP, RB, VBZ, VBG, TO, VB, NNP, NNP, IN,...          19  \n","763   [DT, NN, VBZ, NNP, NNP, NNP, NNP, POS, NN, TO,...          30  \n","762   [NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...          40  \n","765   [DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...          34  \n","766   [IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...          20  "]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["df.sort_values(\"doc_id\").groupby('split').head()"]},{"cell_type":"code","execution_count":67,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1699279519522,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"cNjtmOwZOdPh"},"outputs":[{"name":"stdout","output_type":"stream","text":["saving dictionaries as pickle files\n"]}],"source":["from collections import OrderedDict\n","import pickle\n","\n","dict_path = os.path.join(data_folder,'dictionaries.pkl') #path where dictionaries will be saved \n","\n","def build_dict(words : list[str], tags : list[str]): \n","    \"\"\"\n","        Builds 4 dictionaries word2int, int2word, tag2int, int2tag and returns them\n","    \"\"\"\n","    \n","    word2int = OrderedDict()\n","    int2word = OrderedDict()\n","\n","    for i, word in enumerate(words):\n","        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n","        int2word[i+1] = word\n","\n","    tag2int = OrderedDict()\n","    int2tag = OrderedDict()\n","\n","    for i, tag in enumerate(tags):\n","        tag2int[tag] = i+1\n","        int2tag[i+1] = tag\n","    \n","    print('saving dictionaries as pickle files')\n","    pickle_files = [word2int,int2word,tag2int,int2tag]\n","    \n","    with open(dict_path, 'wb') as f:\n","        pickle.dump(pickle_files, f)\n","\n","    return word2int,int2word,tag2int,int2tag\n","\n","word2int,int2word,tag2int,int2tag = build_dict(unique_words,unique_tags)"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initiating numberization of words and tags in dataframe\n","Numberization completed\n","\n","All right with dataset numberization\n","Saving indexed dataframe\n"]}],"source":["indexed_df_path = os.path.join(data_folder, \"indexed_dataset.pkl\") #numberized dataframe path\n","\n","def build_indexed_dataframe(word2int, tag2int, df):\n","    \"\"\"\n","        Given the dictionaries word2int, tag2int and the dataframe, creates a dataframe were every word and tag is represented by its number and returns it\n","    \"\"\"\n","    print('Initiating numberization of words and tags in dataframe')\n","    indexed_rows = []\n","    for words,tags in zip(df['words'],df['tags']):\n","        indexed_row = {'indexed_words':[word2int[word] for word in words ],'indexed_tags':[tag2int[tag] for tag in tags ]}\n","        indexed_rows.append(indexed_row)\n","    \n","    indexed_df = pd.DataFrame(indexed_rows)\n","\n","    indexed_df.insert(0,'split',df['split'])\n","    indexed_df.insert(1,'num_tokens',df['num_tokens'])\n","\n","    print('Numberization completed')\n","\n","    return indexed_df\n","\n","\n","def check_dataframe_numberization(indexed_df, normal_df, int2word, int2tag) :\n","    \"\"\"\n","       Checks if the numberized dataframe will lead to the normal dataframe usind the dictionaries int2word and int2tag\n","    \"\"\"\n","    for n, (w_t, t_t) in enumerate(zip(indexed_df['indexed_words'],indexed_df['indexed_tags'])):\n","        if not normal_df.loc[n,'words'] == [int2word[indexed_word] for indexed_word in w_t]:\n","            print('words numberization gone wrong') \n","            return False\n","        if not normal_df.loc[n,'tags'] == [int2tag[indexed_tag] for indexed_tag in t_t]:\n","            print('tags numberization gone wrong')\n","            return False \n","    \n","    print('\\nAll right with dataset numberization')\n","    print('Saving indexed dataframe')\n","    \n","    indexed_df.to_pickle(indexed_df_path)\n","\n","\n","indexed_df = build_indexed_dataframe(word2int,tag2int,df)\n","check_dataframe_numberization(indexed_df,df, int2word, int2tag)\n"]},{"cell_type":"markdown","metadata":{"id":"MQvHxKRcO0l8"},"source":["# [Task 2 - 0.5 points] Text encoding\n","\n","To train a neural POS tagger, you first need to encode text into numerical format."]},{"cell_type":"markdown","metadata":{"id":"0EnUuOWTO0l9"},"source":["### Instructions\n","\n","* Embed words using **GloVe embeddings**.\n","* You are **free** to pick any embedding dimension.\n","* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206458,"status":"ok","timestamp":1699279728592,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"eVsfe-QKO0l9","outputId":"15f823d9-6f5c-47dc-c24d-cbd4f892bdac"},"outputs":[],"source":["import torch\n","from torchtext.vocab import GloVe\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","embedding_dimension = 300\n","\n","glove_embeddings = GloVe(name='6B', dim=embedding_dimension)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving embedding matrix\n","Embedding matrix shape: (10948, 300)\n"]}],"source":["def build_embedding_matrix(emb_model, word2int):\n","    \"\"\"\n","        Given the embedding model and the dict. word2int. If there is the embedding for the word, we add it to the embedding_matrix. In negative case we put a list of random values.\n","        Return the embedding matrix\n","    \"\"\"\n","    #check_value_distribution_glove(emb_model)\n","   \n","    embedding_dimension = len(emb_model[0]) #how many numbers each emb vector is composed of                                                           \n","    embedding_matrix = np.zeros((len(word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n","\n","    for word, idx in word2int.items():\n","        if word in emb_model.stoi:\n","            embedding_matrix[idx] = emb_model[word]\n","        else:\n","            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n","            \n","    print('Saving embedding matrix')\n","    path = os.path.join(data_folder, \"emb_matrix\")\n","    np.save(path,embedding_matrix,allow_pickle=True)\n","\n","    print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n","\n","    return embedding_matrix\n","\n","embedding_matrix = build_embedding_matrix(glove_embeddings, word2int)"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading embedding matrix\n","Loading numberized dataset\n","Loading dictionaries\n","All data loaded\n"]}],"source":["def load_data():\n","    \"\"\"\n","        Loads the data \"emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag \" and returns them\n","    \"\"\"\n","    emb_matrix_path = os.path.join(data_folder,'emb_matrix.npy')\n","    indexed_dataset_path = os.path.join(data_folder,'indexed_dataset.pkl')\n","    dictionaries_path = os.path.join(data_folder,'dictionaries.pkl')\n","\n","    if os.path.exists(emb_matrix_path) and os.path.exists(indexed_dataset_path):\n","        print('Loading embedding matrix')\n","        emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n","        print('Loading numberized dataset')\n","        indexed_dataset = pd.read_pickle(indexed_dataset_path)\n","        print('Loading dictionaries')\n","        with open(dictionaries_path, 'rb') as f:\n","            word2int,int2word,tag2int,int2tag = pickle.load(f)\n","        \n","        print('All data loaded')\n","    else:\n","        print('What you are looking for is not present in the folder')\n","        emb_matrix, indexed_dataset = None, None\n","\n","    return emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag\n","\n","emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag = load_data()"]},{"cell_type":"markdown","metadata":{"id":"N4AChgkfO0l_"},"source":["# [Task 3 - 1.0 points] Model definition\n","\n","You are now tasked to define your neural POS tagger."]},{"cell_type":"markdown","metadata":{"id":"jkERwLU7O0l_"},"source":["### Instructions\n","\n","* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n","* You are **free** to experiment with hyper-parameters to define the baseline model.\n","\n","* **Model 1**: add an additional LSTM layer to the Baseline model.\n","* **Model 2**: add an additional Dense layer to the Baseline model.\n","\n","* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n","\n","**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."]},{"cell_type":"markdown","metadata":{"id":"babZmAQlWGIw"},"source":["### Embedding layer"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def create_emb_layer(weights_matrix, pad_idx = 0):\n","    \"\"\"\n","        Creates and returns the embedding layer\n","    \"\"\"\n","    matrix = torch.Tensor(weights_matrix)   #the embedding matrix \n","    _ , embedding_dim = matrix.shape \n","    emb_layer = nn.Embedding.from_pretrained(matrix, freeze=True, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable \n","    return emb_layer, embedding_dim"]},{"cell_type":"markdown","metadata":{},"source":["### Baseline model"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"RSGVLm3iWF2a"},"outputs":[],"source":["class Baseline(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer(packed_sentences)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output\n","\n","        "]},{"cell_type":"markdown","metadata":{"id":"Zd-pJQyu6h-V"},"source":["### Model 1"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"4pOy7gXR62UC"},"outputs":[],"source":["class Model1(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer_1 = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.bidirectional_layer_2 = nn.LSTM(bidirectional=True, input_size=lstm_dimension*2, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer_1(packed_sentences)\n","        packed_output, _ = self.bidirectional_layer_2(packed_output)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"BY9Vy9997y5Q"},"source":["### Model 2"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"_RM739C970T5"},"outputs":[],"source":["class Model2(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer_1 = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.dense_layer_2 = nn.Linear(in_features=dense_dimension, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer(packed_sentences)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer_1(output)\n","        output = self.dense_layer_2(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"p9vgsKiaO0mA"},"source":["# [Task 4 - 1.0 points] Metrics\n","\n","Before training the models, you are tasked to define the evaluation metrics for comparison."]},{"cell_type":"markdown","metadata":{"id":"q_T6Bm1fO0mA"},"source":["### Instructions\n","\n","* Evaluate your models using macro F1-score, compute over **all** tokens.\n","* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n","* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"]},{"cell_type":"markdown","metadata":{"id":"NpoB8xTRO0mA"},"source":["**Note**: What about OOV tokens?\n","   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n","   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n","   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"Xx7AKsk69Zcw"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def accuracy_and_f1(y_pred, y_true):\n","    correct = y_pred.eq(y_true)          \n","    acc = correct.sum()/y_true.shape[0] \n","    f1 = f1_score(y_true,y_pred,average='macro')\n","    return acc,f1"]},{"cell_type":"markdown","metadata":{},"source":["### Let's check our OOV tokens"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of unique words in dataset: 10947\n","['old-house', 'hallwood', 'tarwhine', 'rubinfien', '13.625', 'college-bowl', '30,841', '278.7', 'bank-backed', 'stock-manipulation', 'car-development', 'pro-iranian', 'nih-appointed', 'index-options', '2645.90', 'exxon-owned', 'asset-sale', 'mutchin', 'training-wage', 'lezovich', 'amphobiles', 'new-car', 're-thought', 'superpremiums', 'lynch-mob', 'rate-sensitive', '341.20', 'radio-station', 'money-center', '494.50', 'corton-charlemagne', '14\\\\/32', 'much-larger', '434.4', 'muscolina', 'yoshihashi', '967,809', 'enzor', 'iran\\\\/contra', '5.2180', '87-store', 'auto-safety', '23,403', 'bronces', 'twin-jet', 'above-market', '319.75', 'safe-deposit', '8.575', 'erbamont', '226,570,380', 'wheeland', '120-a-share', '1.5755', 'savers\\\\/investors', 'when-issued', 'derel', 'nearly-30', 'year-ago', 'romanee-conti', '220.45', 'index-fund', 'constitutional-law', 'million-a-year', 'investor-relations', '126.15', '14.', '236.74', '-lcb-', 'long-tenured', 'we-japanese', 'church-goers', 'scypher', 'boogieman', 'food-shop', 'anti-programmers', 'prize-fighter', 'pennview', 'arbitrage-related', 'odd-sounding', 'market-makers', 'deposits-a', 'sometimes-tawdry', 'yen-support', 'abortion-related', 'colonsville', 'odd-year', 'waymar', 'conn.based', '2\\\\/32', 'malizia', 'sino-u.s.', 'identity-management', 'trockenbeerenauslesen', 'prior-year', 'crane-safety', 'hummerstone', '-rrb-', 'txo', 'nipponese', 'junk-bond', '520-lawyer', 'double-c', 'tire-kickers', 'money-fund', '35500.64', 'buy-outs', '69-point', '11,762', '158,666', 'school-board', 'certin', 'less-than-brilliant', 'truth-in-lending', '3.253', 'ednie', 'meinders', 'drobnick', '7.458', 'quantitive', 'diceon', 'test-practice', 'nissho-iwai', 'sport-utility', 'stock-index', 'post-hearing', 'more-advanced', 'monchecourt', 'lobsenz', '2,050-passenger', 'teacher-cadet', 'collective-bargaining', 'unicorp', 'superdot', '8300s', 'beer-belly', 'cray-3', 'two-sevenths', 'program-trading', '263.07', 'ballantine\\\\/del', 'flim-flammery', '143.08', 'purepac', 'wamre', 'detective-story', '62%-owned', 'c.j.b.', 'takeover-stock', 'language-housekeeper', 'pricings', '143.93', '237-seat', '630.9', '500,004', '-lrb-', 'uzi-model', 'test-preparation', 'summer\\\\/winter', '7\\\\/8', 'telephone-information', 'secilia', 'inter-tel', 'chemplus', 'cup-tote', 'financial-services', 'dollar-yen', 'hasbrouk', 'larger-than-normal', '40-megabyte', '6\\\\/2', '71,309', 'sewing-machine', 'property\\\\/casualty', '18-a-share', 'veraldi', '3436.58', 'wheel-loader', '737.5', '170,262', 'cash-rich', 'two-time-losers', 'anku', 'price-support', 'ctbs', 'bald-faced', 'more-efficient', 'product-design', 'anti-program', 'retin-a', 'arighi', 'sticker-shock', 'hart-scott-rodino', 'acid-rain', 'per-share', '9,118', '100,980', 'sanderoff', 'seven-yen', 'several-year', 'stock-picking', 'test-drive', 'year-earlier', 'heebie-jeebies', 'subindustry', '45-a-share', 'savings-and-loan', 'bread-and-butter', 'disputada', 'asset-valuation', 'disaster-assistance', '436.01', 'capital-gains', 'most-likely-successor', 'cost-control', '1937-40', 'sacramento-based', '608,413', 'shirt-sleeved', 'motor-home', '1.1650', 'video-viewing', '84-month', 'futures-related', 'anti-miscarriage', 'cash-and-stock', '446.62', 'incentive-bonus', 'red-blooded', 'tete-a-tete', '271,124', '1.457', 'lookee-loos', 'computer-services', '618.1', 'express-buick', '361.8', 'buttoned-down', 'melt-textured', 'electrical-safety', 'crocidolite', 'bumkins', 'shareholder-rights', '131.01', 'circuit-board', 'triple-a-rated', 'crystal-lattice', 'sharedata', 'price-depressing', 'ensrud', 'derchin', 'jalaalwalikraam', '352.7', 'flightiness', 'page-one', '7\\\\/16', 'manmade-fiber', 'rey\\\\/fawcett', 'polyproplene', 'computer-system-design', 'write-downs', '382-37', 'direct-investment', 'wtd', 'senate-house', 'interleukin-3', 'school-district', 'launch-vehicle', 'pathlogy', 'bermuda-based', 'mouth-up', 'dead-eyed', 'shokubai', '16,072', 'index-arbitrage', '7.422', 'non-callable', 'wine-buying', 'contingency-fee', 'potables', 'norwick', '-rcb-', '83,206', 'mutual-fund', 'clean-air', 'non-biodegradable', 'low-ball', 'pramual', 'delwin', 'one-newspaper', 'securities-based', '143.80', 'highest-pitched', 'floating-rate', '29year', 'preparatives', 'diloreto', 'credit-rating', 'three-sevenths', '90-cent-an-hour', '1\\\\/4', 'severable', 'intellectual-property', 'garden-variety', 'red-flag', 'big-ticket', 'lower-priority', 'chafic', 'state-supervised', '415.6', 'freshbake', '1\\\\/10th', 'thin-lipped', 'less-serious', 'freudtoy', 'custom-chip', 'twindam', 'high-rolling', '1.8415', 'one-upsmanship', '300-day', 'nekoosa', 'disputado', 'investment-grade', 'unenticing', 'one-house', 'early-retirement', 'life-of-contract', '62.625', 'bottom-line', 'fiber-end', 'light-truck', 'akerfeldt', 'samnick', 'johnson-era', 'biondi-santi', 'g.m.b', 'egnuss', 'six-bottle', '271-147', 'copper-rich', 'small-company', 'newspaper-printing', 'index-related', '26,956', 'labor-backed', 'automotive-parts', 'satrum', 'sell-offs', 'bell-ringer', 'macmillan\\\\/mcgraw', '19-month-old', 'times-stock', 'equal-opportunity', '230-215', 'dust-up', 'guber\\\\/peters', 'chinchon', 'solaia', 'phacoflex', 'high-balance', 'mariotta', '47.125', '1206.26', 'anti-takeover', 'trading-company', 'vinken', 'jerritts', '190-point', '36-store', 'ex-dividend', 'recession-inspired', 'staff-reduction', 'corn-buying', 'propagandizes', 'moleculon', '7.272', 'antitrust-law', 'dydee', '3648.82', '35564.43', 'tiphook', '30,537', '70-a-share', 'computer-driven', 'landonne', 'yet-to-be-formed', 'ghkm', 'insurance-company', 'parts-engineering', 'arbitraging', '12,252', '1\\\\/2', '14,821', '34.625', 'c-90', 'super-absorbent', 'intelogic', 'subskills', '95,142', '497.34', 'macmillan\\\\/mcgraw-hill', '877,663', 'merger-related', '16\\\\/32', 'nofzinger', 'automotive-lighting', 'minimum-wage', 'hadson', 'eight-count', 'c.d.s', 'cop-killer', 'pattenden', 'revenue-desperate', 'lentjes', 'fetal-tissue', 'house-senate', 'built-from-kit', 'water-authority', '9\\\\/32', 'capital-markets', 'heiwado', 'school-research', 'minicrash', 'pension-fund', 'foreign-stock', '100-megabyte', '37-a-share', '6,799', 'higher-salaried', '5.435', '95.09', 'student-test', 'near-limit', 'rapanelli', '292.32', 'nylev', '50\\\\/50', 'pre-1933', '38.875', 'waertsilae', 'low-ability', 'univest', 'security-type', 'n.v', '5.276', '300-a-share', 'three-lawyer', 'energy-services', 'free-enterprise', '3,288,453', 'seven-million-ton', 'new-home', 'pre-1917', 'acquisition-minded', 'airline-related', '3057', 'sogo-shosha', '3,250,000', 'integra-a', '13\\\\/16', 'veselich', 'building-products', '2141.7', '3\\\\/8', 'vitulli', '16.125', 'news-american', 'industrial-production', 'stock-specialist', 'sub-segments', 'reupke', 'card-member', '13,056', '811.9', 'lightning-fast', '238,000-circulation', '12\\\\/32', 'limited-partnership', 'bellringers', 'makato', 'passenger-car', 'government-certified', 'rope-sight', 'blood-cell', '4.898', 'intecknings', 'morale-damaging', 'high-polluting', 'citizen-sparked', 'sulfur-dioxide', '188.84', 'alurralde', 'aslacton', '127.03', '234.4', '2160.1', 'subminimum', '43.875', 'multi-crystal', '2163.2', 'macheski', 'continuingly', '361,376', 'northy', 'information-services', 'one-yen', '1\\\\/8', 'mortgage-based', '734.9', 'food-industry', 'protein-1', 'ingersoll-rand', '1.916', '2,303,328', 'test-prep', 'car-safety', 'pianist-comedian', 'insider-trading', 'coche-dury', '82,389', 'single-lot', '236.79', '8.467', 'ntg', 'chong-sik', '142.84', 'side-crash', '3\\\\/4', 'headcount-control', '11,390,000', '352.9', 'triple-c', '1.637', '456.64', 'corporate-wide', 'home-market', 'weapons-modernization', '0.0085', 'incentive-backed', 'yen-denominated', 'rexinger', 'tissue-transplant', 'durable-goods', '415.8', 'prudential-bache', 'gingl', 'replacement-car', '5,699', 'pro-forma', 'bridgestone\\\\/firestone', 'search-and-seizure', 'change-ringing', 'midwesco', 'marketing-communications', 'mega-stadium', 'test-coaching', '129.91', '372.14', 'cents-a-unit', 'purhasing', 'then-speaker', 'executive-office', 'trettien', 'lafite-rothschild', 'middle-ground', 'kalipharma', 'ratners', '374.19', 'sub-markets', 'blue-chips', 'ft-se', 'wfrr', 'four-foot-high', 'nagymaros', 'money-market', 'greenmailer', 'crookery', 'boorse', '20-stock', 'sidak', 'circuit-breaker', '705.6', 'profit-taking', '22\\\\/32', 'housing-assistance', 'ac-130u', 'social-studies', 'unfair-trade', '3,040,000', 'six-packs', 'co-developers', 'heavy-truck', 'anti-abortionists', 'louisiana-pacific', 'walbrecher', 'top-yielding', 'one-country', 'drag-down', 'car-care', 'non-encapsulating', 'band-wagon', '377.60', 'mehrens', 'anti-deficiency', '18,444', 'weisfield', 'achievement-test', 'market-share', '5\\\\/8', '372.9', '374.20', 'electric-utility', '300-113', 'forest-products', '2003\\\\/2007', 'colorliner', \"creator's\", '11-month-old', '11\\\\/16', 'micronite', 'family-planning', 'foreign-led', 'lap-shoulder', 'forest-product', 'noncompetitively', 'autions', '566.54', 'mininum-wage', 'industry-supported', 'herald-american', 'centerbank', '4,393,237', 'cotran', 'land-idling', 'stirlen', '387.8', 'ariail', '449.04', 'liquid-nitrogen', '1928-33', 'stock-price', '1738.1', 'anti-china', '55-a-share', '154,240,000', 'front-seat', 'yeargin', 'bankruptcy-law', 'synergistics', 'glenham', '38.375', 'cleaner-burning', 'machine-gun-toting', 'life-insurance', 'besuboru', 'foldability', 'breakey', 'school-improvement', 'high-rate', 'nesb', 'sept.30', '2691.19', 'sometimes-exhausting', 'yttrium-containing', 'roof-crush', 'anti-morning-sickness', 'subskill', 'war-rationed', 'gates-warren', 'chilver', 'equity-purchase', '142.85', 'stock-selection', 'substance-abusing', 'prevalance']\n","Total OOV terms: 676 (6.18%)\n","Some OOV terms: ['industry-supported', 'intellectual-property', 'stock-manipulation', 'triple-a-rated', 'two-sevenths', 'besuboru', 'mehrens', '38.375', '2645.90', '69-point', 'built-from-kit', '143.93', 'achievement-test', 'computer-system-design', '170,262']\n"]}],"source":["def check_OOV_terms(embedding_model, unique_words, lower):\n","    oov_words = []\n","    int_oov_words = []\n","\n","    if lower:\n","        words = set([x.lower() for x in unique_words])\n","    else: \n","        words = unique_words\n","\n","    for word in words:\n","        if word not in embedding_model.itos:\n","           oov_words.append(word) \n","           int_oov_words.append(word2int[word]) \n","    \n","    print(\"Total number of unique words in dataset:\",len(words))\n","    print(oov_words)\n","    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(words))*100))\n","    print(\"Some OOV terms:\",random.sample(oov_words,15))\n","    \n","    return oov_words, int_oov_words\n","\n","oov_words, int_oov_words = check_OOV_terms(glove_embeddings, unique_words,True)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see the number of OOV words is very high (31.29% of the total words in the corpus). So, we have to find a way to deal with them.<br>\n","A good idea could be to use lowercased words, in fact in this way the number of OOV decreases to 6.18%, but we have to be careful because we could lose some information. <br>\n","For example, the word \"Pierre\" is a name, but \"pierre\" is a noun. So, we have to find a way to deal with this problem.<br>\n","We decided to test the model with the two different approaches and see which one is better."]},{"cell_type":"markdown","metadata":{"id":"hy2OFkD_O0mA"},"source":["# [Task 5 - 1.0 points] Training and Evaluation\n","\n","You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."]},{"cell_type":"markdown","metadata":{"id":"_G20Sgu1O0mB"},"source":["### Instructions\n","\n","* Train **all** models on the train set.\n","* Evaluate **all** models on the validation set.\n","* Compute metrics on the validation set.\n","* Pick **at least** three seeds for robust estimation.\n","* Pick the **best** performing model according to the observed validation set performance."]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["def initialize_weights(model):\n","    for _, param in model.named_parameters():\n","        if isinstance(model, nn.LSTM) or isinstance(model, nn.Linear):\n","            nn.init.normal_(param.data, mean = 0, std = 0.1)"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["def get_to_be_masked_tags():\n","    punctuation_tags = ['$', '``', '.', ',', '#', 'SYM', ':', \"''\",'-RRB-','-LRB-']   #tags to be masked \n","    token_punctuation = [tag2int[tag] for tag in punctuation_tags]\n","    return torch.LongTensor(token_punctuation+[0])\n","\n","to_mask = get_to_be_masked_tags()\n","\n","def reshape_and_mask(predictions,targets): \n","    non_masked_elements = torch.isin(targets, to_mask, invert=True)\n","    \n","    return predictions[non_masked_elements],targets[non_masked_elements]\n"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"BdvD5aq-EiHV"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class PosDataset(Dataset):\n","    def __init__(self, text, labels):\n","        self.labels = labels\n","        self.text = text\n","        self.sentence_lengths = [len(sentence) for sentence in self.text]\n","    def __len__(self):\n","            return len(self.labels)\n","    def __getitem__(self, idx):\n","            label = self.labels[idx]\n","            text = self.text[idx]\n","            sample = (text, label, self.sentence_lengths[idx])\n","            return sample\n","\n","\n","def collate_fn(data):\n","    return ([x[0] for x in data], [x[1] for x in data], [x[2] for x in data])\n","\n","\n","def create_dataloaders(b_s : int):\n","    train_df = indexed_dataset[indexed_dataset['split'] == 'train'].reset_index(drop=True)      \n","    val_df = indexed_dataset[indexed_dataset['split'] == 'val'].reset_index(drop=True)\n","    test_df = indexed_dataset[indexed_dataset['split'] == 'test'].reset_index(drop=True)\n","\n","    #create DataframeDataset objects for each split \n","    train_dataset = PosDataset(train_df.iloc[:,2],train_df.iloc[:,3])\n","    val_dataset = PosDataset(val_df.iloc[:,2],val_df.iloc[:,3])\n","    test_dataset = PosDataset(test_df.iloc[:,2],test_df.iloc[:,3])\n","\n","    train_dataloader = DataLoader(train_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","    val_dataloader = DataLoader(val_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","    test_dataloader = DataLoader(test_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","\n","    return train_dataloader,val_dataloader,test_dataloader "]},{"cell_type":"code","execution_count":82,"metadata":{"id":"wuKaQtDuGAia"},"outputs":[],"source":["batch_size = 32\n","\n","tr_dl, val_dl, test_dl = create_dataloaders(batch_size)"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"xzlrW52UIRet"},"outputs":[],"source":["from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import torch.nn.utils.rnn as rnn\n","\n","def train(model, epochs, loss_function, dataloader, optimizer, padding_value = 0):\n","    model.train()\n","    for epoch in range(epochs):\n","        for sentences, pos, s_len in dataloader:\n","            optimizer.zero_grad()\n","            \n","            tensor_sentences = [torch.LongTensor(s) for s in sentences]\n","            tensor_pos = [torch.LongTensor(p) for p in pos]\n","\n","            padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = 0)\n","            padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n","\n","            predicted = model(padded_sentences, s_len)\n","\n","            predicted = predicted.view(-1,predicted.shape[-1])    \n","            targets = padded_pos.view(-1)\n","\n","            predicted, targets = reshape_and_mask(predicted, targets)\n","\n","            loss = loss_function(predicted, targets)\n","            loss.backward()\n","            optimizer.step()\n","        print(f'Train epoch [{epoch+1}/{epochs}] loss: {loss.item()}')\n","\n","def evaluate(model, loss_function, dataloader, padding_value=0):\n","    model.eval()\n","    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n","    epoch_loss = 0\n","    for sentences, pos, s_len in dataloader:\n","        tensor_sentences = [torch.LongTensor(s) for s in sentences]\n","        tensor_pos = [torch.LongTensor(p) for p in pos]\n","\n","        padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = 0)\n","        padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n","\n","        predicted = model(padded_sentences, s_len)\n","        predicted = predicted.view(-1,predicted.shape[-1])    \n","        targets = padded_pos.view(-1)\n","\n","        predicted, targets = reshape_and_mask(predicted, targets)\n","\n","        loss = loss_function(predicted, targets)\n","\n","        predicted = predicted.argmax(dim=1)\n","\n","        tot_pred = torch.cat((tot_pred,predicted))\n","        tot_targ = torch.cat((tot_targ,targets))\n","\n","        epoch_loss += loss.item()\n","    full_accuracy, full_f1 = accuracy_and_f1(tot_pred,tot_targ)\n","    print(f'Eval: loss: {epoch_loss} accuracy: {full_accuracy} f1: {full_f1}')\n","    return full_accuracy,full_f1,tot_pred,tot_targ\n"]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"elapsed":712,"status":"error","timestamp":1699270756409,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"_E-2S0beJm-0","outputId":"5b3a2d5a-4f15-4ba6-a6dc-edd45a02a17f"},"outputs":[],"source":["loss_function = CrossEntropyLoss()\n","\n","\n","lstm_dimension = 16\n","dense_dimension = len(unique_tags)+1\n","\n","baseline_model = Baseline(lstm_dimension, dense_dimension)\n","baseline_model.apply(initialize_weights)\n","double_lstm_model = Model1(lstm_dimension, dense_dimension)\n","double_lstm_model.apply(initialize_weights)\n","double_dense_model = Model2(lstm_dimension, dense_dimension)\n","double_dense_model.apply(initialize_weights)"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb Cell 43\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m baseline_model\u001b[39m.\u001b[39mapply(initialize_weights)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(baseline_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m5e-4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train(baseline_model, epochs, loss_function, tr_dl, optimizer)\n","\u001b[1;32m/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb Cell 43\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#X55sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     predicted, targets \u001b[39m=\u001b[39m reshape_and_mask(predicted, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#X55sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(predicted, targets)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#X55sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#X55sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stefano/Public/NLP-assignments/Assignment-1/Assignment1.ipynb#X55sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain epoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m] loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["epochs = 30\n","optimizer = Adam(baseline_model.parameters(), lr=5e-4)\n","\n","train(baseline_model, epochs, loss_function, tr_dl, optimizer)"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [1/30] loss: 3.285120964050293\n","Train epoch [2/30] loss: 2.667982339859009\n","Train epoch [3/30] loss: 2.6150708198547363\n","Train epoch [4/30] loss: 2.640927314758301\n","Train epoch [5/30] loss: 2.876405715942383\n","Train epoch [6/30] loss: 2.79787540435791\n","Train epoch [7/30] loss: 2.5937769412994385\n","Train epoch [8/30] loss: 2.566309690475464\n","Train epoch [9/30] loss: 2.4106545448303223\n","Train epoch [10/30] loss: 2.3668131828308105\n","Train epoch [11/30] loss: 2.037339210510254\n","Train epoch [12/30] loss: 2.0174338817596436\n","Train epoch [13/30] loss: 1.8545552492141724\n","Train epoch [14/30] loss: 1.849299669265747\n","Train epoch [15/30] loss: 1.6976778507232666\n","Train epoch [16/30] loss: 1.642504096031189\n","Train epoch [17/30] loss: 1.6102566719055176\n","Train epoch [18/30] loss: 1.4706017971038818\n","Train epoch [19/30] loss: 1.2183908224105835\n","Train epoch [20/30] loss: 1.3028658628463745\n","Train epoch [21/30] loss: 1.3777451515197754\n","Train epoch [22/30] loss: 1.2762205600738525\n","Train epoch [23/30] loss: 1.4322055578231812\n","Train epoch [24/30] loss: 1.1015188694000244\n","Train epoch [25/30] loss: 1.188070297241211\n","Train epoch [26/30] loss: 1.0217267274856567\n","Train epoch [27/30] loss: 1.1113379001617432\n","Train epoch [28/30] loss: 1.0099537372589111\n","Train epoch [29/30] loss: 1.1212841272354126\n","Train epoch [30/30] loss: 1.0828663110733032\n"]}],"source":["epochs = 30\n","optimizer = Adam(double_lstm_model.parameters(), lr=5e-4)\n","\n","train(double_lstm_model, epochs, loss_function, tr_dl, optimizer)"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [1/40] loss: 0.6017288565635681\n","Train epoch [2/40] loss: 0.6938601732254028\n","Train epoch [3/40] loss: 0.4742712080478668\n","Train epoch [4/40] loss: 0.5592638254165649\n","Train epoch [5/40] loss: 0.32801929116249084\n","Train epoch [6/40] loss: 0.39896032214164734\n","Train epoch [7/40] loss: 0.48460033535957336\n","Train epoch [8/40] loss: 0.38117948174476624\n","Train epoch [9/40] loss: 0.5485690832138062\n","Train epoch [10/40] loss: 0.3266785740852356\n","Train epoch [11/40] loss: 0.43117257952690125\n","Train epoch [12/40] loss: 0.28589046001434326\n","Train epoch [13/40] loss: 0.2928452789783478\n","Train epoch [14/40] loss: 0.3467297852039337\n","Train epoch [15/40] loss: 0.25365257263183594\n","Train epoch [16/40] loss: 0.33260563015937805\n","Train epoch [17/40] loss: 0.21312782168388367\n","Train epoch [18/40] loss: 0.16909128427505493\n","Train epoch [19/40] loss: 0.30344298481941223\n","Train epoch [20/40] loss: 0.1930406540632248\n","Train epoch [21/40] loss: 0.3564225137233734\n","Train epoch [22/40] loss: 0.22266274690628052\n","Train epoch [23/40] loss: 0.19942468404769897\n","Train epoch [24/40] loss: 0.24333468079566956\n","Train epoch [25/40] loss: 0.15818950533866882\n","Train epoch [26/40] loss: 0.2040669023990631\n","Train epoch [27/40] loss: 0.22480836510658264\n","Train epoch [28/40] loss: 0.22450760006904602\n","Train epoch [29/40] loss: 0.18755482137203217\n","Train epoch [30/40] loss: 0.22864103317260742\n","Train epoch [31/40] loss: 0.18914732336997986\n","Train epoch [32/40] loss: 0.272616982460022\n","Train epoch [33/40] loss: 0.1377926915884018\n","Train epoch [34/40] loss: 0.20099280774593353\n","Train epoch [35/40] loss: 0.1837022453546524\n","Train epoch [36/40] loss: 0.13900195062160492\n","Train epoch [37/40] loss: 0.13536477088928223\n","Train epoch [38/40] loss: 0.16807685792446136\n","Train epoch [39/40] loss: 0.10714114457368851\n","Train epoch [40/40] loss: 0.1103500947356224\n"]}],"source":["epochs = 50\n","optimizer = Adam(double_dense_model.parameters(), lr=5e-4)\n","\n","train(double_dense_model, epochs, loss_function, tr_dl, optimizer)"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['PRP', 'RB', 'TO', 'VB', 'NNS']\n"]}],"source":["double_dense_model.eval()\n","test_phrase = ['i', 'like', 'to', 'eat', 'apples']\n","test_phrase = [word2int[word] for word in test_phrase]\n","test_phrase = torch.LongTensor(test_phrase).unsqueeze(0)\n","test_phrase_len = [len(test_phrase[0])]\n","\n","predicted = double_dense_model(test_phrase, test_phrase_len)\n","predicted = predicted.view(-1,predicted.shape[-1])\n","predicted = torch.argmax(predicted, dim=1)\n","predicted = [int2tag[int(p)] for p in predicted]\n","print(predicted)\n"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Eval: loss: 41.928628861904144 accuracy: 0.7187511324882507 f1: 0.39796282914782366\n"]}],"source":["baseline_accuracy, baseline_f1, baseline_pred, baseline_targ = evaluate(baseline_model, loss_function, val_dl)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Eval: loss: 42.69097936153412 accuracy: 0.7071889638900757 f1: 0.35210249239547836\n"]}],"source":["double_lstm_accuracy, double_lstm_f1, double_lstm_pred, double_lstm_targ = evaluate(double_lstm_model, loss_function, val_dl)"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Eval: loss: 16.91549661755562 accuracy: 0.8895211219787598 f1: 0.7406234257330444\n"]}],"source":["double_dense_accuracy, double_dense_f1, double_dense_pred, double_dense_targ = evaluate(double_dense_model, loss_function, val_dl)"]},{"cell_type":"markdown","metadata":{"id":"_sN60F5MO0mB"},"source":["# [Task 6 - 1.0 points] Error Analysis\n","\n","You are tasked to evaluate your best performing model."]},{"cell_type":"markdown","metadata":{"id":"5IvcbIwiO0mB"},"source":["### Instructions\n","\n","* Compare the errors made on the validation and test sets.\n","* Aggregate model errors into categories (if possible)\n","* Comment the about errors and propose possible solutions on how to address them."]},{"cell_type":"markdown","metadata":{"id":"32F8eX_hO0mB"},"source":["# [Task 7 - 1.0 points] Report\n","\n","Wrap up your experiment in a short report (up to 2 pages)."]},{"cell_type":"markdown","metadata":{"id":"xv4OVz1vO0mC"},"source":["### Instructions\n","\n","* Use the NLP course report template.\n","* Summarize each task in the report following the provided template."]},{"cell_type":"markdown","metadata":{"id":"LkAPKCbIO0mC"},"source":["### Recommendations\n","\n","The report is not a copy-paste of graphs, tables, and command outputs.\n","\n","* Summarize classification performance in Table format.\n","* **Do not** report command outputs or screenshots.\n","* Report learning curves in Figure format.\n","* The error analysis section should summarize your findings."]},{"cell_type":"markdown","metadata":{"id":"449lJeI-O0mC"},"source":["# Submission\n","\n","* **Submit** your report in PDF format.\n","* **Submit** your python notebook.\n","* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n","* You can upload **model weights** in a cloud repository and report the link in the report."]},{"cell_type":"markdown","metadata":{"id":"adKOVVP-O0mC"},"source":["# FAQ\n","\n","Please check this frequently asked questions before contacting us"]},{"cell_type":"markdown","metadata":{"id":"x9faC6QnO0mC"},"source":["### Trainable Embeddings\n","\n","You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."]},{"cell_type":"markdown","metadata":{"id":"KML9gLHgO0mC"},"source":["### Model architecture\n","\n","You **should not** change the architecture of a model (i.e., its layers).\n","\n","However, you are **free** to play with their hyper-parameters."]},{"cell_type":"markdown","metadata":{"id":"8cgg47FVO0mD"},"source":["### Neural Libraries\n","\n","You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"]},{"cell_type":"markdown","metadata":{"id":"6qmgkNGsO0mD"},"source":["### Keras TimeDistributed Dense layer\n","\n","If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."]},{"cell_type":"markdown","metadata":{"id":"AKxd8OlIO0mD"},"source":["### Error Analysis\n","\n","Some topics for discussion include:\n","   * Model performance on most/less frequent classes.\n","   * Precision/Recall curves.\n","   * Confusion matrices.\n","   * Specific misclassified samples."]},{"cell_type":"markdown","metadata":{"id":"_m5SkbO0O0mD"},"source":["### Punctuation\n","\n","**Do not** remove punctuation from documents since it may be helpful to the model.\n","\n","You should **ignore** it during metrics computation.\n","\n","If you are curious, you can run additional experiments to verify the impact of removing punctuation."]},{"cell_type":"markdown","metadata":{"id":"urd139anO0mD"},"source":["# The End"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
