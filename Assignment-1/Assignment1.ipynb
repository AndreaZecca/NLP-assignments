{"cells":[{"cell_type":"markdown","metadata":{"id":"-WeCeITXoxLf"},"source":["# Assignment 1\n","\n","**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n","\n","**Keywords**: POS tagging, Sequence labelling, RNNs"]},{"cell_type":"markdown","metadata":{"id":"mEMlC0FYO0l2"},"source":["\n","# Contact\n","\n","For any doubt, question, issue or help, you can always contact us at the following email addresses:\n","\n","Teaching Assistants:\n","\n","* Federico Ruggeri -> federico.ruggeri6@unibo.it\n","* Eleonora Mancini -> e.mancini@unibo.it\n","\n","Professor:\n","\n","* Paolo Torroni -> p.torroni@unibo.it"]},{"cell_type":"markdown","metadata":{"id":"tDP-HaMpO0l3"},"source":["# Introduction\n","\n","You are tasked to address the task of POS tagging.\n","\n","<center>\n","    <img src=\"./images/pos_tagging.png\" alt=\"POS tagging\" />\n","</center>"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18918,"status":"ok","timestamp":1699279441314,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"DM-Jq2mLO6Nj","outputId":"052e880c-0294-4a1f-d0f0-fed00fdee923"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7380,"status":"ok","timestamp":1699279473704,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"ykSh18iMPDEU"},"outputs":[],"source":["#!cp -rf /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/data ./\n","#!cp -rf /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/images ./\n","#!cp /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/data.csv ./"]},{"cell_type":"markdown","metadata":{"id":"sr8QdeOXO0l4"},"source":["# [Task 1 - 0.5 points] Corpus\n","\n","You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n","\n","**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n","\n","### Example\n","\n","```Pierre\tNNP\t2\n","Vinken\tNNP\t8\n",",\t,\t2\n","61\tCD\t5\n","years\tNNS\t6\n","old\tJJ\t2\n",",\t,\t2\n","will\tMD\t0\n","join\tVB\t8\n","the\tDT\t11\n","board\tNN\t9\n","as\tIN\t9\n","a\tDT\t15\n","nonexecutive\tJJ\t15\n","director\tNN\t12\n","Nov.\tNNP\t9\n","29\tCD\t16\n",".\t.\t8\n","```"]},{"cell_type":"markdown","metadata":{"id":"Dd_jkm9hO0l5"},"source":["### Splits\n","\n","The corpus contains 200 documents.\n","\n","   * **Train**: Documents 1-100\n","   * **Validation**: Documents 101-150\n","   * **Test**: Documents 151-199"]},{"cell_type":"markdown","metadata":{"id":"4Hx0FbERO0l6"},"source":["### Instructions\n","\n","* **Download** the corpus.\n","* **Encode** the corpus into a pandas.DataFrame object.\n","* **Split** it in training, validation, and test sets."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoding dataset as pandas dataframe...\n"]},{"name":"stdout","output_type":"stream","text":["Encoding completed!\n","Some words from the dataset: ['referral', 'siegal', 'forward', 'headline', '30,841', 'e.w.', 'alzheimer', 'consideration', 'rhythm', 'procurement', 'asserts', 'combat', 'chrysler', 'advancing', 'norwick']\n","Some tags from the dataset: ['RBS', 'RP', 'NN', 'JJS', 'RB', 'VB', 'WP$', 'FW', 'WP$', 'VBZ', 'VB', '#', '.', '.', 'MD']\n","\n","encoded dataframe:\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import random\n","\n","data_folder = \"./data\"\n","def encode_dataset(dataset_name: str, to_lower: bool) -> pd.DataFrame:\n","  \"\"\"\n","    Takes the dataset and encodes it in a pandas dataframe having six columns ['split', 'doc_id', 'sentence_num', 'words', 'tags', 'num_tokens']. Computes also unique tags set and unique words set and returns them with the dataframe.\n","  \n","  \"\"\"\n","  print(\"Encoding dataset as pandas dataframe...\")\n","\n","  dataset_folder = os.path.join(data_folder+ \"/dataset\")\n","  \n","  dataframe_rows = []             #dataframe that will contain all the sentences in all the documents, each sentence as a list of word and a list of corresponding tags\n","  unique_tags = set()             \n","  unique_words = set()\n","\n","  for doc in os.listdir(dataset_folder):\n","    if doc.endswith(\".csv\") or doc.endswith(\".pkl\"): continue\n","    doc_num = int(doc[5:8])\n","    doc_path = os.path.join(dataset_folder,doc)\n","\n","    with open(doc_path, mode='r', encoding='utf-8') as file:\n","      df = pd.read_csv(file,sep='\\t',header=None,skip_blank_lines=False)\n","      df.rename(columns={0:'word',1:\"TAG\",2:\"remove\"},inplace=True)\n","      df.drop(\"remove\",axis=1,inplace=True)\n","\n","      if to_lower: df['word'] = df[\"word\"].str.lower() #set all words to lower case\n","      \n","      #create another column that indicate the group id by sentence \n","      df[\"group_num\"] = df.isnull().all(axis=1).cumsum()\n","      df.dropna(inplace=True)\n","      df.reset_index(drop=True, inplace=True)\n","      \n","      unique_tags.update(df['TAG'].unique())     #save all the unique tags in a set \n","      unique_words.update(df['word'].unique())   #save all the unique words in a set \n","\n","      #generate sentence list in a document \n","      df_list = [df.iloc[rows] for _, rows in df.groupby('group_num').groups.items()]\n","      for n,d in enumerate(df_list) :           #for each sentence create a row in the final dataframe\n","          dataframe_row = {\n","              \"split\" : 'train' if doc_num<=100 else ('val' if doc_num<=150  else 'test'),\n","              \"doc_id\" : doc_num,\n","              \"sentence_num\" : n,\n","              \"words\": d['word'].tolist(),\n","              \"tags\":  d['TAG'].tolist(),\n","              \"num_tokens\": len(d['word'])\n","          }\n","          dataframe_rows.append(dataframe_row)\n","\n","  dataframe_path = os.path.join(data_folder, dataset_name)\n","  df_final = pd.DataFrame(dataframe_rows)\n","  df_final.to_csv(dataframe_path + \".csv\")                      #save as csv to inspect\n","\n","  print(\"Encoding completed!\")\n","    \n","  return  df_final, unique_tags, unique_words\n","\n","df, unique_tags, unique_words = encode_dataset(\"encoded_dataset\", to_lower = True)\n","\n","print('Some words from the dataset:', random.choices(list(unique_words),k=15))\n","print('Some tags from the dataset:', random.choices(list(unique_tags),k=15))\n","\n","print('\\nencoded dataframe:')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1699279510294,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"135YHdmjPj60","outputId":"c778bad9-720a-43f9-97ab-520cf4e345e6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>split</th>\n","      <th>doc_id</th>\n","      <th>sentence_num</th>\n","      <th>words</th>\n","      <th>tags</th>\n","      <th>num_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3249</th>\n","      <td>train</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n","      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>3250</th>\n","      <td>train</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>[mr., vinken, is, chairman, of, elsevier, n.v....</td>\n","      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>3376</th>\n","      <td>train</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>[rudolph, agnew, ,, 55, years, old, and, forme...</td>\n","      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>2243</th>\n","      <td>train</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>[although, preliminary, findings, were, report...</td>\n","      <td>[IN, JJ, NNS, VBD, VBN, RBR, IN, DT, NN, IN, ,...</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>2269</th>\n","      <td>train</td>\n","      <td>3</td>\n","      <td>29</td>\n","      <td>[it, has, no, bearing, on, our, work, force, t...</td>\n","      <td>[PRP, VBZ, DT, NN, IN, PRP$, NN, NN, NN, .]</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3494</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>9</td>\n","      <td>[in, a, second, area, of, common, concern, ,, ...</td>\n","      <td>[IN, DT, JJ, NN, IN, JJ, NN, ,, DT, NN, NN, ,,...</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>3485</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>0</td>\n","      <td>[a, house-senate, conference, approved, major,...</td>\n","      <td>[DT, NNP, NN, VBD, JJ, NNS, IN, DT, NN, IN, JJ...</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>3489</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>4</td>\n","      <td>[these, fiscal, pressures, are, also, a, facto...</td>\n","      <td>[DT, JJ, NNS, VBP, RB, DT, NN, IN, VBG, DT, NN...</td>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>3490</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>5</td>\n","      <td>[to, accommodate, the, additional, cash, assis...</td>\n","      <td>[TO, VB, DT, JJ, NN, NN, ,, DT, NNP, NNPS, NNP...</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>3486</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>1</td>\n","      <td>[for, the, agency, for, international, develop...</td>\n","      <td>[IN, DT, NNP, IN, NNP, NNP, ,, NNS, VBD, $, CD...</td>\n","      <td>51</td>\n","    </tr>\n","    <tr>\n","      <th>764</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>2</td>\n","      <td>[mr., ackerman, already, is, seeking, to, oust...</td>\n","      <td>[NNP, NNP, RB, VBZ, VBG, TO, VB, NNP, NNP, IN,...</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>763</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>1</td>\n","      <td>[the, move, boosts, intelogic, chairman, asher...</td>\n","      <td>[DT, NN, VBZ, NNP, NNP, NNP, NNP, POS, NN, TO,...</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>762</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>0</td>\n","      <td>[intelogic, trace, inc., ,, san, antonio, ,, t...</td>\n","      <td>[NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>765</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>3</td>\n","      <td>[the, action, followed, by, one, day, an, inte...</td>\n","      <td>[DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>766</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>4</td>\n","      <td>[in, new, york, stock, exchange, composite, tr...</td>\n","      <td>[IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...</td>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      split  doc_id  sentence_num  \\\n","3249  train       1             0   \n","3250  train       1             1   \n","3376  train       2             0   \n","2243  train       3             3   \n","2269  train       3            29   \n","3494    val     101             9   \n","3485    val     101             0   \n","3489    val     101             4   \n","3490    val     101             5   \n","3486    val     101             1   \n","764    test     151             2   \n","763    test     151             1   \n","762    test     151             0   \n","765    test     151             3   \n","766    test     151             4   \n","\n","                                                  words  \\\n","3249  [pierre, vinken, ,, 61, years, old, ,, will, j...   \n","3250  [mr., vinken, is, chairman, of, elsevier, n.v....   \n","3376  [rudolph, agnew, ,, 55, years, old, and, forme...   \n","2243  [although, preliminary, findings, were, report...   \n","2269  [it, has, no, bearing, on, our, work, force, t...   \n","3494  [in, a, second, area, of, common, concern, ,, ...   \n","3485  [a, house-senate, conference, approved, major,...   \n","3489  [these, fiscal, pressures, are, also, a, facto...   \n","3490  [to, accommodate, the, additional, cash, assis...   \n","3486  [for, the, agency, for, international, develop...   \n","764   [mr., ackerman, already, is, seeking, to, oust...   \n","763   [the, move, boosts, intelogic, chairman, asher...   \n","762   [intelogic, trace, inc., ,, san, antonio, ,, t...   \n","765   [the, action, followed, by, one, day, an, inte...   \n","766   [in, new, york, stock, exchange, composite, tr...   \n","\n","                                                   tags  num_tokens  \n","3249  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...          18  \n","3250  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...          13  \n","3376  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...          26  \n","2243  [IN, JJ, NNS, VBD, VBN, RBR, IN, DT, NN, IN, ,...          35  \n","2269        [PRP, VBZ, DT, NN, IN, PRP$, NN, NN, NN, .]          10  \n","3494  [IN, DT, JJ, NN, IN, JJ, NN, ,, DT, NN, NN, ,,...          43  \n","3485  [DT, NNP, NN, VBD, JJ, NNS, IN, DT, NN, IN, JJ...          44  \n","3489  [DT, JJ, NNS, VBP, RB, DT, NN, IN, VBG, DT, NN...          39  \n","3490  [TO, VB, DT, JJ, NN, NN, ,, DT, NNP, NNPS, NNP...          26  \n","3486  [IN, DT, NNP, IN, NNP, NNP, ,, NNS, VBD, $, CD...          51  \n","764   [NNP, NNP, RB, VBZ, VBG, TO, VB, NNP, NNP, IN,...          19  \n","763   [DT, NN, VBZ, NNP, NNP, NNP, NNP, POS, NN, TO,...          30  \n","762   [NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...          40  \n","765   [DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...          34  \n","766   [IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...          20  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df.sort_values(\"doc_id\").groupby('split').head()"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1699279519522,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"cNjtmOwZOdPh"},"outputs":[{"name":"stdout","output_type":"stream","text":["saving dictionaries as pickle files\n"]}],"source":["from collections import OrderedDict\n","import pickle\n","\n","dict_path = os.path.join(data_folder,'dictionaries.pkl') #path where dictionaries will be saved \n","\n","def build_dict(words : list[str], tags : list[str]): \n","    \"\"\"\n","        Builds 4 dictionaries word2int, int2word, tag2int, int2tag and returns them\n","    \"\"\"\n","    \n","    word2int = OrderedDict()\n","    int2word = OrderedDict()\n","\n","    for i, word in enumerate(words):\n","        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n","        int2word[i+1] = word\n","\n","    tag2int = OrderedDict()\n","    int2tag = OrderedDict()\n","\n","    for i, tag in enumerate(tags):\n","        tag2int[tag] = i+1\n","        int2tag[i+1] = tag\n","    \n","    print('saving dictionaries as pickle files')\n","    pickle_files = [word2int,int2word,tag2int,int2tag]\n","    \n","    with open(dict_path, 'wb') as f:\n","        pickle.dump(pickle_files, f)\n","\n","    return word2int,int2word,tag2int,int2tag\n","\n","word2int,int2word,tag2int,int2tag = build_dict(unique_words,unique_tags)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initiating numberization of words and tags in dataframe\n","Numberization completed\n"]},{"name":"stdout","output_type":"stream","text":["\n","All right with dataset numberization\n","Saving indexed dataframe\n"]}],"source":["indexed_df_path = os.path.join(data_folder, \"indexed_dataset.pkl\") #numberized dataframe path\n","\n","def build_indexed_dataframe(word2int, tag2int, df):\n","    \"\"\"\n","        Given the dictionaries word2int, tag2int and the dataframe, creates a dataframe were every word and tag is represented by its number and returns it\n","    \"\"\"\n","    print('Initiating numberization of words and tags in dataframe')\n","    indexed_rows = []\n","    for words,tags in zip(df['words'],df['tags']):\n","        indexed_row = {'indexed_words':[word2int[word] for word in words ],'indexed_tags':[tag2int[tag] for tag in tags ]}\n","        indexed_rows.append(indexed_row)\n","    \n","    indexed_df = pd.DataFrame(indexed_rows)\n","\n","    indexed_df.insert(0,'split',df['split'])\n","    indexed_df.insert(1,'num_tokens',df['num_tokens'])\n","\n","    print('Numberization completed')\n","\n","    return indexed_df\n","\n","\n","def check_dataframe_numberization(indexed_df, normal_df, int2word, int2tag) :\n","    \"\"\"\n","       Checks if the numberized dataframe will lead to the normal dataframe usind the dictionaries int2word and int2tag\n","    \"\"\"\n","    for n, (w_t, t_t) in enumerate(zip(indexed_df['indexed_words'],indexed_df['indexed_tags'])):\n","        if not normal_df.loc[n,'words'] == [int2word[indexed_word] for indexed_word in w_t]:\n","            print('words numberization gone wrong') \n","            return False\n","        if not normal_df.loc[n,'tags'] == [int2tag[indexed_tag] for indexed_tag in t_t]:\n","            print('tags numberization gone wrong')\n","            return False \n","    \n","    print('\\nAll right with dataset numberization')\n","    print('Saving indexed dataframe')\n","    \n","    indexed_df.to_pickle(indexed_df_path)\n","\n","\n","indexed_df = build_indexed_dataframe(word2int,tag2int,df)\n","check_dataframe_numberization(indexed_df,df, int2word, int2tag)\n"]},{"cell_type":"markdown","metadata":{"id":"MQvHxKRcO0l8"},"source":["# [Task 2 - 0.5 points] Text encoding\n","\n","To train a neural POS tagger, you first need to encode text into numerical format."]},{"cell_type":"markdown","metadata":{"id":"0EnUuOWTO0l9"},"source":["### Instructions\n","\n","* Embed words using **GloVe embeddings**.\n","* You are **free** to pick any embedding dimension.\n","* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206458,"status":"ok","timestamp":1699279728592,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"eVsfe-QKO0l9","outputId":"15f823d9-6f5c-47dc-c24d-cbd4f892bdac"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/stefano/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","from torchtext.vocab import GloVe\n","\n","embedding_dimension = 50\n","\n","glove_embeddings = GloVe(name='6B', dim=embedding_dimension)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving embedding matrix\n","Embedding matrix shape: (10948, 50)\n"]}],"source":["def build_embedding_matrix(emb_model, word2int):\n","    \"\"\"\n","        Given the embedding model and the dict. word2int. If there is the embedding for the word, we add it to the embedding_matrix. In negative case we put a list of random values.\n","        Return the embedding matrix\n","    \"\"\"\n","    #check_value_distribution_glove(emb_model)\n","   \n","    embedding_dimension = len(emb_model[0]) #how many numbers each emb vector is composed of                                                           \n","    embedding_matrix = np.zeros((len(word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n","\n","    for word, idx in word2int.items():\n","        try:\n","            embedding_vector = emb_model[word]\n","        except (KeyError, TypeError):\n","            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n","\n","        embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n","    \n","    print('Saving embedding matrix')\n","    path = os.path.join(data_folder, \"emb_matrix\")\n","    np.save(path,embedding_matrix,allow_pickle=True)\n","\n","    print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n","\n","    return embedding_matrix\n","\n","embedding_matrix = build_embedding_matrix(glove_embeddings, word2int)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading embedding matrix\n","Loading numberized dataset\n","Loading dictionaries\n","All data loaded\n"]}],"source":["def load_data():\n","    \"\"\"\n","        Loads the data \"emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag \" and returns them\n","    \"\"\"\n","    emb_matrix_path = os.path.join(data_folder,'emb_matrix.npy')\n","    indexed_dataset_path = os.path.join(data_folder,'indexed_dataset.pkl')\n","    dictionaries_path = os.path.join(data_folder,'dictionaries.pkl')\n","\n","    if os.path.exists(emb_matrix_path) and os.path.exists(indexed_dataset_path):\n","        print('Loading embedding matrix')\n","        emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n","        print('Loading numberized dataset')\n","        indexed_dataset = pd.read_pickle(indexed_dataset_path)\n","        print('Loading dictionaries')\n","        with open(dictionaries_path, 'rb') as f:\n","            word2int,int2word,tag2int,int2tag = pickle.load(f)\n","        \n","        print('All data loaded')\n","    else:\n","        print('What you are looking for is not present in the folder')\n","        emb_matrix, indexed_dataset = None, None\n","\n","    return emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag\n","\n","emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag = load_data()"]},{"cell_type":"markdown","metadata":{"id":"N4AChgkfO0l_"},"source":["# [Task 3 - 1.0 points] Model definition\n","\n","You are now tasked to define your neural POS tagger."]},{"cell_type":"markdown","metadata":{"id":"jkERwLU7O0l_"},"source":["### Instructions\n","\n","* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n","* You are **free** to experiment with hyper-parameters to define the baseline model.\n","\n","* **Model 1**: add an additional LSTM layer to the Baseline model.\n","* **Model 2**: add an additional Dense layer to the Baseline model.\n","\n","* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n","\n","**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."]},{"cell_type":"markdown","metadata":{"id":"babZmAQlWGIw"},"source":["### Baseline"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def create_emb_layer(weights_matrix, pad_idx = 0):\n","    \"\"\"\n","        Creates and returns the embedding layer\n","    \"\"\"\n","    matrix = torch.Tensor(weights_matrix)   #the embedding matrix \n","    _ , embedding_dim = matrix.shape \n","    emb_layer = nn.Embedding.from_pretrained(matrix, freeze=True, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable \n","    return emb_layer, embedding_dim"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"RSGVLm3iWF2a"},"outputs":[],"source":["class Baseline(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer(packed_sentences)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output\n","\n","        "]},{"cell_type":"markdown","metadata":{"id":"Zd-pJQyu6h-V"},"source":["### Model 1"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"4pOy7gXR62UC"},"outputs":[],"source":["class Model1(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer_1 = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.bidirectional_layer_2 = nn.LSTM(bidirectional=True, input_size=lstm_dimension*2, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer_1(packed_sentences)\n","        packed_output, _ = self.bidirectional_layer_2(packed_output)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"BY9Vy9997y5Q"},"source":["### Model 2"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"_RM739C970T5"},"outputs":[],"source":["class Model2(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer_1 = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.dense_layer_2 = nn.Linear(in_features=dense_dimension, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer(packed_sentences)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer_1(output)\n","        output = self.dense_layer_2(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"p9vgsKiaO0mA"},"source":["# [Task 4 - 1.0 points] Metrics\n","\n","Before training the models, you are tasked to define the evaluation metrics for comparison."]},{"cell_type":"markdown","metadata":{"id":"q_T6Bm1fO0mA"},"source":["### Instructions\n","\n","* Evaluate your models using macro F1-score, compute over **all** tokens.\n","* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n","* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"]},{"cell_type":"markdown","metadata":{"id":"NpoB8xTRO0mA"},"source":["**Note**: What about OOV tokens?\n","   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n","   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n","   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xx7AKsk69Zcw"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hy2OFkD_O0mA"},"source":["# [Task 5 - 1.0 points] Training and Evaluation\n","\n","You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."]},{"cell_type":"markdown","metadata":{"id":"_G20Sgu1O0mB"},"source":["### Instructions\n","\n","* Train **all** models on the train set.\n","* Evaluate **all** models on the validation set.\n","* Compute metrics on the validation set.\n","* Pick **at least** three seeds for robust estimation.\n","* Pick the **best** performing model according to the observed validation set performance."]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["def get_to_be_masked_tags():\n","    punctuation_tags = ['$', '``', '.', ',', '#', 'SYM', ':', \"''\",'-RRB-','-LRB-']   #tags to be masked \n","    token_punctuation = [tag2int[tag] for tag in punctuation_tags]\n","    return torch.LongTensor(token_punctuation+[0])\n","\n","to_mask = get_to_be_masked_tags()\n","\n","def reshape_and_mask(predictions,targets): \n","    non_masked_elements = torch.isin(targets, to_mask, invert=True)\n","    \n","    return predictions[non_masked_elements],targets[non_masked_elements]\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"BdvD5aq-EiHV"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class PosDataset(Dataset):\n","    def __init__(self, text, labels):\n","        self.labels = labels\n","        self.text = text\n","        self.sentence_lengths = [len(sentence) for sentence in self.text]\n","    def __len__(self):\n","            return len(self.labels)\n","    def __getitem__(self, idx):\n","            label = self.labels[idx]\n","            text = self.text[idx]\n","            sample = (text, label, self.sentence_lengths[idx])\n","            return sample"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def collate_fn(data):\n","    return ([x[0] for x in data], [x[1] for x in data], [x[2] for x in data])"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"AUtm_ceVFTgH"},"outputs":[],"source":["def create_dataloaders(b_s : int):     #b_s = batch_size\n","    \n","    train_df = indexed_dataset[indexed_dataset['split'] == 'train'].reset_index(drop=True)      \n","    val_df = indexed_dataset[indexed_dataset['split'] == 'val'].reset_index(drop=True)\n","    test_df = indexed_dataset[indexed_dataset['split'] == 'test'].reset_index(drop=True)\n","\n","    #create DataframeDataset objects for each split \n","    train_dataset = PosDataset(train_df.iloc[:,2],train_df.iloc[:,3])\n","    val_dataset = PosDataset(val_df.iloc[:,2],val_df.iloc[:,3])\n","    test_dataset = PosDataset(test_df.iloc[:,2],test_df.iloc[:,3])\n","\n","    train_dataloader = DataLoader(train_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","    val_dataloader = DataLoader(val_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","    test_dataloader = DataLoader(test_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","\n","    return train_dataloader,val_dataloader,test_dataloader "]},{"cell_type":"code","execution_count":25,"metadata":{"id":"wuKaQtDuGAia"},"outputs":[],"source":["batch_size = 32\n","\n","tr_dl, val_dl, test_dl = create_dataloaders(batch_size)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"xzlrW52UIRet"},"outputs":[],"source":["from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import torch.nn.utils.rnn as rnn\n","\n","def train(model, epochs, loss_function, dataloader):\n","    model.train()\n","    optimizer = Adam(model.parameters(), lr=5e-4)\n","    for epoch in range(epochs):\n","        for sentences, pos, s_len in dataloader:\n","            optimizer.zero_grad()\n","            \n","            tensor_sentences = [torch.LongTensor(s) for s in sentences]\n","            tensor_pos = [torch.LongTensor(p) for p in pos]\n","\n","            padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = 0)\n","            padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value = 0)\n","\n","            predicted = model(padded_sentences, s_len)\n","\n","            predicted = predicted.view(-1,predicted.shape[-1])    \n","            targets = padded_pos.view(-1)\n","\n","            predicted, targets = reshape_and_mask(predicted, targets)\n","\n","            loss = loss_function(predicted, targets)\n","            loss.backward()\n","            optimizer.step()\n","        print(f'Train epoch [{epoch+1}/{epochs}] loss: {loss.item()}')"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"elapsed":712,"status":"error","timestamp":1699270756409,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"_E-2S0beJm-0","outputId":"5b3a2d5a-4f15-4ba6-a6dc-edd45a02a17f"},"outputs":[],"source":["loss_function = CrossEntropyLoss()\n","\n","lstm_dimension = 16\n","dense_dimension = len(unique_tags)+1\n","\n","baseline_model = Baseline(lstm_dimension, dense_dimension)\n","#double_lstm_model = Model1(lstm_dimension, dense_dimension)\n","#double_dense_model = Model2(lstm_dimension, dense_dimension)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [0/10] loss: 0.8145679831504822\n","Train epoch [1/10] loss: 0.9911481738090515\n","Train epoch [2/10] loss: 0.7941203713417053\n","Train epoch [3/10] loss: 0.8584253191947937\n","Train epoch [4/10] loss: 0.7546745538711548\n","Train epoch [5/10] loss: 0.9665728807449341\n","Train epoch [6/10] loss: 0.8961250185966492\n","Train epoch [7/10] loss: 0.6722370982170105\n","Train epoch [8/10] loss: 0.8010509610176086\n","Train epoch [9/10] loss: 0.7266693711280823\n"]}],"source":["epochs = 10\n","train(baseline_model, epochs, loss_function, tr_dl)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['PRP', 'IN', 'TO', 'VB', 'NNP']\n"]}],"source":["test_phrase = ['i', 'like', 'to', 'eat', 'apples']\n","test_phrase = [word2int[word] for word in test_phrase]\n","test_phrase = torch.LongTensor(test_phrase).unsqueeze(0)\n","test_phrase_len = [len(test_phrase[0])]\n","\n","predicted = baseline_model(test_phrase, test_phrase_len)\n","predicted = predicted.view(-1,predicted.shape[-1])\n","predicted = torch.argmax(predicted, dim=1)\n","predicted = [int2tag[int(p)] for p in predicted]\n","print(predicted)\n"]},{"cell_type":"markdown","metadata":{"id":"_sN60F5MO0mB"},"source":["# [Task 6 - 1.0 points] Error Analysis\n","\n","You are tasked to evaluate your best performing model."]},{"cell_type":"markdown","metadata":{"id":"5IvcbIwiO0mB"},"source":["### Instructions\n","\n","* Compare the errors made on the validation and test sets.\n","* Aggregate model errors into categories (if possible)\n","* Comment the about errors and propose possible solutions on how to address them."]},{"cell_type":"markdown","metadata":{"id":"32F8eX_hO0mB"},"source":["# [Task 7 - 1.0 points] Report\n","\n","Wrap up your experiment in a short report (up to 2 pages)."]},{"cell_type":"markdown","metadata":{"id":"xv4OVz1vO0mC"},"source":["### Instructions\n","\n","* Use the NLP course report template.\n","* Summarize each task in the report following the provided template."]},{"cell_type":"markdown","metadata":{"id":"LkAPKCbIO0mC"},"source":["### Recommendations\n","\n","The report is not a copy-paste of graphs, tables, and command outputs.\n","\n","* Summarize classification performance in Table format.\n","* **Do not** report command outputs or screenshots.\n","* Report learning curves in Figure format.\n","* The error analysis section should summarize your findings."]},{"cell_type":"markdown","metadata":{"id":"449lJeI-O0mC"},"source":["# Submission\n","\n","* **Submit** your report in PDF format.\n","* **Submit** your python notebook.\n","* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n","* You can upload **model weights** in a cloud repository and report the link in the report."]},{"cell_type":"markdown","metadata":{"id":"adKOVVP-O0mC"},"source":["# FAQ\n","\n","Please check this frequently asked questions before contacting us"]},{"cell_type":"markdown","metadata":{"id":"x9faC6QnO0mC"},"source":["### Trainable Embeddings\n","\n","You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."]},{"cell_type":"markdown","metadata":{"id":"KML9gLHgO0mC"},"source":["### Model architecture\n","\n","You **should not** change the architecture of a model (i.e., its layers).\n","\n","However, you are **free** to play with their hyper-parameters."]},{"cell_type":"markdown","metadata":{"id":"8cgg47FVO0mD"},"source":["### Neural Libraries\n","\n","You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"]},{"cell_type":"markdown","metadata":{"id":"6qmgkNGsO0mD"},"source":["### Keras TimeDistributed Dense layer\n","\n","If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."]},{"cell_type":"markdown","metadata":{"id":"AKxd8OlIO0mD"},"source":["### Error Analysis\n","\n","Some topics for discussion include:\n","   * Model performance on most/less frequent classes.\n","   * Precision/Recall curves.\n","   * Confusion matrices.\n","   * Specific misclassified samples."]},{"cell_type":"markdown","metadata":{"id":"_m5SkbO0O0mD"},"source":["### Punctuation\n","\n","**Do not** remove punctuation from documents since it may be helpful to the model.\n","\n","You should **ignore** it during metrics computation.\n","\n","If you are curious, you can run additional experiments to verify the impact of removing punctuation."]},{"cell_type":"markdown","metadata":{"id":"urd139anO0mD"},"source":["# The End"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
