{"cells":[{"cell_type":"markdown","metadata":{"id":"-WeCeITXoxLf"},"source":["# Assignment 1\n","\n","**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n","\n","**Keywords**: POS tagging, Sequence labelling, RNNs"]},{"cell_type":"markdown","metadata":{"id":"mEMlC0FYO0l2"},"source":["\n","# Contact\n","\n","For any doubt, question, issue or help, you can always contact us at the following email addresses:\n","\n","Teaching Assistants:\n","\n","* Federico Ruggeri -> federico.ruggeri6@unibo.it\n","* Eleonora Mancini -> e.mancini@unibo.it\n","\n","Professor:\n","\n","* Paolo Torroni -> p.torroni@unibo.it"]},{"cell_type":"markdown","metadata":{"id":"tDP-HaMpO0l3"},"source":["# Introduction\n","\n","You are tasked to address the task of POS tagging.\n","\n","<center>\n","    <img src=\"./images/pos_tagging.png\" alt=\"POS tagging\" />\n","</center>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18918,"status":"ok","timestamp":1699279441314,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"DM-Jq2mLO6Nj","outputId":"052e880c-0294-4a1f-d0f0-fed00fdee923"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7380,"status":"ok","timestamp":1699279473704,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"ykSh18iMPDEU"},"outputs":[],"source":["#!cp -rf /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/data ./\n","#!cp -rf /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/images ./\n","#!cp /content/drive/MyDrive/UNIBO/NLP/Assignments/Assignment-1/data.csv ./"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["seed = 0"]},{"cell_type":"markdown","metadata":{"id":"sr8QdeOXO0l4"},"source":["# [Task 1 - 0.5 points] Corpus\n","\n","You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n","\n","**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n","\n","### Example\n","\n","```Pierre\tNNP\t2\n","Vinken\tNNP\t8\n",",\t,\t2\n","61\tCD\t5\n","years\tNNS\t6\n","old\tJJ\t2\n",",\t,\t2\n","will\tMD\t0\n","join\tVB\t8\n","the\tDT\t11\n","board\tNN\t9\n","as\tIN\t9\n","a\tDT\t15\n","nonexecutive\tJJ\t15\n","director\tNN\t12\n","Nov.\tNNP\t9\n","29\tCD\t16\n",".\t.\t8\n","```"]},{"cell_type":"markdown","metadata":{"id":"Dd_jkm9hO0l5"},"source":["### Splits\n","\n","The corpus contains 200 documents.\n","\n","   * **Train**: Documents 1-100\n","   * **Validation**: Documents 101-150\n","   * **Test**: Documents 151-199"]},{"cell_type":"markdown","metadata":{"id":"4Hx0FbERO0l6"},"source":["### Instructions\n","\n","* **Download** the corpus.\n","* **Encode** the corpus into a pandas.DataFrame object.\n","* **Split** it in training, validation, and test sets."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoding dataset as pandas dataframe...\n","Encoding completed!\n","Some words from the dataset: ['producer', 'slowly', 'seduce', 'u.s.a.', 'increasing', 'random', 'forecast', 'dating', 'fanciful', 'overruns', 'lender', 'prepared', 'nets', 'entering', 'profitability']\n","Some tags from the dataset: ['VBD', 'RB', 'CD', 'VB', 'RB', 'WRB', 'JJS', 'RB', '$', '-LRB-', 'WP$', 'VBN', 'VBP', 'PRP$', 'PDT']\n","\n","encoded dataframe:\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import random\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","\n","data_folder = \"./data\"\n","def encode_dataset(dataset_name: str, to_lower: bool) -> pd.DataFrame:\n","  \"\"\"\n","    Takes the dataset and encodes it in a pandas dataframe having six columns ['split', 'doc_id', 'sentence_num', 'words', 'tags', 'num_tokens']. Computes also unique tags set and unique words set and returns them with the dataframe.\n","  \n","  \"\"\"\n","  print(\"Encoding dataset as pandas dataframe...\")\n","\n","  dataset_folder = os.path.join(data_folder+ \"/dataset\")\n","  \n","  dataframe_rows = []             #dataframe that will contain all the sentences in all the documents, each sentence as a list of word and a list of corresponding tags\n","  unique_tags = set()             \n","  unique_words = set()\n","\n","  for doc in os.listdir(dataset_folder):\n","    if doc.endswith(\".csv\") or doc.endswith(\".pkl\"): continue\n","    doc_num = int(doc[5:8])\n","    doc_path = os.path.join(dataset_folder,doc)\n","\n","    with open(doc_path, mode='r', encoding='utf-8') as file:\n","      df = pd.read_csv(file,sep='\\t',header=None,skip_blank_lines=False)\n","      df.rename(columns={0:'word',1:\"TAG\",2:\"remove\"},inplace=True)\n","      df.drop(\"remove\",axis=1,inplace=True)\n","\n","      if to_lower: df['word'] = df[\"word\"].str.lower() #set all words to lower case\n","      \n","      #create another column that indicate the group id by sentence \n","      df[\"group_num\"] = df.isnull().all(axis=1).cumsum()\n","      df.dropna(inplace=True)\n","      df.reset_index(drop=True, inplace=True)\n","      \n","      unique_tags.update(df['TAG'].unique())     #save all the unique tags in a set \n","      unique_words.update(df['word'].unique())   #save all the unique words in a set \n","\n","      #generate sentence list in a document \n","      df_list = [df.iloc[rows] for _, rows in df.groupby('group_num').groups.items()]\n","      for n,d in enumerate(df_list) :           #for each sentence create a row in the final dataframe\n","          dataframe_row = {\n","              \"split\" : 'train' if doc_num<=100 else ('val' if doc_num<=150  else 'test'),\n","              \"doc_id\" : doc_num,\n","              \"sentence_num\" : n,\n","              \"words\": d['word'].tolist(),\n","              \"tags\":  d['TAG'].tolist(),\n","              \"num_tokens\": len(d['word'])\n","          }\n","          dataframe_rows.append(dataframe_row)\n","\n","  dataframe_path = os.path.join(data_folder, dataset_name)\n","  df_final = pd.DataFrame(dataframe_rows)\n","  df_final.to_csv(dataframe_path + \".csv\")                      #save as csv to inspect\n","\n","  print(\"Encoding completed!\")\n","    \n","  return  df_final, unique_tags, unique_words\n","\n","df, unique_tags, unique_words = encode_dataset(\"encoded_dataset\", to_lower = True)\n","\n","print('Some words from the dataset:', random.choices(list(unique_words),k=15))\n","print('Some tags from the dataset:', random.choices(list(unique_tags),k=15))\n","\n","print('\\nencoded dataframe:')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1699279510294,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"135YHdmjPj60","outputId":"c778bad9-720a-43f9-97ab-520cf4e345e6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>split</th>\n","      <th>doc_id</th>\n","      <th>sentence_num</th>\n","      <th>words</th>\n","      <th>tags</th>\n","      <th>num_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2370</th>\n","      <td>train</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n","      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>2371</th>\n","      <td>train</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>[mr., vinken, is, chairman, of, elsevier, n.v....</td>\n","      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>921</th>\n","      <td>train</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>[rudolph, agnew, ,, 55, years, old, and, forme...</td>\n","      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>453</th>\n","      <td>train</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>[a, form, of, asbestos, once, used, to, make, ...</td>\n","      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n","      <td>36</td>\n","    </tr>\n","    <tr>\n","      <th>454</th>\n","      <td>train</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>[the, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n","      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>623</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>1</td>\n","      <td>[for, the, agency, for, international, develop...</td>\n","      <td>[IN, DT, NNP, IN, NNP, NNP, ,, NNS, VBD, $, CD...</td>\n","      <td>51</td>\n","    </tr>\n","    <tr>\n","      <th>636</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>14</td>\n","      <td>[rep., jerry, lewis, ,, a, conservative, calif...</td>\n","      <td>[NNP, NNP, NNP, ,, DT, JJ, NN, ,, VBD, DT, NN,...</td>\n","      <td>53</td>\n","    </tr>\n","    <tr>\n","      <th>624</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>2</td>\n","      <td>[the, conference, approved, at, least, $, 55, ...</td>\n","      <td>[DT, NN, VBD, IN, JJS, $, CD, CD, IN, JJ, NN, ...</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>622</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>0</td>\n","      <td>[a, house-senate, conference, approved, major,...</td>\n","      <td>[DT, NNP, NN, VBD, JJ, NNS, IN, DT, NN, IN, JJ...</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>625</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>3</td>\n","      <td>[the, agreement, on, poland, contrasts, with, ...</td>\n","      <td>[DT, NN, IN, NNP, VBZ, IN, DT, JJ, NNS, VBG, I...</td>\n","      <td>37</td>\n","    </tr>\n","    <tr>\n","      <th>3431</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>6</td>\n","      <td>[he, added, ,, ``, this, has, nothing, to, do,...</td>\n","      <td>[PRP, VBD, ,, ``, DT, VBZ, NN, TO, VB, IN, NNP...</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>3430</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>5</td>\n","      <td>[mr., edelman, declined, to, specify, what, pr...</td>\n","      <td>[NNP, NNP, VBD, TO, VB, WP, VBD, DT, JJ, NNS, ...</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>3429</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>4</td>\n","      <td>[in, new, york, stock, exchange, composite, tr...</td>\n","      <td>[IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>3428</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>3</td>\n","      <td>[the, action, followed, by, one, day, an, inte...</td>\n","      <td>[DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>3427</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>2</td>\n","      <td>[mr., ackerman, already, is, seeking, to, oust...</td>\n","      <td>[NNP, NNP, RB, VBZ, VBG, TO, VB, NNP, NNP, IN,...</td>\n","      <td>19</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      split  doc_id  sentence_num   \n","2370  train       1             0  \\\n","2371  train       1             1   \n","921   train       2             0   \n","453   train       3             0   \n","454   train       3             1   \n","623     val     101             1   \n","636     val     101            14   \n","624     val     101             2   \n","622     val     101             0   \n","625     val     101             3   \n","3431   test     151             6   \n","3430   test     151             5   \n","3429   test     151             4   \n","3428   test     151             3   \n","3427   test     151             2   \n","\n","                                                  words   \n","2370  [pierre, vinken, ,, 61, years, old, ,, will, j...  \\\n","2371  [mr., vinken, is, chairman, of, elsevier, n.v....   \n","921   [rudolph, agnew, ,, 55, years, old, and, forme...   \n","453   [a, form, of, asbestos, once, used, to, make, ...   \n","454   [the, asbestos, fiber, ,, crocidolite, ,, is, ...   \n","623   [for, the, agency, for, international, develop...   \n","636   [rep., jerry, lewis, ,, a, conservative, calif...   \n","624   [the, conference, approved, at, least, $, 55, ...   \n","622   [a, house-senate, conference, approved, major,...   \n","625   [the, agreement, on, poland, contrasts, with, ...   \n","3431  [he, added, ,, ``, this, has, nothing, to, do,...   \n","3430  [mr., edelman, declined, to, specify, what, pr...   \n","3429  [in, new, york, stock, exchange, composite, tr...   \n","3428  [the, action, followed, by, one, day, an, inte...   \n","3427  [mr., ackerman, already, is, seeking, to, oust...   \n","\n","                                                   tags  num_tokens  \n","2370  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...          18  \n","2371  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...          13  \n","921   [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...          26  \n","453   [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...          36  \n","454   [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...          32  \n","623   [IN, DT, NNP, IN, NNP, NNP, ,, NNS, VBD, $, CD...          51  \n","636   [NNP, NNP, NNP, ,, DT, JJ, NN, ,, VBD, DT, NN,...          53  \n","624   [DT, NN, VBD, IN, JJS, $, CD, CD, IN, JJ, NN, ...          48  \n","622   [DT, NNP, NN, VBD, JJ, NNS, IN, DT, NN, IN, JJ...          44  \n","625   [DT, NN, IN, NNP, VBZ, IN, DT, JJ, NNS, VBG, I...          37  \n","3431  [PRP, VBD, ,, ``, DT, VBZ, NN, TO, VB, IN, NNP...          27  \n","3430  [NNP, NNP, VBD, TO, VB, WP, VBD, DT, JJ, NNS, ...          29  \n","3429  [IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...          20  \n","3428  [DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...          34  \n","3427  [NNP, NNP, RB, VBZ, VBG, TO, VB, NNP, NNP, IN,...          19  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.sort_values(\"doc_id\").groupby('split').head()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1699279519522,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"cNjtmOwZOdPh"},"outputs":[{"name":"stdout","output_type":"stream","text":["saving dictionaries as pickle files\n"]}],"source":["from collections import OrderedDict\n","import pickle\n","\n","dict_path = os.path.join(data_folder,'dictionaries.pkl') #path where dictionaries will be saved \n","\n","def build_dict(words : list[str], tags : list[str]): \n","    \"\"\"\n","        Builds 4 dictionaries word2int, int2word, tag2int, int2tag and returns them\n","    \"\"\"\n","    \n","    word2int = OrderedDict()\n","    int2word = OrderedDict()\n","\n","    for i, word in enumerate(words):\n","        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n","        int2word[i+1] = word\n","\n","    tag2int = OrderedDict()\n","    int2tag = OrderedDict()\n","\n","    for i, tag in enumerate(tags):\n","        tag2int[tag] = i+1\n","        int2tag[i+1] = tag\n","    \n","    print('saving dictionaries as pickle files')\n","    pickle_files = [word2int,int2word,tag2int,int2tag]\n","    \n","    with open(dict_path, 'wb') as f:\n","        pickle.dump(pickle_files, f)\n","\n","    return word2int,int2word,tag2int,int2tag\n","\n","word2int,int2word,tag2int,int2tag = build_dict(unique_words,unique_tags)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initiating numberization of words and tags in dataframe\n","Numberization completed\n","\n","All right with dataset numberization\n","Saving indexed dataframe\n"]}],"source":["indexed_df_path = os.path.join(data_folder, \"indexed_dataset.pkl\") #numberized dataframe path\n","\n","def build_indexed_dataframe(word2int, tag2int, df):\n","    \"\"\"\n","        Given the dictionaries word2int, tag2int and the dataframe, creates a dataframe were every word and tag is represented by its number and returns it\n","    \"\"\"\n","    print('Initiating numberization of words and tags in dataframe')\n","    indexed_rows = []\n","    for words,tags in zip(df['words'],df['tags']):\n","        indexed_row = {'indexed_words':[word2int[word] for word in words ],'indexed_tags':[tag2int[tag] for tag in tags ]}\n","        indexed_rows.append(indexed_row)\n","    \n","    indexed_df = pd.DataFrame(indexed_rows)\n","\n","    indexed_df.insert(0,'split',df['split'])\n","    indexed_df.insert(1,'num_tokens',df['num_tokens'])\n","\n","    print('Numberization completed')\n","\n","    return indexed_df\n","\n","\n","def check_dataframe_numberization(indexed_df, normal_df, int2word, int2tag) :\n","    \"\"\"\n","       Checks if the numberized dataframe will lead to the normal dataframe usind the dictionaries int2word and int2tag\n","    \"\"\"\n","    for n, (w_t, t_t) in enumerate(zip(indexed_df['indexed_words'],indexed_df['indexed_tags'])):\n","        if not normal_df.loc[n,'words'] == [int2word[indexed_word] for indexed_word in w_t]:\n","            print('words numberization gone wrong') \n","            return False\n","        if not normal_df.loc[n,'tags'] == [int2tag[indexed_tag] for indexed_tag in t_t]:\n","            print('tags numberization gone wrong')\n","            return False \n","    \n","    print('\\nAll right with dataset numberization')\n","    print('Saving indexed dataframe')\n","    \n","    indexed_df.to_pickle(indexed_df_path)\n","\n","\n","indexed_df = build_indexed_dataframe(word2int,tag2int,df)\n","check_dataframe_numberization(indexed_df,df, int2word, int2tag)\n"]},{"cell_type":"markdown","metadata":{"id":"MQvHxKRcO0l8"},"source":["# [Task 2 - 0.5 points] Text encoding\n","\n","To train a neural POS tagger, you first need to encode text into numerical format."]},{"cell_type":"markdown","metadata":{"id":"0EnUuOWTO0l9"},"source":["### Instructions\n","\n","* Embed words using **GloVe embeddings**.\n","* You are **free** to pick any embedding dimension.\n","* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206458,"status":"ok","timestamp":1699279728592,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"eVsfe-QKO0l9","outputId":"15f823d9-6f5c-47dc-c24d-cbd4f892bdac"},"outputs":[],"source":["import torch\n","from torchtext.vocab import GloVe\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","embedding_dimension = 300\n","\n","glove_embeddings = GloVe(name='6B', dim=embedding_dimension)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving embedding matrix\n","Embedding matrix shape: (10948, 300)\n"]}],"source":["def build_embedding_matrix(emb_model, word2int):\n","    \"\"\"\n","        Given the embedding model and the dict. word2int. If there is the embedding for the word, we add it to the embedding_matrix. In negative case we put a list of random values.\n","        Return the embedding matrix\n","    \"\"\"\n","    #check_value_distribution_glove(emb_model)\n","   \n","    embedding_dimension = len(emb_model[0]) #how many numbers each emb vector is composed of                                                           \n","    embedding_matrix = np.zeros((len(word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n","\n","    for word, idx in word2int.items():\n","        if word in emb_model.stoi:\n","            embedding_matrix[idx] = emb_model[word]\n","        else:\n","            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n","            \n","    print('Saving embedding matrix')\n","    path = os.path.join(data_folder, \"emb_matrix\")\n","    np.save(path,embedding_matrix,allow_pickle=True)\n","\n","    print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n","\n","    return embedding_matrix\n","\n","embedding_matrix = build_embedding_matrix(glove_embeddings, word2int)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading embedding matrix\n","Loading numberized dataset\n","Loading dictionaries\n","All data loaded\n"]}],"source":["def load_data():\n","    \"\"\"\n","        Loads the data \"emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag \" and returns them\n","    \"\"\"\n","    emb_matrix_path = os.path.join(data_folder,'emb_matrix.npy')\n","    indexed_dataset_path = os.path.join(data_folder,'indexed_dataset.pkl')\n","    dictionaries_path = os.path.join(data_folder,'dictionaries.pkl')\n","\n","    if os.path.exists(emb_matrix_path) and os.path.exists(indexed_dataset_path):\n","        print('Loading embedding matrix')\n","        emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n","        print('Loading numberized dataset')\n","        indexed_dataset = pd.read_pickle(indexed_dataset_path)\n","        print('Loading dictionaries')\n","        with open(dictionaries_path, 'rb') as f:\n","            word2int,int2word,tag2int,int2tag = pickle.load(f)\n","        \n","        print('All data loaded')\n","    else:\n","        print('What you are looking for is not present in the folder')\n","        emb_matrix, indexed_dataset = None, None\n","\n","    return emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag\n","\n","emb_matrix, indexed_dataset, word2int, int2word, tag2int, int2tag = load_data()"]},{"cell_type":"markdown","metadata":{"id":"N4AChgkfO0l_"},"source":["# [Task 3 - 1.0 points] Model definition\n","\n","You are now tasked to define your neural POS tagger."]},{"cell_type":"markdown","metadata":{"id":"jkERwLU7O0l_"},"source":["### Instructions\n","\n","* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n","* You are **free** to experiment with hyper-parameters to define the baseline model.\n","\n","* **Model 1**: add an additional LSTM layer to the Baseline model.\n","* **Model 2**: add an additional Dense layer to the Baseline model.\n","\n","* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n","\n","**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."]},{"cell_type":"markdown","metadata":{"id":"babZmAQlWGIw"},"source":["### Embedding layer"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def create_emb_layer(weights_matrix, pad_idx = 0):\n","    \"\"\"\n","        Creates and returns the embedding layer\n","    \"\"\"\n","    matrix = torch.Tensor(weights_matrix)   #the embedding matrix \n","    _ , embedding_dim = matrix.shape \n","    emb_layer = nn.Embedding.from_pretrained(matrix, freeze=True, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable \n","    return emb_layer, embedding_dim"]},{"cell_type":"markdown","metadata":{},"source":["### Baseline model"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"RSGVLm3iWF2a"},"outputs":[],"source":["class Baseline(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer(packed_sentences)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output\n","\n","        "]},{"cell_type":"markdown","metadata":{"id":"Zd-pJQyu6h-V"},"source":["### Model 1"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"4pOy7gXR62UC"},"outputs":[],"source":["class Model1(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer_1 = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.bidirectional_layer_2 = nn.LSTM(bidirectional=True, input_size=lstm_dimension*2, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer_1(packed_sentences)\n","        packed_output, _ = self.bidirectional_layer_2(packed_output)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"BY9Vy9997y5Q"},"source":["### Model 2"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"_RM739C970T5"},"outputs":[],"source":["class Model2(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=embedding_dimension, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer_1 = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.dense_layer_2 = nn.Linear(in_features=dense_dimension, out_features=dense_dimension)\n","        self.embedding_layer, self.embedding_dim = create_emb_layer(embedding_matrix)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer(packed_sentences)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer_1(output)\n","        output = self.dense_layer_2(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"p9vgsKiaO0mA"},"source":["# [Task 4 - 1.0 points] Metrics\n","\n","Before training the models, you are tasked to define the evaluation metrics for comparison."]},{"cell_type":"markdown","metadata":{"id":"q_T6Bm1fO0mA"},"source":["### Instructions\n","\n","* Evaluate your models using macro F1-score, compute over **all** tokens.\n","* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n","* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"]},{"cell_type":"markdown","metadata":{"id":"NpoB8xTRO0mA"},"source":["**Note**: What about OOV tokens?\n","   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n","   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n","   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Xx7AKsk69Zcw"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def accuracy_and_f1(y_pred, y_true):\n","    correct = y_pred.eq(y_true)          \n","    acc = correct.sum()/y_true.shape[0] \n","    f1 = f1_score(y_true,y_pred,average='macro')\n","    return acc,f1"]},{"cell_type":"markdown","metadata":{},"source":["### Let's check our OOV tokens"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of unique words in dataset: 10947\n","['2691.19', 'language-housekeeper', '372.9', '95.09', '13\\\\/16', 'wheel-loader', 'news-american', '1206.26', 'chilver', 'pension-fund', 'ingersoll-rand', '-lcb-', 'hart-scott-rodino', 'crane-safety', 'intellectual-property', 'boogieman', 'life-of-contract', 'pro-forma', 'chemplus', '7\\\\/16', 'futures-related', 'marketing-communications', '237-seat', 'car-development', 'chong-sik', 'non-biodegradable', 'recession-inspired', 'phacoflex', 'tete-a-tete', 'propagandizes', 'stock-index', 'bell-ringer', 'shokubai', 'collective-bargaining', 'prudential-bache', '35564.43', 'profit-taking', 'quantitive', 'asset-valuation', '188.84', 'several-year', 'nylev', '35500.64', 'stock-selection', '100,980', '11-month-old', '34.625', 'trettien', 'pro-iranian', 'heebie-jeebies', 'ghkm', '29year', 'million-a-year', '5.435', 'yen-support', 'anti-programmers', 'arbitrage-related', 'uzi-model', 'headcount-control', '82,389', 'red-flag', '1.637', 'enzor', 'savings-and-loan', 'anti-program', '62.625', 'sub-markets', 'more-advanced', 'breakey', '11,390,000', 'nipponese', '1.457', 'wine-buying', '16\\\\/32', '71,309', 'ft-se', 'mariotta', 'lobsenz', '9,118', '12\\\\/32', 'heavy-truck', 'less-than-brilliant', '434.4', '55-a-share', 'drag-down', 'house-senate', '382-37', 'one-house', 'one-country', 'test-coaching', 'ntg', 'sulfur-dioxide', 'junk-bond', 'antitrust-law', '143.80', 'food-industry', 'stirlen', 'industrial-production', '4,393,237', 'pattenden', 'non-callable', '2160.1', 'band-wagon', 'larger-than-normal', 'garden-variety', 'incentive-backed', 'nearly-30', 'less-serious', 'secilia', 'auto-safety', 'manmade-fiber', 'four-foot-high', '3,250,000', 'free-enterprise', 'test-drive', '3648.82', '16,072', '100-megabyte', 'derchin', '271-147', 'motor-home', 'high-balance', 'double-c', 'test-prep', 'anti-takeover', 'identity-management', 'stock-specialist', 'minicrash', 'training-wage', 'sub-segments', 'ex-dividend', 'test-practice', 'dydee', '8.575', '737.5', '377.60', 'mortgage-based', 'makato', '520-lawyer', 'lentjes', '-lrb-', '30,841', 'cash-rich', 'lezovich', 'vinken', 'eight-count', 'coche-dury', 'roof-crush', '-rrb-', '38.875', 'interleukin-3', 'security-type', '446.62', '319.75', 'newspaper-printing', '236.74', 'chafic', '18-a-share', 'shirt-sleeved', 'triple-a-rated', 'nih-appointed', 'test-preparation', 'prevalance', '38.375', 'delwin', 'mutual-fund', 'asset-sale', '449.04', 'write-downs', 'money-center', 'summer\\\\/winter', 'guber\\\\/peters', '292.32', 'lafite-rothschild', 'flightiness', '6\\\\/2', 'student-test', 'staff-reduction', 'abortion-related', 'above-market', 'detective-story', '13,056', '4.898', 'bridgestone\\\\/firestone', 'durable-goods', 'senate-house', 'centerbank', 'hadson', 'insider-trading', 'co-developers', 'passenger-car', '11,762', 'year-earlier', 'gates-warren', 'equal-opportunity', '127.03', '234.4', 'acid-rain', 'credit-rating', 'norwick', 'computer-services', '3\\\\/8', '126.15', 'circuit-breaker', '230-215', 'rey\\\\/fawcett', 'multi-crystal', 'anku', 'odd-sounding', 'chinchon', 'index-related', '1.5755', 'constitutional-law', 'mininum-wage', 'ballantine\\\\/del', '1\\\\/10th', 'herald-american', 'high-rate', 'c.j.b.', 'polyproplene', 'pennview', 'fetal-tissue', 'louisiana-pacific', 'diloreto', 'drobnick', 'one-newspaper', 'noncompetitively', 'waertsilae', 'ednie', '300-a-share', 'buttoned-down', 'post-hearing', 'sanderoff', 'forest-product', 'launch-vehicle', 'trockenbeerenauslesen', 'nissho-iwai', 'subminimum', 'cost-control', 'year-ago', 'home-market', 'anti-deficiency', 'money-market', 'weisfield', 'automotive-lighting', 'foreign-stock', 'telephone-information', 'landonne', 'jerritts', 'super-absorbent', 'macmillan\\\\/mcgraw', '618.1', 'tarwhine', 'lower-priority', '6,799', '341.20', 'executive-office', '131.01', 'anti-miscarriage', '120-a-share', 'pramual', '734.9', 'property\\\\/casualty', 'investor-relations', '967,809', '20-stock', 'electrical-safety', 'seven-million-ton', '300-day', '190-point', 'monchecourt', '158,666', 'corn-buying', 'incentive-bonus', 'midwesco', 'industry-supported', 'floating-rate', '47.125', 'school-improvement', 'electric-utility', 'acquisition-minded', '129.91', 'nesb', 'merger-related', 'inter-tel', 'government-certified', '278.7', 'one-yen', '5\\\\/8', '1928-33', '300-113', '142.84', 'early-retirement', 'besuboru', '7\\\\/8', 'amphobiles', 'c.d.s', 'bellringers', '26,956', '5,699', 'insurance-company', 'corporate-wide', '5.276', 'new-home', 'cleaner-burning', 'mutchin', 'highest-pitched', 'sticker-shock', '14,821', '608,413', '220.45', 'meinders', 'yttrium-containing', 'labor-backed', 'automotive-parts', 'information-services', '0.0085', 'yet-to-be-formed', '143.93', '170,262', 'subskill', 'single-lot', 'unfair-trade', 'beer-belly', '1738.1', '2\\\\/32', 'lightning-fast', '90-cent-an-hour', 'radio-station', 'pricings', 'war-rationed', 'high-rolling', 'triple-c', 'two-time-losers', 'custom-chip', 'bankruptcy-law', 'kalipharma', 'cop-killer', 'waymar', '236.79', 'colorliner', '630.9', 'product-design', 'intelogic', 'small-company', 'retin-a', 'akerfeldt', 'rapanelli', 'housing-assistance', '40-megabyte', 'computer-driven', 'direct-investment', 'disputada', 'freudtoy', 'price-support', 'sell-offs', 'lap-shoulder', '1\\\\/8', 'jalaalwalikraam', 'most-likely-successor', '238,000-circulation', 'pre-1933', '361.8', 'veraldi', 'yoshihashi', '226,570,380', 'corton-charlemagne', '456.64', '5.2180', 'sino-u.s.', 'micronite', 'school-research', '1.8415', 'northy', 'capital-gains', 'market-share', 'three-lawyer', '9\\\\/32', 'mega-stadium', 'higher-salaried', 'big-ticket', '2,050-passenger', '271,124', '415.6', 'church-goers', 'red-blooded', 'bronces', 'school-district', 're-thought', 'cents-a-unit', '7.272', 'energy-services', 'heiwado', '14\\\\/32', 'dead-eyed', 'mehrens', 'contingency-fee', 'substance-abusing', '2,303,328', '494.50', 'top-yielding', 'replacement-car', 'subindustry', 'teacher-cadet', 'cotran', '23,403', 'reupke', 'disaster-assistance', 'purhasing', 'prior-year', 'price-depressing', 'twin-jet', 'romanee-conti', '2163.2', 'capital-markets', 'tire-kickers', 'express-buick', 'pre-1917', '8.467', 'sometimes-exhausting', '387.8', 'new-car', 'wheeland', '1937-40', 'state-supervised', 'six-packs', 'n.v', '1.916', 'macmillan\\\\/mcgraw-hill', 'near-limit', 'nagymaros', 'macheski', 'much-larger', 'crocidolite', 'sometimes-tawdry', 'sept.30', '361,376', '374.19', '16.125', 'life-insurance', 'sogo-shosha', 'card-member', 'then-speaker', 'building-products', 'walbrecher', 'hallwood', 'glenham', 'buy-outs', '37-a-share', '3,288,453', 'conn.based', '3.253', 'iran\\\\/contra', 'front-seat', 'vitulli', 'crystal-lattice', '352.9', 'cash-and-stock', 'car-safety', 'per-share', 'one-upsmanship', '8300s', 'equity-purchase', \"creator's\", 'money-fund', 'lynch-mob', 'hasbrouk', 'twindam', 'certin', 'index-options', 'sidak', 'ac-130u', 'melt-textured', 'disputado', 'veselich', 'c-90', '415.8', 'parts-engineering', 'biondi-santi', 'superpremiums', 'intecknings', 'middle-ground', 'wamre', 'derel', 'college-bowl', '352.7', 'old-house', 'minimum-wage', 'unicorp', 'rate-sensitive', 'seven-yen', 'investment-grade', 'sewing-machine', '374.20', 'satrum', 'yeargin', '13.625', '3\\\\/4', 'computer-system-design', 'malizia', '3,040,000', 'index-fund', 'crookery', 'non-encapsulating', 'anti-morning-sickness', 'savers\\\\/investors', 'pathlogy', '36-store', 'weapons-modernization', 'boorse', 'rope-sight', 'revenue-desperate', '3436.58', 'fiber-end', 'hummerstone', 'anti-china', 'side-crash', 'sacramento-based', '62%-owned', 'low-ability', 'school-board', 'we-japanese', '2141.7', 'airline-related', 'diceon', 'univest', '11\\\\/16', 'ensrud', 'dollar-yen', 'arighi', 'family-planning', 'low-ball', 'clean-air', 'video-viewing', 'circuit-board', 'protein-1', 'continuingly', 'dust-up', 'high-polluting', '436.01', 'securities-based', 'arbitraging', 'preparatives', 'thin-lipped', 'erbamont', '2645.90', 'cup-tote', 'achievement-test', '84-month', 'trading-company', '497.34', 'built-from-kit', 'car-care', 'samnick', '7.422', 'machine-gun-toting', '372.14', 'stock-picking', 'index-arbitrage', 'copper-rich', 'land-idling', 'freshbake', 'aslacton', '-rcb-', 'bald-faced', 'bottom-line', 'food-shop', '7.458', 'mouth-up', 'prize-fighter', 'stock-manipulation', 'wtd', '19-month-old', 'rexinger', '70-a-share', 'yen-denominated', 'nofzinger', '22\\\\/32', 'anti-abortionists', 'bank-backed', '500,004', 'colonsville', 'synergistics', 'bumkins', 'tiphook', 'integra-a', 'flim-flammery', 'change-ringing', 'alurralde', 'takeover-stock', 'more-efficient', '87-store', '1\\\\/4', 'three-sevenths', 'blue-chips', 'safe-deposit', '3057', 'egnuss', 'forest-products', 'autions', '14.', 'lookee-loos', 'ctbs', 'purepac', '43.875', '263.07', 'wfrr', '1\\\\/2', 'market-makers', 'johnson-era', 'nekoosa', 'sport-utility', 'truth-in-lending', 'shareholder-rights', '566.54', 'foldability', 'ratners', '45-a-share', 'txo', 'foreign-led', 'sharedata', '18,444', 'tissue-transplant', 'exxon-owned', '95,142', '2003\\\\/2007', 'severable', '705.6', '142.85', 'pianist-comedian', 'g.m.b', '30,537', 'citizen-sparked', '154,240,000', 'two-sevenths', '83,206', 'stock-price', 'program-trading', 'light-truck', 'odd-year', 'moleculon', '50\\\\/50', 'times-stock', 'blood-cell', '877,663', 'solaia', 'superdot', 'ariail', 'rubinfien', '1.1650', 'deposits-a', 'subskills', 'cray-3', 'financial-services', 'scypher', 'when-issued', 'morale-damaging', 'unenticing', '69-point', 'water-authority', 'search-and-seizure', 'gingl', 'limited-partnership', 'liquid-nitrogen', '143.08', 'six-bottle', 'bermuda-based', 'bread-and-butter', 'potables', 'social-studies', '12,252', '811.9', 'greenmailer', 'long-tenured', 'page-one', 'muscolina']\n","Total OOV terms: 676 (6.18%)\n","Some OOV terms: ['unicorp', 'front-seat', 'video-viewing', 'electrical-safety', 'savings-and-loan', 'freshbake', 'intellectual-property', 'food-industry', 'purhasing', 'moleculon', 'language-housekeeper', 'severable', 'savers\\\\/investors', '630.9', 'super-absorbent']\n"]}],"source":["def check_OOV_terms(embedding_model, unique_words, lower):\n","    oov_words = []\n","    int_oov_words = []\n","\n","    if lower:\n","        words = set([x.lower() for x in unique_words])\n","    else: \n","        words = unique_words\n","\n","    for word in words:\n","        if word not in embedding_model.itos:\n","           oov_words.append(word) \n","           int_oov_words.append(word2int[word]) \n","    \n","    print(\"Total number of unique words in dataset:\",len(words))\n","    print(oov_words)\n","    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(words))*100))\n","    print(\"Some OOV terms:\",random.sample(oov_words,15))\n","    \n","    return oov_words, int_oov_words\n","\n","oov_words, int_oov_words = check_OOV_terms(glove_embeddings, unique_words,True)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see the number of OOV words is very high (31.29% of the total words in the corpus). So, we have to find a way to deal with them.<br>\n","A good idea could be to use lowercased words, in fact in this way the number of OOV decreases to 6.18%, but we have to be careful because we could lose some information. <br>\n","For example, the word \"Pierre\" is a name, but \"pierre\" is a noun. So, we have to find a way to deal with this problem.<br>\n","We decided to test the model with the two different approaches and see which one is better."]},{"cell_type":"markdown","metadata":{"id":"hy2OFkD_O0mA"},"source":["# [Task 5 - 1.0 points] Training and Evaluation\n","\n","You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."]},{"cell_type":"markdown","metadata":{"id":"_G20Sgu1O0mB"},"source":["### Instructions\n","\n","* Train **all** models on the train set.\n","* Evaluate **all** models on the validation set.\n","* Compute metrics on the validation set.\n","* Pick **at least** three seeds for robust estimation.\n","* Pick the **best** performing model according to the observed validation set performance."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def initialize_weights(model):\n","    for _, param in model.named_parameters():\n","        if isinstance(model, nn.LSTM) or isinstance(model, nn.Linear):\n","            nn.init.normal_(param.data, mean = 0, std = 0.1)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def number_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def get_to_be_masked_tags():\n","    punctuation_tags = ['$', '``', '.', ',', '#', 'SYM', ':', \"''\",'-RRB-','-LRB-']   #tags to be masked \n","    token_punctuation = [tag2int[tag] for tag in punctuation_tags]\n","    return torch.LongTensor(token_punctuation+[0])\n","\n","to_mask = get_to_be_masked_tags()\n","\n","def reshape_and_mask(predictions,targets): \n","    non_masked_elements = torch.isin(targets, to_mask, invert=True)\n","    \n","    return predictions[non_masked_elements],targets[non_masked_elements]\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"BdvD5aq-EiHV"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class PosDataset(Dataset):\n","    def __init__(self, text, labels):\n","        self.labels = labels\n","        self.text = text\n","        self.sentence_lengths = [len(sentence) for sentence in self.text]\n","    def __len__(self):\n","            return len(self.labels)\n","    def __getitem__(self, idx):\n","            label = self.labels[idx]\n","            text = self.text[idx]\n","            sample = (text, label, self.sentence_lengths[idx])\n","            return sample\n","\n","\n","def collate_fn(data):\n","    return ([x[0] for x in data], [x[1] for x in data], [x[2] for x in data])\n","\n","\n","def create_dataloaders(b_s : int):\n","    train_df = indexed_dataset[indexed_dataset['split'] == 'train'].reset_index(drop=True)      \n","    val_df = indexed_dataset[indexed_dataset['split'] == 'val'].reset_index(drop=True)\n","    test_df = indexed_dataset[indexed_dataset['split'] == 'test'].reset_index(drop=True)\n","\n","    #create DataframeDataset objects for each split \n","    train_dataset = PosDataset(train_df.iloc[:,2],train_df.iloc[:,3])\n","    val_dataset = PosDataset(val_df.iloc[:,2],val_df.iloc[:,3])\n","    test_dataset = PosDataset(test_df.iloc[:,2],test_df.iloc[:,3])\n","\n","    train_dataloader = DataLoader(train_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","    val_dataloader = DataLoader(val_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","    test_dataloader = DataLoader(test_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","\n","    return train_dataloader,val_dataloader,test_dataloader "]},{"cell_type":"code","execution_count":19,"metadata":{"id":"wuKaQtDuGAia"},"outputs":[],"source":["batch_size = 32\n","\n","tr_dl, val_dl, test_dl = create_dataloaders(batch_size)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"xzlrW52UIRet"},"outputs":[],"source":["from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import torch.nn.utils.rnn as rnn\n","import time\n","\n","def train(model, epochs, loss_function, dataloader, optimizer, padding_value = 0, seed):\n","    model.train()\n","    for epoch in range(epochs):\n","        start_time = time.time()\n","        for sentences, pos, s_len in dataloader:\n","            optimizer.zero_grad()\n","            \n","            tensor_sentences = [torch.LongTensor(s) for s in sentences]\n","            tensor_pos = [torch.LongTensor(p) for p in pos]\n","\n","            padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = 0)\n","            padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n","\n","            predicted = model(padded_sentences, s_len)\n","\n","            predicted = predicted.view(-1,predicted.shape[-1])    \n","            targets = padded_pos.view(-1)\n","\n","            predicted, targets = reshape_and_mask(predicted, targets)\n","\n","            loss = loss_function(predicted, targets)\n","            loss.backward()\n","            optimizer.step()\n","        elapsed = time.time() - start_time \n","        print(f'Train epoch [{epoch+1}/{epochs}] loss: {loss.item()} time: {elapsed:.2f}s')\n","\n","def evaluate(model, loss_function, dataloader, padding_value=0):\n","    model.eval()\n","    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n","    epoch_loss = 0\n","    for sentences, pos, s_len in dataloader:\n","        tensor_sentences = [torch.LongTensor(s) for s in sentences]\n","        tensor_pos = [torch.LongTensor(p) for p in pos]\n","\n","        padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = 0)\n","        padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n","\n","        predicted = model(padded_sentences, s_len)\n","        predicted = predicted.view(-1,predicted.shape[-1])    \n","        targets = padded_pos.view(-1)\n","\n","        predicted, targets = reshape_and_mask(predicted, targets)\n","\n","        loss = loss_function(predicted, targets)\n","\n","        predicted = predicted.argmax(dim=1)\n","\n","        tot_pred = torch.cat((tot_pred,predicted))\n","        tot_targ = torch.cat((tot_targ,targets))\n","\n","        epoch_loss += loss.item()\n","    full_accuracy, full_f1 = accuracy_and_f1(tot_pred,tot_targ)\n","    print(f'Eval: loss: {epoch_loss:.2f} accuracy: {full_accuracy:.2f} f1: {full_f1:.2f}')\n","    return full_accuracy,full_f1,tot_pred,tot_targ\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"elapsed":712,"status":"error","timestamp":1699270756409,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"_E-2S0beJm-0","outputId":"5b3a2d5a-4f15-4ba6-a6dc-edd45a02a17f"},"outputs":[{"data":{"text/plain":["Model2(\n","  (bidirectional_layer): LSTM(300, 16, batch_first=True, bidirectional=True)\n","  (dense_layer_1): Linear(in_features=32, out_features=46, bias=True)\n","  (dense_layer_2): Linear(in_features=46, out_features=46, bias=True)\n","  (embedding_layer): Embedding(10948, 300, padding_idx=0)\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["loss_function = CrossEntropyLoss()\n","\n","\n","lstm_dimension = 16\n","dense_dimension = len(unique_tags)+1\n","\n","baseline_model = Baseline(lstm_dimension, dense_dimension)\n","baseline_model.apply(initialize_weights)\n","\n","double_lstm_model = Model1(lstm_dimension, dense_dimension)\n","double_lstm_model.apply(initialize_weights)\n","\n","double_dense_model = Model2(lstm_dimension, dense_dimension)\n","double_dense_model.apply(initialize_weights)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters in baseline model: 42222\n","Number of parameters in double lstm model: 48622\n","Number of parameters in double dense model: 44384\n"]}],"source":["print(f'Number of parameters in baseline model: {number_parameters(baseline_model)}')\n","print(f'Number of parameters in double lstm model: {number_parameters(double_lstm_model)}')\n","print(f'Number of parameters in double dense model: {number_parameters(double_dense_model)}')"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["weight_folder = \"./weights\"\n","def save_weights(model, model_name):\n","    \"\"\"\n","        Saves the model weights in the folder \"weights\"\n","    \"\"\"\n","    path = os.path.join(weight_folder, model_name)\n","    torch.save(model.state_dict(), path)\n","\n","def load_weights(model, model_name):\n","    \"\"\"\n","        Loads the model weights from the folder \"weights\"\n","    \"\"\"\n","    path = os.path.join(weight_folder, model_name)\n","    model.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [1/50] loss: 3.017608404159546 time: 3.67s\n","Train epoch [2/50] loss: 2.545351266860962 time: 3.01s\n","Train epoch [3/50] loss: 2.2228682041168213 time: 2.97s\n","Train epoch [4/50] loss: 2.0715620517730713 time: 3.02s\n","Train epoch [5/50] loss: 1.9211663007736206 time: 3.08s\n","Train epoch [6/50] loss: 1.6415138244628906 time: 3.01s\n","Train epoch [7/50] loss: 1.524673342704773 time: 2.68s\n","Train epoch [8/50] loss: 1.443713665008545 time: 2.89s\n","Train epoch [9/50] loss: 1.3459240198135376 time: 2.88s\n","Train epoch [10/50] loss: 1.067432165145874 time: 3.21s\n","Train epoch [11/50] loss: 1.2001962661743164 time: 3.16s\n","Train epoch [12/50] loss: 0.9225680232048035 time: 3.08s\n","Train epoch [13/50] loss: 0.9394091367721558 time: 3.33s\n","Train epoch [14/50] loss: 0.8425342440605164 time: 2.94s\n","Train epoch [15/50] loss: 0.8610190749168396 time: 3.02s\n","Train epoch [16/50] loss: 0.9595901966094971 time: 3.05s\n","Train epoch [17/50] loss: 0.6177699565887451 time: 2.71s\n","Train epoch [18/50] loss: 0.9604779481887817 time: 2.84s\n","Train epoch [19/50] loss: 0.7100412845611572 time: 3.09s\n","Train epoch [20/50] loss: 0.7056536674499512 time: 2.88s\n","Train epoch [21/50] loss: 0.6783331632614136 time: 2.65s\n","Train epoch [22/50] loss: 0.7020344734191895 time: 2.68s\n","Train epoch [23/50] loss: 0.695562481880188 time: 2.66s\n","Train epoch [24/50] loss: 0.6777469515800476 time: 3.10s\n","Train epoch [25/50] loss: 0.7175003886222839 time: 3.27s\n","Train epoch [26/50] loss: 0.5395486950874329 time: 2.80s\n","Train epoch [27/50] loss: 0.37741619348526 time: 2.96s\n","Train epoch [28/50] loss: 0.4671040177345276 time: 3.07s\n","Train epoch [29/50] loss: 0.6504510641098022 time: 2.94s\n","Train epoch [30/50] loss: 0.4883417785167694 time: 3.16s\n","Train epoch [31/50] loss: 0.4072501063346863 time: 2.92s\n","Train epoch [32/50] loss: 0.4452451169490814 time: 3.16s\n","Train epoch [33/50] loss: 0.2919303774833679 time: 3.35s\n","Train epoch [34/50] loss: 0.4535902142524719 time: 3.39s\n","Train epoch [35/50] loss: 0.38338467478752136 time: 3.44s\n","Train epoch [36/50] loss: 0.39813458919525146 time: 3.32s\n","Train epoch [37/50] loss: 0.36789947748184204 time: 3.38s\n","Train epoch [38/50] loss: 0.3722873628139496 time: 3.08s\n","Train epoch [39/50] loss: 0.32520338892936707 time: 3.04s\n","Train epoch [40/50] loss: 0.3754589557647705 time: 3.06s\n","Train epoch [41/50] loss: 0.3111255168914795 time: 3.09s\n","Train epoch [42/50] loss: 0.3820657432079315 time: 3.14s\n","Train epoch [43/50] loss: 0.4239235520362854 time: 3.35s\n","Train epoch [44/50] loss: 0.4589430093765259 time: 3.18s\n","Train epoch [45/50] loss: 0.3404945731163025 time: 3.66s\n","Train epoch [46/50] loss: 0.2809849977493286 time: 3.15s\n","Train epoch [47/50] loss: 0.41509297490119934 time: 3.12s\n","Train epoch [48/50] loss: 0.35738104581832886 time: 3.10s\n","Train epoch [49/50] loss: 0.2728104293346405 time: 3.27s\n","Train epoch [50/50] loss: 0.27108490467071533 time: 3.34s\n"]}],"source":["epochs = 50\n","optimizer = Adam(baseline_model.parameters(), lr=5e-4)\n","\n","train(baseline_model, epochs, loss_function, tr_dl, optimizer)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [1/50] loss: 3.01688289642334 time: 5.85s\n","Train epoch [2/50] loss: 2.6593449115753174 time: 5.29s\n","Train epoch [3/50] loss: 2.752267837524414 time: 5.81s\n","Train epoch [4/50] loss: 2.7615883350372314 time: 5.22s\n","Train epoch [5/50] loss: 2.5704667568206787 time: 5.13s\n","Train epoch [6/50] loss: 2.2798640727996826 time: 5.96s\n","Train epoch [7/50] loss: 1.9929006099700928 time: 5.38s\n","Train epoch [8/50] loss: 1.7936513423919678 time: 5.33s\n","Train epoch [9/50] loss: 1.795961856842041 time: 5.14s\n","Train epoch [10/50] loss: 1.4651871919631958 time: 5.21s\n","Train epoch [11/50] loss: 1.4048948287963867 time: 5.26s\n","Train epoch [12/50] loss: 1.2603811025619507 time: 6.00s\n","Train epoch [13/50] loss: 1.2478976249694824 time: 5.45s\n","Train epoch [14/50] loss: 1.163589596748352 time: 5.30s\n","Train epoch [15/50] loss: 0.9240286946296692 time: 5.44s\n","Train epoch [16/50] loss: 0.9154030084609985 time: 5.25s\n","Train epoch [17/50] loss: 1.0240864753723145 time: 5.56s\n","Train epoch [18/50] loss: 0.6122284531593323 time: 5.61s\n","Train epoch [19/50] loss: 0.8273354768753052 time: 5.43s\n","Train epoch [20/50] loss: 0.7887446880340576 time: 5.55s\n","Train epoch [21/50] loss: 0.7247347235679626 time: 5.70s\n","Train epoch [22/50] loss: 0.6917254328727722 time: 5.25s\n","Train epoch [23/50] loss: 0.6196427345275879 time: 5.67s\n","Train epoch [24/50] loss: 0.5782877206802368 time: 5.63s\n","Train epoch [25/50] loss: 0.6976796984672546 time: 5.40s\n","Train epoch [26/50] loss: 0.532345175743103 time: 5.34s\n","Train epoch [27/50] loss: 0.5829942226409912 time: 5.46s\n","Train epoch [28/50] loss: 0.4569432735443115 time: 6.14s\n","Train epoch [29/50] loss: 0.47234663367271423 time: 6.62s\n","Train epoch [30/50] loss: 0.5175538659095764 time: 6.33s\n","Train epoch [31/50] loss: 0.5920566320419312 time: 6.61s\n","Train epoch [32/50] loss: 0.5089215636253357 time: 6.13s\n","Train epoch [33/50] loss: 0.4512472152709961 time: 5.21s\n","Train epoch [34/50] loss: 0.45294883847236633 time: 5.72s\n","Train epoch [35/50] loss: 0.356976181268692 time: 5.46s\n","Train epoch [36/50] loss: 0.3885401785373688 time: 6.06s\n","Train epoch [37/50] loss: 0.35860511660575867 time: 6.03s\n","Train epoch [38/50] loss: 0.37449219822883606 time: 5.82s\n","Train epoch [39/50] loss: 0.31153953075408936 time: 6.34s\n","Train epoch [40/50] loss: 0.23756402730941772 time: 5.67s\n","Train epoch [41/50] loss: 0.2382829338312149 time: 5.77s\n","Train epoch [42/50] loss: 0.22906950116157532 time: 5.66s\n","Train epoch [43/50] loss: 0.20020106434822083 time: 5.18s\n","Train epoch [44/50] loss: 0.2766376733779907 time: 5.29s\n","Train epoch [45/50] loss: 0.3034743666648865 time: 5.78s\n","Train epoch [46/50] loss: 0.28805217146873474 time: 5.78s\n","Train epoch [47/50] loss: 0.25005653500556946 time: 5.63s\n","Train epoch [48/50] loss: 0.1934165358543396 time: 5.78s\n","Train epoch [49/50] loss: 0.2751582860946655 time: 5.47s\n","Train epoch [50/50] loss: 0.20883220434188843 time: 5.64s\n"]}],"source":["epochs = 50\n","optimizer = Adam(double_lstm_model.parameters(), lr=5e-4)\n","\n","train(double_lstm_model, epochs, loss_function, tr_dl, optimizer)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [1/50] loss: 2.7972629070281982 time: 3.19s\n","Train epoch [2/50] loss: 2.3723249435424805 time: 3.19s\n","Train epoch [3/50] loss: 1.8511394262313843 time: 3.08s\n","Train epoch [4/50] loss: 1.5592678785324097 time: 3.26s\n","Train epoch [5/50] loss: 1.351058840751648 time: 3.08s\n","Train epoch [6/50] loss: 1.2130072116851807 time: 2.81s\n","Train epoch [7/50] loss: 1.0376696586608887 time: 3.13s\n","Train epoch [8/50] loss: 0.835885226726532 time: 2.98s\n","Train epoch [9/50] loss: 0.7687786221504211 time: 3.44s\n","Train epoch [10/50] loss: 0.8114917874336243 time: 3.12s\n","Train epoch [11/50] loss: 0.6615330576896667 time: 3.32s\n","Train epoch [12/50] loss: 0.7295700907707214 time: 3.26s\n","Train epoch [13/50] loss: 0.3838130533695221 time: 3.35s\n","Train epoch [14/50] loss: 0.6530866622924805 time: 3.06s\n","Train epoch [15/50] loss: 0.43102726340293884 time: 3.08s\n","Train epoch [16/50] loss: 0.7163329124450684 time: 3.16s\n","Train epoch [17/50] loss: 0.5856622457504272 time: 2.98s\n","Train epoch [18/50] loss: 0.40148046612739563 time: 3.07s\n","Train epoch [19/50] loss: 0.33587539196014404 time: 3.03s\n","Train epoch [20/50] loss: 0.39468440413475037 time: 2.98s\n","Train epoch [21/50] loss: 0.3765321373939514 time: 3.07s\n","Train epoch [22/50] loss: 0.3615843653678894 time: 2.95s\n","Train epoch [23/50] loss: 0.3820578455924988 time: 2.98s\n","Train epoch [24/50] loss: 0.29618701338768005 time: 2.99s\n","Train epoch [25/50] loss: 0.22352401912212372 time: 2.91s\n","Train epoch [26/50] loss: 0.36321911215782166 time: 2.94s\n","Train epoch [27/50] loss: 0.28683391213417053 time: 2.82s\n","Train epoch [28/50] loss: 0.32167190313339233 time: 2.93s\n","Train epoch [29/50] loss: 0.19929398596286774 time: 3.01s\n","Train epoch [30/50] loss: 0.2074725329875946 time: 2.89s\n","Train epoch [31/50] loss: 0.30926206707954407 time: 2.82s\n","Train epoch [32/50] loss: 0.3098995089530945 time: 2.73s\n","Train epoch [33/50] loss: 0.22769559919834137 time: 2.83s\n","Train epoch [34/50] loss: 0.31214407086372375 time: 2.77s\n","Train epoch [35/50] loss: 0.2970026731491089 time: 2.78s\n","Train epoch [36/50] loss: 0.24237440526485443 time: 2.73s\n","Train epoch [37/50] loss: 0.18755468726158142 time: 2.74s\n","Train epoch [38/50] loss: 0.16312988102436066 time: 2.75s\n","Train epoch [39/50] loss: 0.18705272674560547 time: 2.74s\n","Train epoch [40/50] loss: 0.17892999947071075 time: 2.87s\n","Train epoch [41/50] loss: 0.23197489976882935 time: 2.82s\n","Train epoch [42/50] loss: 0.209657222032547 time: 3.03s\n","Train epoch [43/50] loss: 0.19442911446094513 time: 3.46s\n","Train epoch [44/50] loss: 0.21841038763523102 time: 3.13s\n","Train epoch [45/50] loss: 0.2612968981266022 time: 3.17s\n","Train epoch [46/50] loss: 0.2089720219373703 time: 3.22s\n","Train epoch [47/50] loss: 0.19119368493556976 time: 3.13s\n","Train epoch [48/50] loss: 0.10786217451095581 time: 3.52s\n","Train epoch [49/50] loss: 0.14953693747520447 time: 3.06s\n","Train epoch [50/50] loss: 0.11689044535160065 time: 3.25s\n"]}],"source":["epochs = 50\n","optimizer = Adam(double_dense_model.parameters(), lr=5e-4)\n","\n","train(double_dense_model, epochs, loss_function, tr_dl, optimizer)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Eval: loss: 18.15 accuracy: 0.88 f1: 0.67\n"]}],"source":["baseline_accuracy, baseline_f1, baseline_pred, baseline_targ = evaluate(baseline_model, loss_function, val_dl)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Eval: loss: 16.87 accuracy: 0.89 f1: 0.65\n"]}],"source":["double_lstm_accuracy, double_lstm_f1, double_lstm_pred, double_lstm_targ = evaluate(double_lstm_model, loss_function, val_dl)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Eval: loss: 16.50 accuracy: 0.89 f1: 0.74\n"]}],"source":["double_dense_accuracy, double_dense_f1, double_dense_pred, double_dense_targ = evaluate(double_dense_model, loss_function, val_dl)"]},{"cell_type":"markdown","metadata":{"id":"_sN60F5MO0mB"},"source":["# [Task 6 - 1.0 points] Error Analysis\n","\n","You are tasked to evaluate your best performing model."]},{"cell_type":"markdown","metadata":{"id":"5IvcbIwiO0mB"},"source":["### Instructions\n","\n","* Compare the errors made on the validation and test sets.\n","* Aggregate model errors into categories (if possible)\n","* Comment the about errors and propose possible solutions on how to address them."]},{"cell_type":"markdown","metadata":{"id":"32F8eX_hO0mB"},"source":["# [Task 7 - 1.0 points] Report\n","\n","Wrap up your experiment in a short report (up to 2 pages)."]},{"cell_type":"markdown","metadata":{"id":"xv4OVz1vO0mC"},"source":["### Instructions\n","\n","* Use the NLP course report template.\n","* Summarize each task in the report following the provided template."]},{"cell_type":"markdown","metadata":{"id":"LkAPKCbIO0mC"},"source":["### Recommendations\n","\n","The report is not a copy-paste of graphs, tables, and command outputs.\n","\n","* Summarize classification performance in Table format.\n","* **Do not** report command outputs or screenshots.\n","* Report learning curves in Figure format.\n","* The error analysis section should summarize your findings."]},{"cell_type":"markdown","metadata":{"id":"449lJeI-O0mC"},"source":["# Submission\n","\n","* **Submit** your report in PDF format.\n","* **Submit** your python notebook.\n","* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n","* You can upload **model weights** in a cloud repository and report the link in the report."]},{"cell_type":"markdown","metadata":{"id":"adKOVVP-O0mC"},"source":["# FAQ\n","\n","Please check this frequently asked questions before contacting us"]},{"cell_type":"markdown","metadata":{"id":"x9faC6QnO0mC"},"source":["### Trainable Embeddings\n","\n","You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."]},{"cell_type":"markdown","metadata":{"id":"KML9gLHgO0mC"},"source":["### Model architecture\n","\n","You **should not** change the architecture of a model (i.e., its layers).\n","\n","However, you are **free** to play with their hyper-parameters."]},{"cell_type":"markdown","metadata":{"id":"8cgg47FVO0mD"},"source":["### Neural Libraries\n","\n","You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"]},{"cell_type":"markdown","metadata":{"id":"6qmgkNGsO0mD"},"source":["### Keras TimeDistributed Dense layer\n","\n","If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."]},{"cell_type":"markdown","metadata":{"id":"AKxd8OlIO0mD"},"source":["### Error Analysis\n","\n","Some topics for discussion include:\n","   * Model performance on most/less frequent classes.\n","   * Precision/Recall curves.\n","   * Confusion matrices.\n","   * Specific misclassified samples."]},{"cell_type":"markdown","metadata":{"id":"_m5SkbO0O0mD"},"source":["### Punctuation\n","\n","**Do not** remove punctuation from documents since it may be helpful to the model.\n","\n","You should **ignore** it during metrics computation.\n","\n","If you are curious, you can run additional experiments to verify the impact of removing punctuation."]},{"cell_type":"markdown","metadata":{"id":"urd139anO0mD"},"source":["# The End"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
