{"cells":[{"cell_type":"markdown","metadata":{"id":"-WeCeITXoxLf"},"source":["# Assignment 1\n","\n","**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n","\n","**Keywords**: POS tagging, Sequence labelling, RNNs"]},{"cell_type":"markdown","metadata":{"id":"mEMlC0FYO0l2"},"source":["\n","# Contact\n","\n","For any doubt, question, issue or help, you can always contact us at the following email addresses:\n","\n","Teaching Assistants:\n","\n","* Federico Ruggeri -> federico.ruggeri6@unibo.it\n","* Eleonora Mancini -> e.mancini@unibo.it\n","\n","Professor:\n","\n","* Paolo Torroni -> p.torroni@unibo.it"]},{"cell_type":"markdown","metadata":{"id":"tDP-HaMpO0l3"},"source":["# Introduction\n","\n","You are tasked to address the task of POS tagging.\n","\n","<center>\n","    <img src=\"./images/pos_tagging.png\" alt=\"POS tagging\" />\n","</center>"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# best models is a dictionary to store the best models for each seed\n","# best losses keep track of the best loss for each seed after a training loop, so that we can re-train the model without \n","#   loosing the best model after the first iteration\n","# total epochs is a dictionary to keep track of the total epochs for each seed, just for information purposes\n","# run this cell only once\n","\n","best_models = {}\n","best_losses = {}\n","total_epochs = {}"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import random\n","import warnings\n","from collections import OrderedDict\n","import pickle\n","import re\n","import torch\n","from torchtext.vocab import GloVe\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import f1_score\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.utils.rnn as rnn\n","import time\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","from sklearn.metrics import confusion_matrix\n","from operator import itemgetter\n","import spacy\n","\n","SEED = 0\n","DATA_FOLDER = './data'\n","WEIGHTS_FOLDER = './weights'\n","\n","LOWER = True\n","\n","NUMBER = False\n","NUM_RE = r\"(\\d*\\,)?\\d+.\\d*\" \n","NUM_TOKEN = '<num>'\n","\n","NER = True\n","NER_TOKEN = {\n","    'CARDINAL' : '<num>',\n","    'ORG' : '<org>',\n","    'PERSON' : '<per>',\n","}\n","\n","\n","PAD_INDEX = 0\n","PAD_TOKEN = '<pad>'\n","\n","EMBEDDING_DIMENSION = 300\n","OOV_EMBEDDING_TYPE = 'random' # either \"mean\" or \"random\"\n","MEAN_EMBEDDING_WINDOW = 1\n","FIXED_OOV = False\n","FIXED_OOV_VECTOR = np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n","\n","BATCH_SIZE = 16\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","warnings.filterwarnings('ignore')\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"markdown","metadata":{"id":"sr8QdeOXO0l4"},"source":["# [Task 1 - 0.5 points] Corpus\n","\n","You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n","\n","**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n","\n","### Example\n","\n","```Pierre\tNNP\t2\n","Vinken\tNNP\t8\n",",\t,\t2\n","61\tCD\t5\n","years\tNNS\t6\n","old\tJJ\t2\n",",\t,\t2\n","will\tMD\t0\n","join\tVB\t8\n","the\tDT\t11\n","board\tNN\t9\n","as\tIN\t9\n","a\tDT\t15\n","nonexecutive\tJJ\t15\n","director\tNN\t12\n","Nov.\tNNP\t9\n","29\tCD\t16\n",".\t.\t8\n","```"]},{"cell_type":"markdown","metadata":{"id":"Dd_jkm9hO0l5"},"source":["### Splits\n","\n","The corpus contains 200 documents.\n","\n","   * **Train**: Documents 1-100\n","   * **Validation**: Documents 101-150\n","   * **Test**: Documents 151-199"]},{"cell_type":"markdown","metadata":{"id":"4Hx0FbERO0l6"},"source":["### Instructions\n","\n","* **Download** the corpus.\n","* **Encode** the corpus into a pandas.DataFrame object.\n","* **Split** it in training, validation, and test sets."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def mask_number(text):\n","    try:\n","        if re.match(NUM_RE, text):\n","            return NUM_TOKEN\n","        else:\n","            return text\n","    except:\n","        return text\n","\n","def mask_ner(text_tokens):\n","    text_tokens_str = [str(token) for token in text_tokens]\n","    text = ' '.join(text_tokens_str)\n","    doc = nlp(text)\n","    for ent in doc.ents:\n","      if ent.label_ in NER_TOKEN:\n","          for sub_token in ent.text.split():\n","            text_tokens[text_tokens == sub_token] = NER_TOKEN[ent.label_]\n","    return text_tokens"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def preprcess_dataset(df) -> pd.DataFrame:\n","  \"\"\"\n","    Preprocess the dataset, lowercase the words and/or mask the numbers\n","  \"\"\"\n","  if NER:\n","    df['word'] = mask_ner(df['word'])\n","  if LOWER: \n","    df['word'] = df[\"word\"].str.lower()\n","  if NUMBER:\n","    df['word'] = df['word'].apply(mask_number)\n","  return df\n","\n","def encode_dataset(dataset_name: str) -> pd.DataFrame:\n","  \"\"\"\n","    Encode the dataset as a pandas dataframe, each row is a sentence, each sentence is a list of words and a list of corresponding tags\n","  \"\"\"\n","  dataset_folder = os.path.join(DATA_FOLDER+ \"/dataset\")\n","  \n","  dataframe_rows = []\n","  unique_tags = set()             \n","  unique_words = set()\n","\n","  for doc in sorted(os.listdir(dataset_folder)):\n","    if doc.endswith(\".csv\") or doc.endswith(\".pkl\"): continue\n","    doc_num = int(doc[5:8])\n","    doc_path = os.path.join(dataset_folder,doc)\n","\n","    with open(doc_path, mode='r', encoding='utf-8') as file:\n","      df = pd.read_csv(file,sep='\\t', header=None, skip_blank_lines=False)\n","      df.rename(columns={0:'word', 1:\"TAG\", 2:\"remove\"}, inplace=True)\n","      df.drop(\"remove\", axis=1, inplace=True)\n","\n","      df = preprcess_dataset(df)\n","      \n","      df[\"group_num\"] = df.isnull().all(axis=1).cumsum()\n","      df.dropna(inplace=True)\n","      df.reset_index(drop=True, inplace=True)\n","      \n","      unique_tags.update(df['TAG'].unique()) \n","      unique_words.update(df['word'].unique()) \n","\n","      df_list = [df.iloc[rows] for _, rows in df.groupby('group_num').groups.items()]\n","      for n,d in enumerate(df_list):\n","          dataframe_row = {\n","              \"split\" : 'train' if doc_num<=100 else ('val' if doc_num<=150  else 'test'),\n","              \"doc_id\" : doc_num,\n","              \"sentence_num\" : n,\n","              \"words\": d['word'].tolist(),\n","              \"tags\":  d['TAG'].tolist(),\n","              \"num_tokens\": len(d['word'])\n","          }\n","          dataframe_rows.append(dataframe_row)\n","\n","  dataframe_path = os.path.join(DATA_FOLDER, dataset_name)\n","  df_final = pd.DataFrame(dataframe_rows)\n","  df_final.to_csv(dataframe_path + \".csv\")\n","\n","  unique_tags_words_path = os.path.join(DATA_FOLDER, \"unique_tags_words.pkl\")\n","  unique_tags_words = [unique_tags, unique_words]\n","  with open(unique_tags_words_path, 'wb') as f:\n","    pickle.dump(unique_tags_words, f)\n","    \n","  return  df_final, unique_tags, unique_words\n","\n","df, unique_tags, unique_words = encode_dataset(\"encoded_dataset\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1699279510294,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"135YHdmjPj60","outputId":"c778bad9-720a-43f9-97ab-520cf4e345e6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>split</th>\n","      <th>doc_id</th>\n","      <th>sentence_num</th>\n","      <th>words</th>\n","      <th>tags</th>\n","      <th>num_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[&lt;per&gt;, &lt;per&gt;, ,, 61, years, old, ,, will, joi...</td>\n","      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>1975</th>\n","      <td>val</td>\n","      <td>101</td>\n","      <td>12</td>\n","      <td>[about, a, quarter, of, this, share, has, alre...</td>\n","      <td>[IN, DT, NN, IN, DT, NN, VBZ, RB, VBN, VBN, ,,...</td>\n","      <td>51</td>\n","    </tr>\n","    <tr>\n","      <th>3262</th>\n","      <td>test</td>\n","      <td>151</td>\n","      <td>0</td>\n","      <td>[&lt;org&gt;, &lt;org&gt;, &lt;org&gt;, ,, san, antonio, ,, texa...</td>\n","      <td>[NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...</td>\n","      <td>40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      split  doc_id  sentence_num   \n","0     train       1             0  \\\n","1975    val     101            12   \n","3262   test     151             0   \n","\n","                                                  words   \n","0     [<per>, <per>, ,, 61, years, old, ,, will, joi...  \\\n","1975  [about, a, quarter, of, this, share, has, alre...   \n","3262  [<org>, <org>, <org>, ,, san, antonio, ,, texa...   \n","\n","                                                   tags  num_tokens  \n","0     [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...          18  \n","1975  [IN, DT, NN, IN, DT, NN, VBZ, RB, VBN, VBN, ,,...          51  \n","3262  [NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...          40  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.sort_values(\"doc_id\").groupby('split').head(1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, unique_tags, unique_words):\n","        self.unique_tags = unique_tags\n","        self.unique_words = unique_words\n","        self.tag2int = {}\n","        self.int2tag = {}\n","        self.tag2int[PAD_TOKEN] = PAD_INDEX\n","        self.int2tag[PAD_INDEX] = PAD_TOKEN\n","        self.word2int = {}\n","        self.int2word = {}\n","        self.word2int[PAD_TOKEN] = PAD_INDEX\n","        self.int2word[PAD_INDEX] = PAD_TOKEN\n","        self.build_vocab()\n","\n","    def build_vocab(self):\n","        for i, word in enumerate(self.unique_words):\n","            self.word2int[word] = i+1\n","            self.int2word[i+1] = word\n","        for i, tag in enumerate(self.unique_tags):\n","            self.tag2int[tag] = i+1\n","            self.int2tag[i+1] = tag\n","        \n","    def __len__(self):\n","        return len(self.unique_words) + 1\n","    \n","    def w2i(self, word):\n","        return self.word2int[word]\n","    \n","    def i2w(self, index):\n","        return self.int2word[index]\n","    \n","    def t2i(self, tag):\n","        return self.tag2int[tag]\n","    \n","    def i2t(self, index):\n","        return self.int2tag[index]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["vocab = Vocabulary(unique_tags, unique_words)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def build_indexed_dataframe(df):\n","    \"\"\"\n","        Builds a dataframe with the same structure as the original one, but with the words and tags replaced by their index\n","    \"\"\"\n","    indexed_rows = []\n","    for words,tags in zip(df['words'], df['tags']):\n","        indexed_row = {'indexed_words': [vocab.w2i(word) for word in words], 'indexed_tags': [vocab.t2i(tag) for tag in tags]}\n","        indexed_rows.append(indexed_row)\n","    \n","    indexed_df = pd.DataFrame(indexed_rows)\n","\n","    indexed_df.insert(0,'split',df['split'])\n","    indexed_df.insert(1,'num_tokens',df['num_tokens'])\n","\n","    indexed_df_path = os.path.join(DATA_FOLDER, \"indexed_dataset.pkl\")\n","    indexed_df.to_pickle(indexed_df_path)\n","\n","    return indexed_df\n","\n","indexed_dataset = build_indexed_dataframe(df)\n"]},{"cell_type":"markdown","metadata":{"id":"MQvHxKRcO0l8"},"source":["# [Task 2 - 0.5 points] Text encoding\n","\n","To train a neural POS tagger, you first need to encode text into numerical format."]},{"cell_type":"markdown","metadata":{"id":"0EnUuOWTO0l9"},"source":["### Instructions\n","\n","* Embed words using **GloVe embeddings**.\n","* You are **free** to pick any embedding dimension.\n","* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206458,"status":"ok","timestamp":1699279728592,"user":{"displayName":"Andrea Zecca","userId":"08710816336383652707"},"user_tz":-60},"id":"eVsfe-QKO0l9","outputId":"15f823d9-6f5c-47dc-c24d-cbd4f892bdac"},"outputs":[],"source":["glove_embeddings = GloVe(name='6B', dim=EMBEDDING_DIMENSION)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def get_oov_by_split(splits):\n","    \"\"\"\n","        Returns a dictionary with the oov words for each split\n","    \"\"\"\n","    oov_by_split = {}\n","    for split in splits:\n","        oov_by_split[split] = set()\n","        for words in indexed_dataset[indexed_dataset['split']==split]['indexed_words']:\n","            oov_by_split[split].update([word for word in words if word not in glove_embeddings.stoi])\n","    return oov_by_split\n","\n","def get_oov_neighbors(oovs, sentences):\n","    \"\"\"\n","        Returns a dictionary with the oov words and their neighbors\n","    \"\"\"\n","    oov_neighbors = {}\n","    for oov in oovs:\n","        oov_neighbors[oov] = set()\n","        for sentence in sentences:\n","            # Save all the words that are in a range of MEAN_EMBEDDING_WINDOW words before and after the oov word\n","            if oov in sentence:\n","                oov_idx = sentence.index(oov)\n","                for i in range(max(0, oov_idx-MEAN_EMBEDDING_WINDOW), min(len(sentence), oov_idx+MEAN_EMBEDDING_WINDOW+1)):\n","                    oov_neighbors[oov].add(sentence[i])\n","    return oov_neighbors\n","\n","def build_split_matrix(oovs, oov_neighbors, emb_model):\n","    embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIMENSION))\n","    for word, idx in vocab.word2int.items():\n","            if word in oov_neighbors:\n","                neighboring_wvs = []\n","                for neighbor in oov_neighbors[word]:\n","                    if neighbor not in oovs:\n","                        neighboring_wvs.append(emb_model[neighbor])\n","                # If there is at least one neighbor, compute the mean of their word vectors\n","                if len(neighboring_wvs) > 1:\n","                    embedding_vector = np.mean(neighboring_wvs, axis=0)\n","                else:\n","                    embedding_vector = FIXED_OOV_VECTOR if FIXED_OOV else np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n","            else:\n","                embedding_vector = emb_model[word]\n","            embedding_matrix[idx] = embedding_vector\n","    return embedding_matrix"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def build_embedding_matrix(emb_model):\n","    \"\"\"\n","        Creates the embedding matrix from the embedding model starting from the vocabulary and from pre-trained Glove embeddings\n","    \"\"\"\n","\n","    embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIMENSION), dtype=np.float32)    # len(vocab) already includes the padding token\n","\n","    if OOV_EMBEDDING_TYPE == 'random':\n","\n","        for word, idx in vocab.word2int.items():\n","            if word in emb_model.stoi:\n","                embedding_vector = emb_model[word]\n","            else:\n","                embedding_vector = FIXED_OOV_VECTOR if FIXED_OOV else np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n","            embedding_matrix[idx] = embedding_vector\n","\n","        path = os.path.join(DATA_FOLDER, \"emb_matrix\")\n","        np.save(path, embedding_matrix, allow_pickle=True)\n","\n","        return embedding_matrix, [], [], []\n","    \n","    if OOV_EMBEDDING_TYPE == 'mean':\n","        oov_by_split = get_oov_by_split(['train', 'val', 'test'])\n","        \n","        train_oov = oov_by_split['train']\n","\n","        val_oov = oov_by_split['val']\n","        # val_oov = val_oov - train_oov\n","\n","        test_oov = oov_by_split['test']\n","        # test_oov = test_oov - train_oov - val_oov\n","\n","        train_sentences = df[df['split']=='train']['words'].tolist()\n","        #val_sentences = df[df['split']=='val']['words'].tolist()\n","        #test_sentences = df[df['split']=='test']['words'].tolist()\n","\n","        train_oov_neighbors = get_oov_neighbors(train_oov, train_sentences)\n","        #val_oov_neighbors = get_oov_neighbors(val_oov, val_sentences)\n","        #test_oov_neighbors = get_oov_neighbors(test_oov, test_sentences)\n","\n","        # Now we construct three different embedding matrix, one for each split\n","        train_embedding_matrix = build_split_matrix(train_oov, train_oov_neighbors, emb_model)\n","        \n","        val_embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIMENSION))\n","        for word, idx in vocab.word2int.items():\n","            if word in train_oov:\n","                embedding_vector = train_embedding_matrix[idx]\n","            elif word in val_oov:\n","                embedding_vector = FIXED_OOV_VECTOR if FIXED_OOV else np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n","            else:\n","                embedding_vector = emb_model[word]\n","            val_embedding_matrix[idx] = embedding_vector\n","\n","        test_embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIMENSION))\n","        for word, idx in vocab.word2int.items():\n","            if word in train_oov:\n","                embedding_vector = train_embedding_matrix[idx]\n","            elif word in val_oov:\n","                embedding_vector = val_embedding_matrix[idx]\n","            elif word in test_oov:\n","                embedding_vector = FIXED_OOV_VECTOR if FIXED_OOV else np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_DIMENSION)\n","            else:\n","                embedding_vector = emb_model[word]\n","            test_embedding_matrix[idx] = embedding_vector\n","\n","        # Finally, we build the embedding matrix for the whole dataset \n","        return test_embedding_matrix, train_embedding_matrix, val_embedding_matrix, test_embedding_matrix\n","                  \n","\n","embedding_matrix, train_embedding_matrix, val_embedding_matrix, test_embedding_matrix = build_embedding_matrix(glove_embeddings);"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def load_data():\n","    \"\"\"\n","        Loads the dataframe, the embedding matrix, the unique tags and unique words and the indexed dataframe\n","    \"\"\"\n","    df_path = os.path.join(DATA_FOLDER,'encoded_dataset.csv')\n","    emb_matrix_path = os.path.join(DATA_FOLDER,'emb_matrix.npy')\n","    unique_tags_words_path = os.path.join(DATA_FOLDER, \"unique_tags_words.pkl\")\n","    indexed_dataset_path = os.path.join(DATA_FOLDER,'indexed_dataset.pkl')\n","\n","    if os.path.exists(emb_matrix_path) and os.path.exists(indexed_dataset_path) and os.path.exists(unique_tags_words_path) and os.path.exists(df_path):\n","        emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n","        indexed_dataset = pd.read_pickle(indexed_dataset_path)\n","        unique_tags_words = pickle.load(open(unique_tags_words_path, 'rb'))\n","        unique_tags, unique_words = unique_tags_words[0], unique_tags_words[1]\n","        df = pd.read_csv(df_path, index_col=0)\n","\n","    else:\n","        print('What you are looking for is not present in the folder')\n","        return None, None, None, None, None, None\n","\n","    return df, emb_matrix, unique_tags, unique_words, indexed_dataset\n","\n","df, embedding_matrix, unique_tags, unique_words, indexed_dataset = load_data()\n","vocab = Vocabulary(unique_tags, unique_words)"]},{"cell_type":"markdown","metadata":{"id":"N4AChgkfO0l_"},"source":["# [Task 3 - 1.0 points] Model definition\n","\n","You are now tasked to define your neural POS tagger."]},{"cell_type":"markdown","metadata":{"id":"jkERwLU7O0l_"},"source":["### Instructions\n","\n","* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n","* You are **free** to experiment with hyper-parameters to define the baseline model.\n","\n","* **Model 1**: add an additional LSTM layer to the Baseline model.\n","* **Model 2**: add an additional Dense layer to the Baseline model.\n","\n","* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n","\n","**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."]},{"cell_type":"markdown","metadata":{"id":"babZmAQlWGIw"},"source":["### Embedding layer"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def create_emb_layer(weights_matrix, pad_idx = PAD_INDEX):\n","    \"\"\"\n","        Creates the embedding layer from the embedding matrix\n","    \"\"\"\n","    matrix = torch.Tensor(weights_matrix)\n","    emb_layer = nn.Embedding.from_pretrained(matrix, freeze=True, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable \n","    return emb_layer"]},{"cell_type":"markdown","metadata":{},"source":["### Baseline model"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"RSGVLm3iWF2a"},"outputs":[],"source":["class Baseline(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.embedding_layer = create_emb_layer(embedding_matrix)\n","        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=EMBEDDING_DIMENSION, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer(packed_sentences)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"Zd-pJQyu6h-V"},"source":["### Model 1"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"4pOy7gXR62UC"},"outputs":[],"source":["class Model1(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.embedding_layer = create_emb_layer(embedding_matrix)\n","        self.bidirectional_layer_1 = nn.LSTM(bidirectional=True, input_size=EMBEDDING_DIMENSION, hidden_size=lstm_dimension, batch_first=True)\n","        self.bidirectional_layer_2 = nn.LSTM(bidirectional=True, input_size=lstm_dimension*2, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer_1(packed_sentences)\n","        packed_output, _ = self.bidirectional_layer_2(packed_output)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"BY9Vy9997y5Q"},"source":["### Model 2"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"_RM739C970T5"},"outputs":[],"source":["class Model2(nn.Module):\n","    def __init__(self, lstm_dimension, dense_dimension):\n","        super().__init__()\n","        self.embedding_layer = create_emb_layer(embedding_matrix)\n","        self.bidirectional_layer = nn.LSTM(bidirectional=True, input_size=EMBEDDING_DIMENSION, hidden_size=lstm_dimension, batch_first=True)\n","        self.dense_layer_1 = nn.Linear(in_features=lstm_dimension*2, out_features=dense_dimension)\n","        self.dense_layer_2 = nn.Linear(in_features=dense_dimension, out_features=dense_dimension)\n","\n","    def forward(self, sentences, sentences_length):\n","        embedded_sentences = self.embedding_layer(sentences)\n","        packed_sentences = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentences_length, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.bidirectional_layer(packed_sentences)\n","        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","        output = self.dense_layer_1(output)\n","        output = self.dense_layer_2(output)\n","        output = F.log_softmax(output, dim=2)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"p9vgsKiaO0mA"},"source":["# [Task 4 - 1.0 points] Metrics\n","\n","Before training the models, you are tasked to define the evaluation metrics for comparison."]},{"cell_type":"markdown","metadata":{"id":"q_T6Bm1fO0mA"},"source":["### Instructions\n","\n","* Evaluate your models using macro F1-score, compute over **all** tokens.\n","* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n","* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"]},{"cell_type":"markdown","metadata":{"id":"NpoB8xTRO0mA"},"source":["**Note**: What about OOV tokens?\n","   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n","   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n","   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Xx7AKsk69Zcw"},"outputs":[],"source":["def accuracy_and_f1(y_pred, y_true):\n","    \"\"\"\n","        Computes the accuracy and the f1 score\n","    \"\"\"\n","    correct = y_pred.eq(y_true)          \n","    acc = correct.sum()/y_true.shape[0] \n","    f1 = f1_score(y_true, y_pred, average='macro')\n","    return acc,f1"]},{"cell_type":"markdown","metadata":{},"source":["### Let's check our OOV tokens"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of unique words in dataset: 9296\n","Total OOV terms: 504 (5.42%)\n"]}],"source":["def check_OOV_terms(embedding_model, words):\n","    \"\"\"\n","        Checks the OOV terms in the dataset and returns the list of OOV terms and their indexes\n","    \"\"\"\n","    oov_words = []\n","    int_oov_words = []\n","\n","    for word in words:\n","        if word not in embedding_model.itos:\n","           oov_words.append(word) \n","           int_oov_words.append(vocab.w2i(word)) \n","    \n","    print(\"Total number of unique words in dataset:\",len(words))\n","    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(words))*100))    \n","    return oov_words, int_oov_words\n","\n","oov_words, int_oov_words = check_OOV_terms(glove_embeddings, unique_words)"]},{"cell_type":"markdown","metadata":{},"source":["Depending on the text pre-processing, you may have OOV tokens in the dataframe.\n","If we apply lowercasing we have 6.18% of OOV tokens in total. While, when we don't apply lowercasing we have 31.29% of OOV tokens in total. <br>\n","So we might proceed by using lowercased words, since the number of OOV decrease significantly, but we have to be careful because we could lose some information. <br>\n","For example, the word \"Pierre\" is a name, but \"pierre\" is a noun. So, we have to find a way to deal with this problem.<br>\n","We decided to test the model with the two different approaches and see which one is better."]},{"cell_type":"markdown","metadata":{"id":"hy2OFkD_O0mA"},"source":["# [Task 5 - 1.0 points] Training and Evaluation\n","\n","You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."]},{"cell_type":"markdown","metadata":{"id":"_G20Sgu1O0mB"},"source":["### Instructions\n","\n","* Train **all** models on the train set.\n","* Evaluate **all** models on the validation set.\n","* Compute metrics on the validation set.\n","* Pick **at least** three seeds for robust estimation.\n","* Pick the **best** performing model according to the observed validation set performance."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def initialize_weights(model):\n","    \"\"\"\n","        Initializes the weights of the model\n","    \"\"\"\n","    for _, param in model.named_parameters():\n","        if isinstance(model, nn.LSTM) or isinstance(model, nn.Linear):\n","            nn.init.normal_(param.data, mean = 0, std = 0.1)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def number_parameters(model):\n","    \"\"\"\n","        Computes the number of trainable parameters in the model\n","    \"\"\"\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def get_to_be_masked_tags():\n","    \"\"\"\n","        Returns the tags that have to be masked\n","    \"\"\"\n","    punctuation_tags = ['$', '``', '.', ',', '#', 'SYM', ':', \"''\",'-RRB-','-LRB-']\n","    token_punctuation = [vocab.t2i(tag) for tag in punctuation_tags]\n","    return torch.LongTensor(token_punctuation+[PAD_INDEX]) \n","\n","to_mask = get_to_be_masked_tags()\n","\n","def reshape_and_mask(predictions,targets): \n","    \"\"\"\n","        Reshapes the predictions and the targets and masks the elements that have to be masked\n","    \"\"\"\n","    non_masked_elements = torch.isin(targets, to_mask, invert=True)\n","    return predictions[non_masked_elements],targets[non_masked_elements]\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"BdvD5aq-EiHV"},"outputs":[],"source":["class PosDataset(Dataset):\n","    \"\"\"\n","        Dataset class for the POS tagging task\n","    \"\"\"\n","    def __init__(self, text, labels):\n","        self.labels = labels\n","        self.text = text\n","        self.sentence_lengths = [len(sentence) for sentence in self.text]\n","    def __len__(self):\n","            return len(self.labels)\n","    def __getitem__(self, idx):\n","            label = self.labels[idx]\n","            text = self.text[idx]\n","            sample = (text, label, self.sentence_lengths[idx])\n","            return sample\n","\n","\n","def collate_fn(data):\n","    \"\"\"\n","        Collate function for the dataloader\n","    \"\"\"\n","    return ([x[0] for x in data], [x[1] for x in data], [x[2] for x in data])\n","\n","\n","def create_dataloaders(b_s):\n","    \"\"\"\n","        Creates the dataloaders for the train, validation and test sets\n","    \"\"\"\n","    train_df = indexed_dataset[indexed_dataset['split'] == 'train'].reset_index(drop=True)      \n","    val_df = indexed_dataset[indexed_dataset['split'] == 'val'].reset_index(drop=True)\n","    test_df = indexed_dataset[indexed_dataset['split'] == 'test'].reset_index(drop=True)\n","\n","    #create DataframeDataset objects for each split \n","    train_dataset = PosDataset(train_df.iloc[:,2],train_df.iloc[:,3])\n","    val_dataset = PosDataset(val_df.iloc[:,2],val_df.iloc[:,3])\n","    test_dataset = PosDataset(test_df.iloc[:,2],test_df.iloc[:,3])\n","\n","    train_dataloader = DataLoader(train_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","    val_dataloader = DataLoader(val_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","    test_dataloader = DataLoader(test_dataset, batch_size=b_s, shuffle=True, collate_fn= collate_fn)\n","\n","    return train_dataloader,val_dataloader,test_dataloader "]},{"cell_type":"code","execution_count":23,"metadata":{"id":"wuKaQtDuGAia"},"outputs":[],"source":["tr_dl, val_dl, test_dl = create_dataloaders(BATCH_SIZE)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"xzlrW52UIRet"},"outputs":[],"source":["def train(model, epochs, loss_function, dataloader, optimizer, scheduler, name, padding_value = PAD_INDEX):\n","    \"\"\"\n","        Training loop for the model with the given parameters\n","    \"\"\"\n","    model.train()\n","\n","    best_epoch_loss = np.inf\n","    epochs_previously_trained = 0\n","    \n","    if SEED in best_losses.keys():\n","        if name in best_losses[SEED].keys():\n","            best_epoch_loss = best_losses[SEED][name]\n","            \n","    if SEED in total_epochs.keys():\n","        if name in total_epochs[SEED].keys():\n","            epochs_previously_trained = total_epochs[SEED][name]\n","    \n","    for epoch in range(epochs):\n","        start_time = time.time()\n","        total_epoch_loss = 0\n","        \n","        for sentences, pos, s_len in dataloader:\n","            optimizer.zero_grad()\n","            \n","            tensor_sentences = [torch.LongTensor(s) for s in sentences]\n","            tensor_pos = [torch.LongTensor(p) for p in pos]\n","\n","            padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = padding_value)\n","            padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n","\n","            predicted = model(padded_sentences, s_len)\n","\n","            predicted = predicted.view(-1,predicted.shape[-1])    \n","            targets = padded_pos.view(-1)\n","\n","            predicted, targets = reshape_and_mask(predicted, targets)\n","\n","            loss = loss_function(predicted, targets)\n","            loss.backward()\n","            optimizer.step()\n","            total_epoch_loss += loss.item()\n","        \n","        if total_epoch_loss < best_epoch_loss:\n","            best_epoch_loss = total_epoch_loss\n","            if SEED not in best_models.keys():\n","                best_models[SEED] = {}\n","            best_models[SEED][name] = model.state_dict()\n","            \n","            if SEED not in best_losses.keys():\n","                best_losses[SEED] = {}\n","            best_losses[SEED][name] = best_epoch_loss\n","        \n","        scheduler.step(total_epoch_loss)\n","        elapsed = time.time() - start_time \n","        \n","        print(f'Train epoch [{epoch+1 + epochs_previously_trained}/{epochs + epochs_previously_trained}] loss: {total_epoch_loss:.2f} time: {elapsed:.2f}s')\n","    if SEED not in total_epochs.keys():\n","        total_epochs[SEED] = {}\n","    total_epochs[SEED][name] = epochs + epochs_previously_trained"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def evaluate(model, loss_function, dataloader, padding_value=PAD_INDEX,  verbose=True):\n","    \"\"\"\n","        Evaluation function for the model\n","    \"\"\"\n","    model.eval()\n","    \n","    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n","    epoch_loss = 0\n","    \n","    for sentences, pos, s_len in dataloader:\n","        tensor_sentences = [torch.LongTensor(s) for s in sentences]\n","        tensor_pos = [torch.LongTensor(p) for p in pos]\n","\n","        padded_sentences = rnn.pad_sequence(tensor_sentences, batch_first = True, padding_value = padding_value)\n","        padded_pos = rnn.pad_sequence(tensor_pos, batch_first = True, padding_value=padding_value)\n","\n","        predicted = model(padded_sentences, s_len)\n","        predicted = predicted.view(-1,predicted.shape[-1])    \n","        targets = padded_pos.view(-1)\n","\n","        predicted, targets = reshape_and_mask(predicted, targets)\n","\n","        loss = loss_function(predicted, targets)\n","\n","        predicted = predicted.argmax(dim=1)\n","\n","        tot_pred = torch.cat((tot_pred,predicted))\n","        tot_targ = torch.cat((tot_targ,targets))\n","\n","        epoch_loss += loss.item()\n","    \n","    full_accuracy, full_f1 = accuracy_and_f1(tot_pred,tot_targ)\n","    \n","    if verbose: print(f'Eval: loss: {epoch_loss:.2f} accuracy: {full_accuracy:.2f} f1: {full_f1:.2f}')\n","    \n","    return full_accuracy, full_f1, tot_pred, tot_targ"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def load_best_model(model, name):\n","    \"\"\"\n","        Loads the best model for the given seed and the given model name\n","    \"\"\"\n","    model.load_state_dict(best_models[SEED][name])\n","    return model"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["def save_model(model, base_name):\n","    \"\"\"\n","        Saves the model in a file\n","    \"\"\"\n","    name = base_name + \"_\" + str(SEED)\n","    name += \"_lower\" * LOWER\n","    name += \"_number\" * NUMBER\n","    name += \"_ner\" * NER\n","    name += f'{OOV_EMBEDDING_TYPE}_{MEAN_EMBEDDING_WINDOW}' * (OOV_EMBEDDING_TYPE == 'mean')\n","    name += \"_fixed_oov\" * FIXED_OOV\n","    name += \".pt\"\n","    path = os.path.join(WEIGHTS_FOLDER, name)\n","    torch.save(model.state_dict(), path)\n","\n","def load_model(model, name):\n","    \"\"\"\n","        Loads the model from a file\n","    \"\"\"\n","    path = os.path.join(WEIGHTS_FOLDER, name)\n","    model.load_state_dict(torch.load(path))\n","    return model"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["LSTM_DIMENSION = 16\n","DENSE_DIMENSION = len(unique_tags) + 1\n","INITIAL_LEARNING_RATE = 0.01"]},{"cell_type":"markdown","metadata":{},"source":["Baseline model definition"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["loss_function_baseline = CrossEntropyLoss()\n","baseline_model = Baseline(LSTM_DIMENSION, DENSE_DIMENSION)\n","optimizer_baseline = Adam(baseline_model.parameters(), lr=INITIAL_LEARNING_RATE)\n","scheduler_baseline = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_baseline, mode='min', factor=0.1, patience=2, verbose=True)\n","baseline_model.apply(initialize_weights);"]},{"cell_type":"markdown","metadata":{},"source":["Double LSTM model definition"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["loss_double_lstm = CrossEntropyLoss()\n","double_lstm_model = Model1(LSTM_DIMENSION, DENSE_DIMENSION)\n","optimizer_double_lstm = Adam(double_lstm_model.parameters(), lr=INITIAL_LEARNING_RATE)\n","scheduler_double_lstm = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_double_lstm, mode='min', factor=0.1, patience=2, verbose=True)\n","double_lstm_model.apply(initialize_weights);"]},{"cell_type":"markdown","metadata":{},"source":["Double Dense model definition"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["loss_double_dense = CrossEntropyLoss()\n","double_dense_model = Model2(LSTM_DIMENSION, DENSE_DIMENSION)\n","optimizer_double_dense = Adam(double_dense_model.parameters(), lr=INITIAL_LEARNING_RATE)\n","scheduler_double_dense = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_double_dense, mode='min', factor=0.1, patience=2, verbose=True)\n","double_dense_model.apply(initialize_weights);"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters in baseline model: 42222\n","Number of parameters in double lstm model: 48622\n","Number of parameters in double dense model: 44384\n"]}],"source":["print(f'Number of parameters in baseline model: {number_parameters(baseline_model)}')\n","print(f'Number of parameters in double lstm model: {number_parameters(double_lstm_model)}')\n","print(f'Number of parameters in double dense model: {number_parameters(double_dense_model)}')"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [71/90] loss: 3.21 time: 5.03s\n","Train epoch [72/90] loss: 3.19 time: 5.18s\n","Train epoch [73/90] loss: 3.12 time: 5.13s\n","Train epoch [74/90] loss: 3.05 time: 4.58s\n","Train epoch [75/90] loss: 3.05 time: 5.03s\n","Train epoch [76/90] loss: 3.01 time: 4.42s\n","Train epoch [77/90] loss: 2.95 time: 5.06s\n","Train epoch [78/90] loss: 2.87 time: 5.07s\n","Train epoch [79/90] loss: 2.88 time: 5.42s\n","Train epoch [80/90] loss: 2.82 time: 5.60s\n","Train epoch [81/90] loss: 2.76 time: 5.37s\n","Train epoch [82/90] loss: 2.73 time: 4.63s\n","Train epoch [83/90] loss: 2.73 time: 5.91s\n","Train epoch [84/90] loss: 2.70 time: 4.64s\n","Train epoch [85/90] loss: 2.64 time: 4.55s\n","Train epoch [86/90] loss: 2.65 time: 4.49s\n","Train epoch [87/90] loss: 2.58 time: 4.80s\n","Train epoch [88/90] loss: 2.56 time: 4.88s\n","Train epoch [89/90] loss: 2.52 time: 4.45s\n","Train epoch [90/90] loss: 2.51 time: 4.49s\n"]}],"source":["EPOCHS_BASELINE = 20\n","train(baseline_model, EPOCHS_BASELINE, loss_function_baseline, tr_dl, optimizer_baseline, scheduler_baseline, 'baseline')\n","best_baseline_model = load_best_model(baseline_model, 'baseline')"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [71/90] loss: 3.60 time: 8.52s\n","Train epoch [72/90] loss: 3.53 time: 8.41s\n","Train epoch [73/90] loss: 3.48 time: 7.94s\n","Train epoch [74/90] loss: 3.44 time: 8.04s\n","Train epoch [75/90] loss: 3.39 time: 7.98s\n","Train epoch [76/90] loss: 3.33 time: 8.06s\n","Train epoch [77/90] loss: 3.28 time: 8.41s\n","Train epoch [78/90] loss: 3.23 time: 7.86s\n","Train epoch [79/90] loss: 3.13 time: 9.49s\n","Train epoch [80/90] loss: 3.10 time: 9.21s\n","Train epoch [81/90] loss: 3.07 time: 9.09s\n","Train epoch [82/90] loss: 3.02 time: 7.98s\n","Train epoch [83/90] loss: 3.01 time: 9.47s\n","Train epoch [84/90] loss: 2.93 time: 9.47s\n","Train epoch [85/90] loss: 2.88 time: 9.39s\n","Train epoch [86/90] loss: 2.80 time: 8.45s\n","Train epoch [87/90] loss: 2.79 time: 8.51s\n","Train epoch [88/90] loss: 2.77 time: 8.48s\n","Train epoch [89/90] loss: 2.74 time: 8.08s\n","Train epoch [90/90] loss: 2.59 time: 8.04s\n"]}],"source":["EPOCHS_DOUBLE_LSTM = 20\n","train(double_lstm_model, EPOCHS_DOUBLE_LSTM, loss_double_lstm, tr_dl, optimizer_double_lstm, scheduler_double_lstm, 'double_lstm')\n","best_double_lstm_model = load_best_model(double_lstm_model, 'double_lstm')"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train epoch [71/90] loss: 1.71 time: 4.93s\n","Train epoch [72/90] loss: 1.63 time: 5.18s\n","Train epoch [73/90] loss: 1.60 time: 4.47s\n","Train epoch [74/90] loss: 1.60 time: 4.35s\n","Train epoch [75/90] loss: 1.51 time: 4.31s\n","Train epoch [76/90] loss: 1.48 time: 4.38s\n","Train epoch [77/90] loss: 1.39 time: 4.27s\n","Train epoch [78/90] loss: 1.40 time: 4.34s\n","Train epoch [79/90] loss: 1.34 time: 4.30s\n","Train epoch [80/90] loss: 1.25 time: 4.83s\n","Train epoch [81/90] loss: 1.24 time: 5.01s\n","Train epoch [82/90] loss: 1.31 time: 4.51s\n","Train epoch [83/90] loss: 1.15 time: 4.47s\n","Train epoch [84/90] loss: 1.12 time: 4.43s\n","Train epoch [85/90] loss: 1.06 time: 4.44s\n","Train epoch [86/90] loss: 1.03 time: 4.42s\n","Train epoch [87/90] loss: 1.13 time: 4.43s\n","Train epoch [88/90] loss: 1.05 time: 4.53s\n","Epoch 00089: reducing learning rate of group 0 to 1.0000e-04.\n","Train epoch [89/90] loss: 1.03 time: 4.51s\n","Train epoch [90/90] loss: 0.84 time: 4.54s\n"]}],"source":["EPOCHS_DOUBLE_DENSE = 20\n","train(double_dense_model, EPOCHS_DOUBLE_DENSE, loss_double_dense, tr_dl, optimizer_double_dense, scheduler_double_dense, 'double_dense')\n","best_double_dense_model = load_best_model(double_dense_model, 'double_dense')"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>baseline</td>\n","      <td>tensor(0.8296)</td>\n","      <td>0.714416</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>double_lstm</td>\n","      <td>tensor(0.8436)</td>\n","      <td>0.738585</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>double_dense</td>\n","      <td>tensor(0.8301)</td>\n","      <td>0.720820</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          model        accuracy        f1\n","0      baseline  tensor(0.8296)  0.714416\n","1   double_lstm  tensor(0.8436)  0.738585\n","2  double_dense  tensor(0.8301)  0.720820"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["baseline_accuracy_val, baseline_f1_val, baseline_pred_val, baseline_targ_val = evaluate(best_baseline_model, loss_function_baseline, val_dl, verbose=False)\n","double_lstm_accuracy_val, double_lstm_f1_val, double_lstm_pred_val, double_lstm_targ_val = evaluate(best_double_lstm_model, loss_double_lstm, val_dl, verbose=False)\n","double_dense_accuracy_val, double_dense_f1_val, double_dense_pred_val, double_dense_targ_val = evaluate(best_double_dense_model, loss_double_dense, val_dl, verbose=False)\n","\n","\n","results = pd.DataFrame(columns=['model','accuracy','f1'])\n","results.loc[0] = ['baseline', baseline_accuracy_val, baseline_f1_val]\n","results.loc[1] = ['double_lstm', double_lstm_accuracy_val, double_lstm_f1_val]\n","results.loc[2] = ['double_dense', double_dense_accuracy_val, double_dense_f1_val]\n","\n","results"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>baseline</td>\n","      <td>tensor(0.8541)</td>\n","      <td>0.779485</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>double_lstm</td>\n","      <td>tensor(0.8597)</td>\n","      <td>0.795125</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>double_dense</td>\n","      <td>tensor(0.8481)</td>\n","      <td>0.767491</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          model        accuracy        f1\n","0      baseline  tensor(0.8541)  0.779485\n","1   double_lstm  tensor(0.8597)  0.795125\n","2  double_dense  tensor(0.8481)  0.767491"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["baseline_accuracy_test, baseline_f1_test, baseline_pred_test, baseline_targ_test = evaluate(best_baseline_model, loss_function_baseline, test_dl, verbose=False)\n","double_lstm_accuracy_test, double_lstm_f1_test, double_lstm_pred_test, double_lstm_targ_test = evaluate(best_double_lstm_model, loss_double_lstm, test_dl, verbose=False)\n","double_dense_accuracy_test, double_dense_f1_test, double_dense_pred_test, double_dense_targ_test = evaluate(best_double_dense_model, loss_double_dense, test_dl, verbose=False)\n","\n","\n","results = pd.DataFrame(columns=['model','accuracy','f1'])\n","results.loc[0] = ['baseline', baseline_accuracy_test, baseline_f1_test]\n","results.loc[1] = ['double_lstm', double_lstm_accuracy_test, double_lstm_f1_test]\n","results.loc[2] = ['double_dense', double_dense_accuracy_test, double_dense_f1_test]\n","\n","results"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["save_model(best_baseline_model, 'baseline')\n","save_model(best_double_lstm_model, 'doublelstm')\n","save_model(best_double_dense_model, 'doubledense')"]},{"cell_type":"markdown","metadata":{"id":"_sN60F5MO0mB"},"source":["# [Task 6 - 1.0 points] Error Analysis\n","\n","You are tasked to evaluate your best performing model."]},{"cell_type":"markdown","metadata":{"id":"5IvcbIwiO0mB"},"source":["### Instructions\n","\n","* Compare the errors made on the validation and test sets.\n","* Aggregate model errors into categories (if possible)\n","* Comment the about errors and propose possible solutions on how to address them."]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def build_classification_report(targ,pred,unique_tags):\n","    \"\"\"\n","        Build classification report and prints it\n","    \"\"\"\n","    report = classification_report(targ,pred,zero_division=0,output_dict=False,target_names=unique_tags)\n","    print(report)\n","\n","def build_confusion_matrix(targ,pred,unique_tags):\n","    \"\"\"\n","        Build confusion matrix, plot and returns it\n","    \"\"\"\n","    cf_matrix = confusion_matrix(targ, pred)\n","    df_cm = pd.DataFrame(cf_matrix, index = unique_tags, columns = unique_tags) \n","    plt.figure(figsize = (40,32))\n","    sn.heatmap(df_cm, annot=True, cmap=\"Blues\", linewidths= 0.05, linecolor='white')\n","    return df_cm\n","\n","def build_errors_dictionary(df_cm):\n","    \"\"\"\n","        Build errors dictionary and prints it\n","    \"\"\"\n","    errors = {}\n","    for true_tag,row in df_cm.iterrows():    #loop on the rows of the dataframe \n","\n","        tag_errors = []\n","        for pred_tag, occurrences in row.items() :     #loop on each column of that specific row \n","            if not pred_tag==true_tag and occurrences!=0 : \n","                tag_errors.append((pred_tag,occurrences))\n","        \n","        tag_errors.sort(key = itemgetter(1),reverse=True)   #sort it so that the tag that is more mistaken for the correct one is the first one to appear on the left \n","\n","        if tag_errors:     \n","            errors[true_tag] = tag_errors     #put it in the dict only if there are actually errors \n","\n","    errors = dict(sorted(errors.items(), key = lambda item : item[1][0][1],reverse=True))    #sort the dictionary in order to have the more wrongly classified tags on top \n","\n","    #pretty print \n","    print('true_TAG --> (pred_TAG, n_times)\\n')\n","    for k,v in errors.items():\n","        print(k,'-->',*v)\n","\n","def get_tag_distribution(indexed_df: pd.DataFrame):\n","    \"\"\"\n","        Count number of occurrences of each TAG in the train set \n","    \"\"\"\n","    tag_frequency = {}\n","    df_temp = indexed_df[indexed_df['split'] == 'train']\n","    for _ ,row in df_temp.iterrows():\n","        for key in row['indexed_tags']:\n","            tag_frequency[int2tag[key]] = tag_frequency.get(int2tag[key],0) + 1\n","\n","    return dict(sorted(tag_frequency.items(), key=lambda item: item[1], reverse = True))"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'baseline_pred' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/andrea/Desktop/NLP/NLP-assignments/Assignment-1/Assignment1.ipynb Cell 66\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andrea/Desktop/NLP/NLP-assignments/Assignment-1/Assignment1.ipynb#Y301sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tags \u001b[39m=\u001b[39m [vocab\u001b[39m.\u001b[39mi2t(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(baseline_pred\u001b[39m.\u001b[39mtolist() \u001b[39m+\u001b[39m baseline_targ\u001b[39m.\u001b[39mtolist())]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/NLP/NLP-assignments/Assignment-1/Assignment1.ipynb#Y301sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_cm \u001b[39m=\u001b[39m build_confusion_matrix(baseline_targ,baseline_pred,tags)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/NLP/NLP-assignments/Assignment-1/Assignment1.ipynb#Y301sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m build_errors_dictionary(df_cm)\n","\u001b[0;31mNameError\u001b[0m: name 'baseline_pred' is not defined"]}],"source":["tags = [vocab.i2t(i) for i in set(baseline_pred.tolist() + baseline_targ.tolist())]\n","\n","df_cm = build_confusion_matrix(baseline_targ,baseline_pred,tags)\n","\n","build_errors_dictionary(df_cm)"]},{"cell_type":"markdown","metadata":{"id":"32F8eX_hO0mB"},"source":["# [Task 7 - 1.0 points] Report\n","\n","Wrap up your experiment in a short report (up to 2 pages)."]},{"cell_type":"markdown","metadata":{"id":"xv4OVz1vO0mC"},"source":["### Instructions\n","\n","* Use the NLP course report template.\n","* Summarize each task in the report following the provided template."]},{"cell_type":"markdown","metadata":{"id":"LkAPKCbIO0mC"},"source":["### Recommendations\n","\n","The report is not a copy-paste of graphs, tables, and command outputs.\n","\n","* Summarize classification performance in Table format.\n","* **Do not** report command outputs or screenshots.\n","* Report learning curves in Figure format.\n","* The error analysis section should summarize your findings."]},{"cell_type":"markdown","metadata":{"id":"449lJeI-O0mC"},"source":["# Submission\n","\n","* **Submit** your report in PDF format.\n","* **Submit** your python notebook.\n","* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n","* You can upload **model weights** in a cloud repository and report the link in the report."]},{"cell_type":"markdown","metadata":{"id":"adKOVVP-O0mC"},"source":["# FAQ\n","\n","Please check this frequently asked questions before contacting us"]},{"cell_type":"markdown","metadata":{"id":"x9faC6QnO0mC"},"source":["### Trainable Embeddings\n","\n","You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."]},{"cell_type":"markdown","metadata":{"id":"KML9gLHgO0mC"},"source":["### Model architecture\n","\n","You **should not** change the architecture of a model (i.e., its layers).\n","\n","However, you are **free** to play with their hyper-parameters."]},{"cell_type":"markdown","metadata":{"id":"8cgg47FVO0mD"},"source":["### Neural Libraries\n","\n","You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"]},{"cell_type":"markdown","metadata":{"id":"6qmgkNGsO0mD"},"source":["### Keras TimeDistributed Dense layer\n","\n","If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."]},{"cell_type":"markdown","metadata":{"id":"AKxd8OlIO0mD"},"source":["### Error Analysis\n","\n","Some topics for discussion include:\n","   * Model performance on most/less frequent classes.\n","   * Precision/Recall curves.\n","   * Confusion matrices.\n","   * Specific misclassified samples."]},{"cell_type":"markdown","metadata":{"id":"_m5SkbO0O0mD"},"source":["### Punctuation\n","\n","**Do not** remove punctuation from documents since it may be helpful to the model.\n","\n","You should **ignore** it during metrics computation.\n","\n","If you are curious, you can run additional experiments to verify the impact of removing punctuation."]},{"cell_type":"markdown","metadata":{"id":"urd139anO0mD"},"source":["# The End"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
